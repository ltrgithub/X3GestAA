"use strict";

/// !doc
/// # Mongodb handle API  
/// usually this object is already instantiated on the context
/// ```javascript
/// var dataModel = require("syracuse-orm/lib/dataModel");
/// var model = dataModel.make(...);
/// // now get the mongodb handle
/// var db = dataModel.getOrm(_, model, dataset);  
/// ```
/// 

var mongodb = require('streamline-mongodb');
var helpers = require("syracuse-core/lib/helpers");
var forEachKey = helpers.object.forEachKey;
var flows = require("streamline/lib/util/flows");
var locale = require("syracuse-core/lib/locale");
var globals = require('streamline/lib/globals');
var filterHelpers = require("./../filters");
var streams = require("streamline/lib/streams/streams");
var ReadableStream = streams.ReadableStream;
var WritableStream = streams.WritableStream;
var ez = require('ez-streams');
var sys = require("util");
var perfmon = require("syracuse-perfmon");
var base64;
var importHandler = require("syracuse-import/lib/jsonImport");
var config;

var processId = require('os').hostname() + "$_" + process.pid;

var tracer; // = console.log;
var connectionPool = null;
var opRef = 0;

var _funnels = {};

var syracuseEscapeMap = {
	$uuid: "_uuid",
	$url: "_url",
	$index: "_index",
	$keys: "_keys",
	$type: "_type",
	$syncUuid: "_syncUuid",
	$variantType: "_variantType",
	$signature: "_signature",
	$updDate: "_updDate",
	$updUser: "_updUser",
	$creDate: "_creDate",
	$creUser: "_creUser",
	$ttl: "_expire",
	$tick: "_tick",
	$stamp: "_stamp",
	$endpoint: "_endpoint"
};
var syracuseUnescapeMap = {
	_uuid: "$uuid",
	_url: "$url",
	_index: "$index",
	_keys: "$keys",
	_type: "$type",
	_syncUuid: "$syncUuid",
	_variantType: "$variantType",
	_signature: "$signature",
	_updDate: "$updDate",
	_updUser: "$updUser",
	_creDate: "$creDate",
	_creUser: "$creUser",
	_expire: "$ttl",
	_tick: "$tick",
	_stamp: "$stamp",
	_endpoint: "$endpoint"
};

// escape :
// $ by \u007F

function escapeArray(input) {
	return input.map(function(item) {
		switch (typeof item) {
			case "object":
				return escapeJson(item);
			case "string":
				return (item[0] === "$") ? ("\u007F" + item.substring(1)) : item;
			default:
				return item;
		}
	});
}

function escapeJson(input) {
	// must clone as original object might be used later unescaped
	if (Array.isArray(input)) return escapeArray(input);
	//
	var out = {};
	forEachKey(input, function(key, value) {
		var escKey = key;
		var escVal = value;
		if (value !== null) {
			if (Array.isArray(value)) {
				escVal = escapeArray(value);
			} else if (typeof value === "object") escVal = escapeJson(value);
			if (syracuseEscapeMap[key]) escKey = syracuseEscapeMap[key];
			else {
				if (key[0] === "$") {
					escKey = "\u007F" + key.substring(1);
					//			escKey = "Ã©"+key.substring(1); 
				}
			}
		}
		out[escKey] = escVal;
	});
	//
	return out;
}
// unescape :
// \u007F by $

function unescapeJson(input) {
	forEachKey(input, function(key, value) {
		var escKey = key;
		var escVal = value;
		if (Array.isArray(value)) {
			escVal = [];
			value.forEach(function(item) {
				if (typeof item === "object") escVal.push(unescapeJson(item));
				else if (typeof item === "string") escVal.push((item[0] === "\u007F") ? ("$" + item.substring(1)) : item);
				else escVal.push(item);
			});
		} else if (typeof value === "object") escVal = unescapeJson(value);
		if (syracuseUnescapeMap[key]) {
			escKey = syracuseUnescapeMap[key];
			delete input[key];
		} else {
			if (key[0] === "\u007F") {
				escKey = "$" + key.substring(1);
				delete input[key];
			}
		}
		input[escKey] = escVal;
	});
	//	console.log("unescaped json: "+sys.inspect(input,null,4));
	return input;
}

function _startProfile() {
	return (new Date()).getTime();
}

function _endProfile(startTime) {
	return (new Date()).getTime() - startTime;
}

function decorateFilter(_, db, entity, filter) {
	function _makeTerm(key, loc, value) {
		var locKey = key.slice(0);
		locKey.push(loc);
		locKey = locKey.join(".");
		var term = {};
		term[locKey] = value;
		return term;
	}

	function _isLocalizedProp(ent, propName) {
		return ent.$properties && ent.$properties[propName] && ent.$properties[propName].$isLocalized;
	}
	flows.eachKey(_, filter, function(_, key, value) {
		if (Array.isArray(value)) {
			value.forEach_(_, function(_, innerFilter) {
				decorateFilter(_, db, entity, innerFilter);
			});
		} else {
			var newKey = null;
			if (key === "$uuid") {
				newKey = "_id";
			} else {
				var sKey = key.split(".");
				if (sKey[sKey.length - 1] === "$uuid") {
					sKey.pop(); // if last part is $uuid ignore it (is added automatically)
				}
				var lastPart = sKey[sKey.length - 1];
				var keysLen = sKey.length;
				// walk
				var targetEnt = entity;
				var rels = [];
				var crtRel = null;
				sKey.forEach(function(part) {
					if (targetEnt.$relations[part]) {
						crtRel = targetEnt.$relations[part];
						targetEnt = targetEnt.$relations[part].targetEntity;
					} else crtRel = null;
					rels.push(crtRel);
				});
				// last part is a relation
				if (crtRel) {
					if (crtRel.isPlural) {
						newKey = sKey.join(".") + "." + syracuseEscapeMap["$uuid"];
						if (value == null) value = {
							$in: [
								[], null
							]
						};
					} else newKey = sKey.join(".") + "." + syracuseEscapeMap["$uuid"];
				} else {
					// lastPart is not a relation but a join to a reference (not child)
					if ((keysLen > 1) && (sKey[keysLen - 1]) && rels[keysLen - 2] && !rels[keysLen - 2].getIsChild()) {
						var rel = rels[keysLen - 2];
						var f = {};
						f[lastPart] = value;
						var js = db.fetchInstances(_, rel.targetEntity, {
							jsonWhere: f
						});
						value = {
							$in: js.map(function(j) {
								return j.$uuid;
							})
						};
						sKey[keysLen - 1] = "_uuid";
						newKey = sKey.join(".");
					} else {
						// case sensitive
						if (targetEnt.$properties && targetEnt.$properties[lastPart] && targetEnt.$properties[lastPart].$caseInsensitive)
							if (typeof value === "string") {
								// escape special characters of regular expressions
								var valueEsc = value.replace(/([\\\^\$\.\(\)\[\]\{\}\*\?\+])/g, "\\$1");
								filter[key] = value = {
									$regex: "^" + valueEsc + "$",
									$options: "i"
								};
							}
							// localization
						if (_isLocalizedProp(targetEnt, lastPart)) {
							if (value != null) {
								var locKey = sKey + "." + locale.current.toLowerCase();
								var t_eq = {};
								t_eq[locKey] = null;
								var t_neq = {};
								t_neq[locKey] = {
									$ne: null
								};
								//
								newKey = "$or";
								value = [{
									$and: [t_neq, _makeTerm(sKey, locale.current.toLowerCase(), value)]
								}, {
									$and: [t_eq, _makeTerm(sKey, "default", value)]
								}];
							}
						} else {
							// is last part the locale code of a localized prop ?
							if (_isLocalizedProp(targetEnt, sKey[sKey.length - 2])) {
								sKey[sKey.length - 1] = sKey[sKey.length - 1].toLowerCase();
								newKey = sKey.join(".");
							} else {
								// last part is property or technical
								if (sKey.length && syracuseEscapeMap[lastPart]) {
									sKey[sKey.length - 1] = syracuseEscapeMap[lastPart];
									newKey = sKey.join(".");
								}
							}
						}
					}
				}
			}
			if (newKey && (newKey !== key)) {
				filter[newKey] = value;
				delete filter[key];
			}
		}
	});
	return filter;
}

function getSelectFields(entity, shallow) {
	var fields = [];
	//	var meta = entity.deepMeta;
	var meta = entity;
	forEachKey(meta.$properties, function(name, prop) {
		if (prop.$isComputed || prop.$isLazy) return;
		fields.push(name);
	});
	forEachKey(meta.$relations, function(name, rel) {
		if (rel.$isComputed || rel.$isLazy) return;
		fields.push(name);
	});
	// add technical fields
	fields.splice(0, 0, "_creUser", "_creDate", "_updUser", "_updDate", "_signature", "_syncUuid", "_tick", "_endpoint", "_stamp", "_expire");
	//
	return fields;
}

// cursor

function _nextObject(cursor, _) {
	return cursor.nextObject(_);
}

function MongodbCursor(db, entity, cursor) {
	this._cursor = cursor;
	this._entity = entity;
	this._db = db;
	this._dataFuture = _nextObject(this._cursor, !_);
}
var cursorProto = MongodbCursor.prototype;
cursorProto.next = function(_) {
	var self = this;
	var data;
	if (data = self._dataFuture(_)) {
		self._dataFuture = _nextObject(self._cursor, !_);
		data.$key = data._id;
		data.$uuid = data._id;
		data.$loaded = true;
		return self._entity.factory.createInstance(_, unescapeJson(data), self._db);
	} else return null;
};

// stores
// writable stream wrapper for grid store

function GridWritableStream(store) {
	this.store = store;
}
var gdWsProto = GridWritableStream.prototype;
gdWsProto.write = function(_, buffer, enc) {
	if (buffer == null)
	// means end
		this.store.close(_);
	else this.store.write(buffer, _);
};
ez.writer.decorate(gdWsProto);
// store constructor

function MongodbFileStore(db, fileName) {
	this.db = db.db;
	this.fileName = fileName;
	this.readStore = null;
	this.writeStore = null;
	this.readPosition = 0;
}
//

function _openStore(_, fileStore, mode) {
	var store = new mongodb.GridStore(fileStore.db, fileStore.fileName, mode);
	return store.open(_);
}
var storeProto = MongodbFileStore.prototype;
storeProto.getProperties = function(_) {
	var store = this.readStore || _openStore(_, this, "r");
	return {
		length: store.length,
		contentType: store.contentType,
		fileName: store.metadata && store.metadata.fileName,
		uploadDate: store.uploadDate,
		chunkSize: store.chunkSize
	};
};
storeProto.fileExists = function(_) {
	if (!this.fileName) return false;
	return mongodb.GridStore.exist(this.db, this.fileName, _);
};
storeProto.setFile = function(_, fileName) {
	this.close(_);
	//
	this.fileName = fileName;
};
// stream interface
storeProto.createReadableStream = function(_) {
	// force open a file store to reset stream read position
	tracer && tracer("mongodbDbHandle.GridFS open store for read: " + this.fileName);
	var store = (new mongodb.GridStore(this.db, this.fileName, "r")).open(_);
	var stream = new ReadableStream(store.stream(true));
	stream.headers = {
		contentType: store.contentType,
		contentLength: store.length,
		filename: store.metadata && store.metadata.fileName,
	};
	return stream;
};
storeProto.createWritableStream = function(_, options) {
	var mode = (options && options.append) && "w+" || "w";
	var store = new mongodb.GridStore(this.db, this.fileName, mode).open(_);
	//
	store.metadata = store.metadata || {};
	if (options) {
		if (options.contentType) store.contentType = options.contentType;
		if (options.fileName) store.metadata.fileName = options.fileName;
		//
		if (options.referingInstance) store.metadata.referingInstance = options.referingInstance;
	}
	var etag = store.metadata.etag || 0;
	store.metadata.etag = ++etag;
	//
	return (new GridWritableStream(store));
};
storeProto.writeMetadata = function(_, options) {
	var store = new mongodb.GridStore(this.db, this.fileName, "w+").open(_);
	//
	store.metadata = store.metadata || {};
	if (options) {
		if (options.contentType) store.contentType = options.contentType;
		if (options.fileName) store.metadata.fileName = options.fileName;
		//
		if (options.referingInstance) store.metadata.referingInstance = options.referingInstance;
	}
	var etag = store.metadata.etag || 0;
	store.metadata.etag = ++etag;
	store.close(_);
};
storeProto.deleteFile = function(_) {
	if (this.fileExists(_)) mongodb.GridStore.unlink(this.db, this.fileName, _);
};
storeProto.close = function(_) {};

function MongoDbHandle(model, dataset) {
	var self = this;
	self.model = model;
	self.dataset = dataset;
	self.baseUrl = ["/sdata", model.contract.application, model.contract.contract, dataset.database].join("/");

	self.escapeJson = function(input) {
		return escapeJson(input);
	};
	self.unescapeJson = function(input) {
		return unescapeJson(input);
	};
	// initialize the database connection
	self.connect = function(_) {
		function _mustSync(_, params) {
			var sa = _getUpdateScripts(model);
			return sa.some(function(us) {
				var meta = require(us.script).metadata || {};
				if (!meta.fileId) return;
				var actualVersion = ((params[meta.fileId] || {}).version != null) ? params[meta.fileId].version : -1;
				return (actualVersion !== (us.version || 0));
			});
		}
		var self = this;
		var automaticImports = [];
		(model.dbMeta.automaticImport || []).forEach(function(item) {
			automaticImports.push(item);
		});
		var host = dataset.hostname;
		var port = dataset.port;
		//		var host = "localhost";
		//		var port = 27017;
		var key = host + '/' + port + '/' + dataset.database + '/' + globals.context.tenantId;
		if (connectionPool && connectionPool[key]) {
			self.db = connectionPool[key];
			tracer && tracer("Mongodb connect - pool connection for : " + host + ":" + port + "/" + dataset.database + " in state: " + self.db.state);
			if (self.db.state != "connected") self.db.open(_);
			return;
		}
		// create a connection
		//		tracer && tracer("Mongodb connect : "+dataset.hostname+":"+dataset.port);
		tracer && tracer("Mongodb connect : " + host + ":" + port);
		var server = new mongodb.Server(host, port, {});
		var dbname = dataset.databaseName ? dataset.databaseName : dataset.database;
		if (globals.context.tenantId) dbname = globals.context.tenantId + '-' + dbname;
		var db = new mongodb.Db(dbname, server, {
			w: "majority"
		});
		db = db.open(_);
		self.db = db;
		// add to pool BEFORE synchronize, because sync scripts might use adminUtil and request the admin orm
		if (connectionPool) connectionPool[key] = db;
		// synchronize start
		// force unlock resque procedure
		if (process.argv[2] === "--dbUnlock") self.unlockDatabase(_);
		if (process.argv[2] === "--dbUnlockAll") self.unlockAll(_);
		// find out whether syncrhonization is necessary
		var sync = false;
		var params = db.collection("dbParam", _).find().toArray(_);
		if (params && params.length) {
			var p = params[0];
			tracer && tracer("mongodbDbHandle.connect ModelName: " + model.name + "; found dbParam: " + sys.inspect(p));
			// structure change >>> : if dbVersion is not object, assume version number is the one of the first script in update scripts array
			var pm = p[model.name] = p[model.name] || {};
			var updScript = model.dbMeta.updateScript || {};
			var firstScript = Array.isArray(updScript) ? updScript[0].script : updScript.script;
			if (pm.dbVersion != null && firstScript) {
				var meta = require(firstScript).metadata;
				if (typeof pm.dbVersion === "object") {
					// this was not so good idea, db's aren't compatible with old code anymore
					// we create a plural instead
					var data = {};
					data.$set = {};
					data.$set[model.name] = {
						dbVersion: (pm.dbVersion[meta.fileId] || {}).version,
						dbVersions: pm.dbVersion
					};
					db.collection("dbParam", _).update({}, data, {
						safe: true,
						upsert: true
					}, _);
				} else if (!pm.dbVersions) {
					var ver = pm.dbVersion;
					pm.dbVersions = {};
					pm.dbVersions[meta.fileId] = {
						version: ver,
						description: meta.description
					};
					var data = {};
					data.$set = {};
					data.$set[model.name] = {
						dbVersion: ver,
						dbVersions: pm.dbVersions
					};
					db.collection("dbParam", _).update({}, data, {
						safe: true,
						upsert: true
					}, _);
				}
			}
			// structure change <<<
			sync = _mustSync(_, pm.dbVersions);
			if (!sync && automaticImports.length > 0) {
				if (!pm.automaticImportEtags) {
					sync = true;
				} else {
					// test modification times of update files first: loop through files until a file has not yet been examined or
					// its modification time is different: if a file has been found, it must be considered for automatic update.
					while (automaticImports.length) {
						var fname = automaticImports.shift();
						var basename = fname.substr(0, fname.indexOf('.'));
						// console.log("AI0 "+fname+" "+basename)
						if (!(basename in pm.automaticImportEtags) || importHandler.importTest(_, fname, pm.automaticImportEtags[basename])) { // some change
							automaticImports.unshift(fname); // put item back into array because it must be handled again
							// console.log("UNSHIFT "+automaticImports)
							sync = true;
							break;
						}
					}
				}
			}
		} else sync = true;
		if (sync) {
			tracer && tracer("Synchronization seems to be necessary. Lock the database");
			// make sure there is not another operation pending
			var start = new Date();
			var lock = self.lockDatabase(_);
			while ((lock.status !== "success") && (((new Date()) - start) < 60000)) {
				// wait some time, 50ms min
				setTimeout(~_, Math.floor(Math.random() * 10000) + 50);
				//
				lock = self.lockDatabase(_);
			}
			if (lock.status !== "success") throw new Error(locale.format(module, "databaseLockTimeout"));
			//
			try {
				// read dbParam again to find out whether synchronization is still necessary
				var pm;
				var params = db.collection("dbParam", _).find().toArray(_);
				if (params && params.length) {
					var p = params[0];
					pm = p[model.name] = p[model.name] || {};
				}
				if (pm && pm.dbVersions) {
					tracer && tracer("mongodbDbHandle.connect ModelName: " + model.name + "; found dbParam: " + sys.inspect(p));
					var sa = _getUpdateScripts(model);
					// execute update for each script
					sa.forEach_(_, function(_, us) {
						var meta = require(us.script).metadata || {};
						if (!meta.fileId) return;
						var actualVersion = ((pm.dbVersions[meta.fileId] || {}).version != null) ? pm.dbVersions[meta.fileId].version : -1;
						tracer && tracer("mongodbDbHandle.connect ActualVersion: " + meta.fileId + "." + actualVersion);
						if (actualVersion !== (us.version || 0)) {
							_synchronizeDb(_, model, db, actualVersion, us);
						}
					});

				} else {
					_initializeDb(_, model, db);
					_setExpireEntities(_, model, db);
				}
				// handle automatic imports
				var config = require('syracuse-main/lib/nodeconfig').config; // must be first syracuse require
				if (!(config.system && config.system.protectSettings)) {
					var syncTracer = console.log;
					syncTracer && syncTracer("!!! EXECUTION OF THIS SCRIPT MAY TAKE UP TO 5 MINUTES, DON'T STOP YOUR SERVER !!!");
					var changeParam = false;
					var diag = [];
					// import files with changes. The files without modifications which have been examined before, will not be
					// 	examined a second time
					if (automaticImports.length > 0) {
						var shouldUpdateParams = false;
						pm = pm || {};
						pm.automaticImportEtags = pm.automaticImportEtags || {};
						while (automaticImports.length > 0) {
							var fname = automaticImports.shift();
							var basename = fname.substr(0, fname.indexOf('.'));
							// console.log("AI "+fname)
							var data = pm.automaticImportEtags[basename];
							if (!data) {
								data = pm.automaticImportEtags[basename] = {
									updDate: null,
									contentHash: ""
								};
							}
							if (!importHandler.jsonImport(_, self, fname, {
								tracer: null,
								importMode: "update",
								$diagnoses: diag,
								ifNoneMatch: data
							})) {
								// remove unnecessary data in pm.automaticImportEtags before saving
								var list = Object.keys(pm.automaticImportEtags);

								if (list.length > model.dbMeta.automaticImport.length) {
									list.forEach(function(item) {
										if (model.dbMeta.automaticImport.indexOf(item + ".json") < 0) {
											delete pm.automaticImportEtags[item];
										}
									});
								}
								shouldUpdateParams = true;
								syncTracer && syncTracer("Initialization file: " + fname + " imported");
							}
						}
						// 	change data in dbParam for this script
						if (shouldUpdateParams) {
							var upd = {};
							upd.$set = {};
							var automaticArray = [];
							// 	convert object back to array (for storage in MongoDB)
							upd.$set[model.name + ".automaticImportEtags"] = pm.automaticImportEtags;
							db.collection("dbParam", _).update({}, upd, {
								safe: true,
								upsert: true
							}, _);
						}
						syncTracer && syncTracer("Import of initialization files ended");
					}
				}
			} finally {
				self.unlockDatabase(_);
			}
		}
	};
	//

	function _getUpdateScripts(model) {
		if (!model.dbMeta.updateScript) return [];
		return Array.isArray(model.dbMeta.updateScript) ? model.dbMeta.updateScript : [{
			script: model.dbMeta.updateScript,
			version: model.dbMeta.version
		}];
	}

	self.ensureExpireIndex = function(_, entity) {
		var db = self.db;
		if (entity.$expire) {
			// check if index already exists  
			var col = db.collection(entity.className, _);
			var indexes = col.indexInformation({
				full: true
			}, _);
			var found = false;
			for (var i = 0; i < indexes.length && !found; i++) {
				found = (indexes[i].key["_expire"]);
			}
			if (!found) {
				col.ensureIndex({
					"_expire": 1
				}, {
					expireAfterSeconds: 0
				}, _);
			}
		}
	};

	function _setExpireEntities(_, model, db) {
		var entities = model.getEntities();
		if (entities) {
			for (var name in entities) self.ensureExpireIndex(_, entities[name]);
		}
	}

	self.updateEntityIndex = function(_, entity) {
		var db = self.db;
		if (entity.$indexes)
			for (var idxName in entity.$indexes) {
				var indexItem = entity.$indexes[idxName];
				var fields = {};
				var isUnique = false;
				for (var i in indexItem) {
					if (i !== "$unique") fields[i] = ((indexItem[i] === "descending") || (indexItem[i] == -1)) ? -1 : 1;
				}
				//
				tracer && tracer("mongodbDbHandle.synchronize ensure index: " + entity.className + "." + sys.inspect(fields));
				db.collection(entity.className, _).ensureIndex(fields, indexItem.$unique, _);
			}
		entity.$uniqueConstraints && entity.$uniqueConstraints.forEach_(_, function(_, uc) {
			var fields = {};
			uc.forEach(function(f) {
				fields[f] = 1;
			});
			tracer && tracer("mongodbDbHandle.synchronize ensure unique index: " + entity.className + "." + sys.inspect(fields));
			db.collection(entity.className, _).ensureIndex(fields, true, _);
		});
	};

	function _initializeDb(_, model, db) {
		function _applyInitScript(_, script) {
			console.log("mongodbDbHandle.apply initialize script: " + script);
			importHandler.jsonImport(_, self, script, {
				tracer: tracer,
				importMode: "update", // leave update because of include before / after, we can modify even in intialization
				$diagnoses: diag
			});
		}
		if (self._isSynchronizing) return;
		self._isSynchronizing = true;
		try {
			var config = require('syracuse-main/lib/nodeconfig').config; // must be first syracuse require
			// db init
			if (!(config.system && config.system.protectSettings) && model.dbMeta && model.dbMeta.initScript) {
				//tracer && tracer("mongodbDbHandle.apply initialize script: " + model.dbMeta.initScript);
				var diag = [];
				if (Array.isArray(model.dbMeta.initScript)) {
					model.dbMeta.initScript.forEach_(_, function(_, scr) {
						_applyInitScript(_, scr);
					});
				} else _applyInitScript(_, model.dbMeta.initScript);
				if (diag.length) tracer && tracer("mongodbDbHandle.apply initialize script errors: " + sys.inspect(diag, null, 4));
			}
			// create indexes
			var entities = model.getEntities();
			if (entities) //
				for (var name in entities) {
					var entity = entities[name];
					self.updateEntityIndex(_, entity);
					//_ensureExpireIndex(_, entity, db);
				}
				// update dbParam
			var sa = _getUpdateScripts(model);
			var data = {};
			data.$set = {};
			var firstScript = sa[0] && sa[0].script;
			var initData = firstScript && require(firstScript).initData;
			if (initData) initData(_, self);
			var meta = firstScript && require(firstScript).metadata;
			data.$set[model.name] = {
				dbVersion: sa[0] && sa[0].version,
				dbVersions: sa.reduce(function(prev, us) {
					var meta = require(us.script).metadata;
					prev[meta.fileId] = {
						version: us.version,
						description: meta.description
					};
					return prev;
				}, {})
			};
			db.collection("dbParam", _).update({}, data, {
				safe: true,
				upsert: true
			}, _);
		} finally {
			self._isSynchronizing = true;
		}
	}

	function _synchronizeDb(_, model, db, actualVersion, updateMeta) {
		tracer && tracer("mongodbDbHandle.synchronize; ActualVersion: " + actualVersion + "; ModelVersion: " + updateMeta.version);
		// avoid dead-lock
		if (self._isSynchronizing) return;
		self._isSynchronizing = true;
		try {
			//if (actualVersion === -1) return;
			// data update procedure
			if ( /*(actualVersion !== -1) && */ updateMeta) {
				var updateScript = require(updateMeta.script);
				updateScript.tracer = tracer;
				updateScript.dataUpdate(_, self, actualVersion, updateMeta.version);
			}
			// update dbVersion
			if (updateMeta && updateMeta.version != null && updateMeta.version > actualVersion) {
				var scriptMeta = require(updateMeta.script).metadata;
				var data = {};
				data.$set = {};
				data.$set[model.name + ".dbVersions." + scriptMeta.fileId] = {
					version: updateMeta.version,
					description: scriptMeta.description
				};
				db.collection("dbParam", _).update({}, data, {
					safe: true,
					upsert: true
				}, _);
			}
		} finally {
			self._isSynchronizing = false;
		}
	}
	//

	function _paramsToFilter(_, parameters, entity) {
		var filter = filterHelpers.sdataFilterToJson(parameters.sdataWhere || parameters.where, {
			tracer: tracer
		});
		// fusion jsonWhere (mostly internal filter parts) in filter (mostly sdata external filter)
		if (parameters.jsonWhere) {
			if (Object.keys(filter).length) {
				filter = {
					$and: [filter, parameters.jsonWhere]
				};
			} else filter = parameters.jsonWhere;
		}
		/*		parameters.jsonWhere && forEachKey(parameters.jsonWhere, function(key, value) {
			filter[key] = value;
		});*/
		// pager
		if (parameters.letter) {
			var sort = _paramsToOrderBy(parameters, entity);
			if (sort.length) {
				filter[sort[0][0]] = sort[0][1] === "ascending" ? {
					$gte: parameters.letter
				} : {
					$lte: parameters.letter
				};
			}
		}
		if (parameters.key) {
			var sort = _paramsToOrderBy(parameters, entity);
			if (sort.length) {
				var s = sort[0][0];
				var k = parameters.key.split(".");
				filter[s] = {};
				filter[s]["$" + k[0]] = k[1];
			}
		}
		// clone filter to avoid returning modified instance
		filter = helpers.object.clone(filter, true);
		// for relations, decorate filter 
		filter = decorateFilter(_, self, entity, filter);
		// change the $uuid key if any
		if (filter._uuid) {
			filter._id = filter._uuid;
			delete filter._uuid;
		}
		//
		return filter;
	}

	function _paramsToOrderBy(parameters, entity) {
		var sort = [];
		if (parameters.orderBy && parameters.orderBy.length) {
			parameters.orderBy.forEach(function(orderBy) {
				if (orderBy.binding[0] === "$")
					orderBy.binding = "_" + orderBy.binding.substring(1);
				sort.push([orderBy.binding, (orderBy.descending ? "descending" : "ascending")]);
			});
		} else {
			if (entity.defaultOrder && entity.defaultOrder.length) {
				entity.defaultOrder.forEach(function(order) {
					sort.push([order[0], (order[1] ? "ascending" : "descending")]);
				});
			} else {
				sort.push(["$uuid", "ascending"]);
			}
		}
		return sort;
	}
	self.getFileStore = function(fileName) {
		return new MongodbFileStore(this, fileName);
	};
	//
	/// -------------
	/// ## getEntity function :
	/// ``` javascript
	/// var entity = db.getEntity(_, entityName);
	/// ```
	/// Get the class metadata as an entity
	/// 
	/// 
	self.getEntity = function(_, entityName) {
		return this.model.getEntity(_, entityName) || this.model.getEntity(_, this.model.singularize(entityName));
	};
	self.getUpdDatePropName = function() {
		return "$updDate";
	};
	// fetch instance
	self.fetchInstance = function(_, entity, uuid) {
		var filter;
		var options = {
			fields: getSelectFields(entity),
			limit: 1
		};
		if (typeof uuid === "object") {
			filter = _paramsToFilter(_, uuid, entity);
			options.sort = _paramsToOrderBy(uuid, entity);
		} else filter = {
			_id: uuid
		};
		var timing = perfmon.start(module, "mongodb.fetchInstance", entity.name + '/' + filter._id);
		tracer && tracer("mongodb.fetchInstance filter: " + sys.inspect(filter, null, 4));
		var dataArray = this.db.collection(entity.className, _).find(filter, options).toArray(_);
		timing.end();
		if (dataArray.length) {
			tracer && tracer("mongodb.fetchInstance found: " + entity.name + "\n+" + sys.inspect(uuid) + "\n+" + sys.inspect(dataArray[0]));
			dataArray[0].$key = dataArray[0]._id;
			dataArray[0].$uuid = dataArray[0]._id;
			dataArray[0].$loaded = true;
			return entity.factory.createInstance(_, unescapeJson(dataArray[0]), this);
		} else {
			tracer && tracer("mongodb.fetchInstance not found: " + entity.name + "\n+" + sys.inspect(uuid, null, 4));
			return null;
		}
	};
	// used for lazy load properties (binary or large text)
	self.fetchInstanceProperty = function(_, entity, propName, param) {
		var filter;
		var options = {
			fields: [propName],
			limit: 1
		};
		if (typeof param === "object") {
			filter = _paramsToFilter(_, param, entity);
			options.sort = _paramsToOrderBy(param, entity);
		} else filter = {
			_id: param
		};
		var timing = perfmon.start(module, "mongodb.fetchInstanceProperty", entity.name + '/' + filter._id + '/' + propName);
		var dataArray = this.db.collection(entity.className, _).find(filter, options).toArray(_);
		timing.end();
		if (dataArray.length) {
			tracer && tracer("mongodb.fetchInstanceProperty found: " + entity.name + "\n+" + param + "\n+" + sys.inspect(dataArray[0]));
			return dataArray[0][propName];
		} else {
			tracer && tracer("mongodb.fetchInstance not found: " + entity.name + "\n+" + param);
			return null;
		}
	};
	//
	self.count = function(_, entity, params) {
		var parameters = params || {};
		//
		var filter = _paramsToFilter(_, parameters, entity);
		//
		return this.db.collection(entity.className, _).count(filter, _);
	};
	//
	/// -------------
	/// ## createCursor function :
	/// ``` javascript
	/// var cursor = db.createCursor(_, entity, params, shallow);
	/// var data;
	/// while(data = cursor.next(_) {
	///   // do something with data witch is an object instance
	/// }
	/// ```
	/// Creates a cursor allowing to iterate over the objects in a collection
	/// function next(_) on the cursor returns the current instance. Returns null at the end of the cursor
	/// 
	/// ```javascript
	/// // parameters example
	/// params = {
	///   count: 20, // cursor fetch limit
	///   startIndex: 2, // skip parameter
	///   orderBy: [{binding:"name", descending: true}, {binding: title}],
	///   jsonWhere: {/* mongodb style json filter */} // or sdataWhere = sdataClause or where = parsed_expression_object
	/// }
	/// ```
	/// 
	self.createCursor = function(_, entity, params, shallow) {
		var parameters = params || {};
		tracer && tracer("mongodb.createCursor " + entity.className + " parameters: " + sys.inspect(parameters, null, 5));
		var options = {};
		if (parameters.count) options.limit = parameters.count;
		if (parameters.startIndex) options.skip = parameters.startIndex - 1;
		options.sort = _paramsToOrderBy(parameters, entity);
		options.fields = getSelectFields(entity, shallow);
		//
		var filter = _paramsToFilter(_, parameters, entity);
		//
		tracer && tracer("mongodb.createCursor filter: " + sys.inspect(filter));
		tracer && tracer("mongodb.createCursor options: " + sys.inspect(options));
		//

		return new MongodbCursor(this, entity, this.db.collection(entity.className, _).find(filter, options));
	};
	// fetch all instances acording to parameters
	self.fetchInstances = function(_, entity, params, shallow, context) {
		var timing = perfmon.start(module, "mongodb.fetchInstances", entity.name);
		var startTime = _startProfile();
		var instances = [];
		var cursor = this.createCursor(_, entity, params, shallow);
		var data;
		var currentTime = new Date().getTime();
		while (data = cursor.next(_)) {
			var isDelete = false;
			if (entity.$expire) {
				try {
					if (!data.$ttl || currentTime > (new Date(data.$ttl).getTime())) {
						isDelete = true;
						data.deleteSelf();
					}
				} catch (e) {
					tracer && tracer("$ttl doesn't contain a date for entity " + entity.name);
				}
			}
			if (!isDelete)
				instances.push(data);
		}
		tracer && tracer("mongodb.fetchInstances found " + instances.length + " " + entity.className + "; TOOK: " + _endProfile(startTime));
		//
		timing.end();
		return instances;
	} //;
	self.saveInstance = function(_, instance) {
		var self = this;
		if (instance._meta.$sequentialStorage) {
			var funnel = _funnels[instance._meta.name];
			if (!funnel) funnel = _funnels[instance._meta.name] = flows.funnel(1);
			return funnel(_, function(_) {
				return _internalSave(_, instance, self.db);
			});
		} else return _internalSave(_, instance, self.db);
	};
	// helper function

	function _getDirtyProperty(_, data, $p, value, path) {
		if (!$p.$compute) {
			if (value && (typeof value === "object")) value = escapeJson(value);
			if ($p.$isLocalized) {
				value && Object.keys(value).forEach(function(loc) {
					data.$set[path + "." + loc] = value[loc];
				});
			} else if ($p.isExternalStorage()) {
				// !!! value is escaped, so use _uuid, instead of $uuid !!!
				if (value && value._uuid) data.$set[path + "._uuid"] = value._uuid;
				else data.$set[path] = {};
			} else if ($p.$encrypt) { // encrypt
				base64 = base64 || require('syracuse-license').load('license');
				// console.log("ENCRYPT " + value.toString());
				if (value.toString().length >= 64) throw new Error(locale.format(module, "cannotEncrypt", value.toString().length));
				data.$set[path] = base64.license(0, value.toString(), new Boolean(true));
				if (data.$set[path] === null || data.$set[path] === undefined) throw new Error("Old version of license module");
				// console.log("CRYPTED " + data.$set[path]);
			} else data.$set[path] = value;
		}
	}

	function _getDirtyPlural(_, data, pull, $r, value, path) {
		// array storage function: value is the array to be persisted
		// need to escape the childrens
		data.$set[path] = value && escapeJson(value);
	}

	function _getDirtyProps(_, data, pull, delta, meta, path) {
		var locPath = (path ? (path + ".") : "");
		flows.eachKey(_, delta, function(_, key, value) {
			// escaped key
			var escKey = syracuseEscapeMap[key] || key;
			//
			if (key === "$index") data.$set[locPath + escKey] = value;
			if (key === "$variantType") data.$set[locPath + escKey] = value;
			if ((key === "$signature") && value) data.$set[locPath + escKey] = value;
			// is a property ?
			var $p = meta.$properties && meta.$properties[key];
			$p && _getDirtyProperty(_, data, $p, value, locPath + escKey);
			// is a relation ?
			var rel = meta.$relations && meta.$relations[key];
			if (rel && !rel.$compute && !rel.isComputed) {
				if (meta.$relations[key].isPlural) {
					_getDirtyPlural(_, data, pull, rel, value, locPath + escKey);
				} else {
					if (rel.getIsChild(value && value.$variantType) || rel.$inlineStore) {
						if (!value) {
							//										throw new Error("Cannot save null child for relation: "+key);
							data.$set[locPath + escKey] = {};
						} else {
							_getDirtyProps(_, data, pull, value, rel.getTargetEntity(value && value.$variantType), locPath + escKey);
							data.$set[locPath + escKey + "." + syracuseEscapeMap["$uuid"]] = value.$uuid;
						}
					} else {
						if (value) {
							if (value.$url) data.$set[locPath + escKey + "." + syracuseEscapeMap["$url"]] = value.$url;
							if (value.$variantType) data.$set[locPath + escKey + "." + syracuseEscapeMap["$variantType"]] = value.$variantType;
							if (value.$uuid) data.$set[locPath + escKey + "." + syracuseEscapeMap["$uuid"]] = value.$uuid;
						} else data.$set[locPath + escKey] = {};
					}
				}
			}
			//					}
			//				}
		});
	}
	// persist instance and its children

	function _internalSave(_, instance, db) {
		var data = {
			$set: {}
		};
		var pull = {};
		//
		var _opRef = opRef++;
		tracer && tracer("mongodb.save enter: opRef=" + _opRef);
		//
		if (instance._snapshotEnabled) var saveDelta = instance.getSaveSnapshotDelta(_);
		else var saveDelta = instance.serializeInstance(_);
		tracer && tracer("mongodb.save delta: opRef=" + _opRef + "; " + sys.inspect(saveDelta));
		//
		_getDirtyProps(_, data, pull, saveDelta, instance._meta);
		// technical meta
		var updDate = new Date();
		if (instance.$created) {
			data.$set._creUser = instance.$creUser;
			data.$set._creDate = instance.$creDate = updDate;
		}
		data.$set._updUser = instance.$updUser;
		// TODO: for now, mongodb doesn't seems to accept javascript in update, so we'll send the Syracuse server date
		// It would be best if it was the mongodb server date 
		data.$set._updDate = instance.$updDate = updDate;
		// save global UUID (remove existing syncUuid only when requested - this is to avoid removal of syncUuid when
		// an instance is stored and during storing, the synchronization digest is formed and syncUuids are assigned to the instances
		if (instance.$syncUuid || instance._deleteSyncUuid) {
			instance._deleteSyncUuid = false;
			data.$set._syncUuid = instance.$syncUuid;
		}
		if (instance.$stamp) {
			data.$set._stamp = new Date(instance.$stamp);
		}
		data.$set._tick = instance.$tick;
		data.$set._endpoint = instance.$endpoint;
		// manage time to live 
		if (instance._meta.$expire) {
			var millisec = instance._meta.$expire(_, instance);
			if (millisec && millisec > 0) {
				var curTime = (new Date()).getTime();
				data.$set._expire = instance.$expire = new Date(curTime + millisec);
			}
		}
		//
		var collection = db.collection(instance.getClassName(), _);
		var instanceFilter = (instance._meta.$lockType && (instance._meta.$lockType === "noLock")) ? {
			_id: instance.$uuid
		} : {
			_id: instance.$uuid,
			$or: [{
				_updDate: instance.$initialUpdDate
			}, {
				_updDate: null
			}]
		};
		// crnit110825
		// in current mongodb version there is an issue making an atomic $pull and $addToSet on the same array
		// http://jira.mongodb.org/browse/SERVER-1050
		// this should be fixed but for now we'll make a distinct pull operation before
		// after mongodb fixes this, add pull to data and make just one call to update
		if (pull && (Object.keys(pull).length > 0)) {
			tracer && tracer("mongodb.save data pull: opRef=" + _opRef + "; " + instance.getClassName() + ";" + sys.inspect(pull));
			collection.update(instanceFilter, pull, {
				safe: true
			}, _);
		}
		//
		tracer && tracer("mongodb.save data: opRef=" + _opRef + "; " + instance.getClassName() + ";" + sys.inspect(data, null, 4));
		// TODO: if result is 0, detail error conditions
		var result;
		if (instance.$created) {
			// creation, do not check concurrency conditions. Use update because of the common use of $set for insert and for update.
			result = collection.update({
				_id: instance.$uuid
			}, data, {
				safe: true,
				upsert: true
			}, _);
		} else {
			tracer && tracer("mongodb.save data instanceFilter: opRef=" + _opRef + "; " + sys.inspect(instanceFilter));
			// check concurrency with updDate
			result = collection.update(instanceFilter, data, {
				safe: true,
				upsert: false
			}, _);
		}
		// keep the count
		if (result) instance.$initialUpdDate = instance.$updDate;
		tracer && tracer("mongodb.save data result: opRef=" + _opRef + "; " + result);
		return result;
	}

	//
	self.deleteInstance = function(_, instance) {
		var self = this;
		if (instance._meta.$sequentialStorage) {
			var funnel = _funnels[instance._meta.name];
			if (!funnel) funnel = _funnels[instance._meta.name] = flows.funnel(1);
			return funnel(_, function(_) {
				return _deleteInstance(_, instance, self.db);
			});
		} else return _deleteInstance(_, instance, self.db);
	};
	// delete instance and its childs

	function _deleteInstance(_, instance, db) {
		tracer && tracer("mongodb.deleteInstance: " + instance.$uuid);
		return db.collection(instance.getClassName(), _).remove({
			_id: instance.$uuid
		}, {
			safe: true
		}, _);
	}

	// atomicaly create a instance lock
	self.lockInstance = function(_, instance) {
		if (!instance) return null;
		if (!instance.$uuid) return null;
		var session = globals.context.session;
		var userLogin = (session && session.getUserLogin(_)) || "anonymous";
		var ssid = (session && session.id) || processId;
		// try un upsert with _id=instance.$uuid and currect sessionId. If allready locked, returns count of 0
		var lockId = instance.$uuid;
		try {
			var coll = this.db.collection("dbLocks", _);
			var res = coll.update({
				_id: lockId,
				sessionId: ssid
			}, {
				$set: {
					sessionId: ssid,
					lockDate: new Date(),
					lockUser: userLogin
				}
			}, {
				safe: true,
				upsert: true
			}, _);
			if (res != 0) return {
				status: "success",
				id: lockId
			};
		} catch (ex) {
			// return null;
			// pk violation, allready locked
		}
		// read the lock record to return meta
		var locks = coll.find({
			_id: lockId
		}).toArray(_);
		if (locks && locks[0]) return {
			status: "locked",
			lock: locks[0]
		};
		else return {
			status: "error"
		};
	};
	self.unlockInstance = function(_, instance) {
		this.db.collection("dbLocks", _).remove({
			_id: instance.$uuid
		}, {
			safe: true
		}, _);
	};
	// atomicaly create a instance lock
	self.lockDatabase = function(_) {
		tracer && tracer("Mongodb locking database");
		var session = globals.context.session;
		var userLogin = (session && session.getUserLogin(_)) || "internal";
		var ssid = self._lockSid = self._lockSid || helpers.uuid.generate();
		// try un upsert with _id=instance.$uuid and currect sessionId. If allready locked, returns count of 0
		var lockId = "database";
		try {
			var coll = this.db.collection("dbLocks", _);
			var res = coll.update({
				_id: lockId,
				sessionId: ssid
			}, {
				$set: {
					sessionId: ssid,
					lockDate: new Date(),
					lockUser: userLogin
				}
			}, {
				safe: true,
				upsert: true
			}, _);
			if (res != 0) {
				tracer && tracer("Mongodb lock database: success");
				return {
					status: "success",
					id: lockId
				};
			}
		} catch (ex) {
			// return null;
			// pk violation, allready locked
		}
		// read the lock record to return meta
		var locks = coll.find({
			_id: lockId
		}).toArray(_);
		if (locks && locks[0]) return {
			status: "locked",
			lock: locks[0]
		};
		else return {
			status: "error"
		};
	};
	// lock the database, but retry it when it does not work out for the first time
	self.lockDatabaseRetry = function(_) {
		var start = new Date();
		var lock = self.lockDatabase(_);
		while ((lock.status !== "success") && (((new Date()) - start) < 60000)) {
			// wait some time, 50ms min
			setTimeout(~_, Math.floor(Math.random() * 10000) + 50);
			//
			lock = self.lockDatabase(_);
		}
		if (lock.status !== "success") throw new Error(locale.format(module, "databaseLockTimeout"));
		return lock;
	};
	self.unlockAll = function(_) {
		tracer && tracer("Mongodb unlocking all database and instance locks");
		this.db.collection("dbLocks", _).remove({}, {
			safe: true
		}, _);
		this._lockId = null;
	};
	self.unlockDatabase = function(_) {
		tracer && tracer("Mongodb unlocking database");
		this.db.collection("dbLocks", _).remove({
			_id: "database"
		}, {
			safe: true
		}, _);
		this._lockId = null;
	};
	//
	/// -------------
	/// ## getCounterValue function :
	/// ``` javascript
	/// var value = db.getCounterValue(_, domain, name, options);
	/// ```
	/// manages counters with model domain and name as unique key. 
	/// options:
	/// * value: if set it will update the counter value to the given value
	/// * increment: if set counter value will be increment by the given value
	/// * data: object that will be saved with the counter. Update of this is differential, meaning that property not in object
	///     aren't updated. To delete a property it must be set to null
	/// if (!value && !increment) existing value is returned without modification
	/// data is updated even if counter value is not modified
	///
	/// return value is in form of:
	/// ``` javascript
	/// {
	///   value: 152,
	///   data: {
	///     "...": "..."
	///   }
	/// }
	/// 
	self.getCounterValue = function(_, counterDomain, counterName, options) {
		function _buildSet(obj, prefix) {
			Object.keys(obj).forEach(function(key) {
				var val = obj[key];
				if (val && (typeof val === "object")) _buildSet(val, prefix + "." + key);
				else upd.$set[prefix + "." + key] = val;
			});
		}
		var counter;
		var opt = options || {};
		var cntColl = this.db.collection("dbCounters", _);
		var key = {
			model: model.name,
			domain: counterDomain,
			name: counterName
		};
		var upd = {};
		// value
		if (opt.increment) {
			upd.$inc = {
				value: opt.increment
			};
		} else if (opt.value) {
			upd.$set = upd.$set || {};
			upd.$set.value = opt.value;
		}
		// data
		if (opt.data && (typeof opt.data === "object")) {
			upd.$set = upd.$set || {};
			_buildSet(opt.data, "data");
		}
		//
		if (opt.increment || opt.value || opt.data) {
			counter = cntColl.findAndModify(key, null, upd, {
				upsert: true,
				"new": true
			}, _);
		} else {
			counter = cntColl.find(key).toArray(_)[0];
		}
		return counter;
	};
	//
	/// -------------
	/// ## pushObjectAction function :
	/// ``` javascript
	/// db.pushObjectAction(_, operation, id, url);
	/// ```
	/// Append a document to scheduled operations collection. A scheduler should consume this collection and apply operations 
	/// 
	self.pushObjectAction = function(_, operation, id, url) {
		return this.db.collection("dbScheduledOperations", _).findAndModify({
			_uuid: id,
			operation: operation
		}, null, {
			_uuid: id,
			operation: operation,
			url: url,
			timestamp: Date.now()
		}, {
			upsert: true
		}, _);
	};
	//
	/// -------------
	/// ## popObjectAction function :
	/// ``` javascript
	/// db.popObjectAction(_, operation);
	/// ```
	/// Append a document to scheduled operations collection. A scheduler should consume this collection and apply operations 
	/// 
	self.popObjectAction = function(_, operation) {
		var key = operation ? {
			operation: operation
		} : null;
		return this.db.collection("dbScheduledOperations", _).findAndModify(key, {
			timestamp: 1
		}, null, {
			remove: true
		}, _);
	};
}

helpers.defineClass(MongoDbHandle);

exports.create = function(_, model, dataset) {
	var handle = new MongoDbHandle(model, dataset);
	handle.connect(_);
	return handle;
};

//
exports.setup = function(mongodbConfig, mongoConnPool) {
	tracer = mongodbConfig && mongodbConfig.tracer;
	connectionPool = mongoConnPool;
};