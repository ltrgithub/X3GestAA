"use strict";

/// !doc
/// # Mongodb handle API  
/// usually this object is already instantiated on the context
/// ```javascript
/// var dataModel = require("syracuse-orm/lib/dataModel");
/// var model = dataModel.make(...);
/// // now get the mongodb handle
/// var db = dataModel.getOrm(_, model, dataset);  
/// ```
/// 

var mongodb = require('streamline-mongodb');
var helpers = require("syracuse-core/lib/helpers");
var forEachKey = helpers.object.forEachKey;
var flows = require("streamline/lib/util/flows");
var locale = require("syracuse-core/lib/locale");
var globals = require('streamline/lib/globals');
var filterHelpers = require("./../filters");
var streams = require("streamline/lib/streams/streams");
var ReadableStream = streams.ReadableStream;
var WritableStream = streams.WritableStream;
var sys = require("util");
var perfmon = require("syracuse-perfmon");
var processId = require('os').hostname() + "$_" + process.pid;

var config = {
	tracer: console.log
};
var connectionPool = null;
var opRef = 0;

var _funnels = {};

var syracuseEscapeMap = {
	$uuid: "_uuid",
	$url: "_url",
	$index: "_index",
	$keys: "_keys",
	$type: "_type",
	$syncUuid: "_syncUuid",
	$variantType: "_variantType",
	$signature: "_signature",
	$updDate: "_updDate",
	$updUser: "_updUser",
	$creDate: "_creDate",
	$creUser: "_creUser",
	$tick: "_tick",
	$endpoint: "_endpoint"
};
var syracuseUnescapeMap = {
	_uuid: "$uuid",
	_url: "$url",
	_index: "$index",
	_keys: "$keys",
	_type: "$type",
	_syncUuid: "$syncUuid",
	_variantType: "$variantType",
	_signature: "$signature",
	_updDate: "$updDate",
	_updUser: "$updUser",
	_creDate: "$creDate",
	_creUser: "$creUser",
	_tick: "$tick",
	_endpoint: "$endpoint"
};

// escape :
// $ by \u007F

function escapeArray(input) {
	return input.map(function(item) {
		switch (typeof item) {
			case "object":
				return escapeJson(item);
			case "string":
				return (item[0] === "$") ? ("\u007F" + item.substring(1)) : item;
			default:
				return item;
		}
	});
}

function escapeJson(input) {
	// must clone as original object might be used later unescaped
	if (Array.isArray(input)) return escapeArray(input);
	//
	var out = {};
	forEachKey(input, function(key, value) {
		var escKey = key;
		var escVal = value;
		if (value !== null) {
			if (Array.isArray(value)) {
				escVal = escapeArray(value);
			} else if (typeof value === "object") escVal = escapeJson(value);
			if (syracuseEscapeMap[key]) escKey = syracuseEscapeMap[key];
			else {
				if (key[0] === "$") {
					escKey = "\u007F" + key.substring(1);
					//			escKey = "Ã©"+key.substring(1); 
				}
			}
		}
		out[escKey] = escVal;
	});
	//
	return out;
}
// unescape :
// \u007F by $

function unescapeJson(input) {
	forEachKey(input, function(key, value) {
		var escKey = key;
		var escVal = value;
		if (Array.isArray(value)) {
			escVal = [];
			value.forEach(function(item) {
				if (typeof item === "object") escVal.push(unescapeJson(item));
				else if (typeof item === "string") escVal.push((item[0] === "\u007F") ? ("$" + item.substring(1)) : item);
				else escVal.push(item);
			});
		} else if (typeof value === "object") escVal = unescapeJson(value);
		if (syracuseUnescapeMap[key]) {
			escKey = syracuseUnescapeMap[key];
			delete input[key];
		} else {
			if (key[0] === "\u007F") {
				escKey = "$" + key.substring(1);
				delete input[key];
			}
		}
		input[escKey] = escVal;
	});
	//	console.log("unescaped json: "+sys.inspect(input,null,4));
	return input;
}

function _startProfile() {
	return (new Date()).getTime();
}

function _endProfile(startTime) {
	return (new Date()).getTime() - startTime;
}

function decorateFilter(_, db, entity, filter) {
	function _makeTerm(key, loc, value) {
		var locKey = key.slice(0);
		locKey.push(loc);
		locKey = locKey.join(".");
		var term = {};
		term[locKey] = value;
		return term;
	}

	function _isLocalizedProp(ent, propName) {
		return ent.$properties && ent.$properties[propName] && ent.$properties[propName].$isLocalized;
	}
	flows.eachKey(_, filter, function(_, key, value) {
		if (Array.isArray(value)) {
			value.forEach_(_, function(_, innerFilter) {
				decorateFilter(_, db, entity, innerFilter);
			});
		} else {
			var newKey = null;
			if (key === "$uuid") {
				newKey = "_id";
			} else {
				var sKey = key.split(".");
				if (sKey[sKey.length - 1] === "$uuid") {
					sKey.pop(); // if last part is $uuid ignore it (is added automatically)
				}
				var lastPart = sKey[sKey.length - 1];
				var keysLen = sKey.length;
				// walk
				var targetEnt = entity;
				var rels = [];
				var crtRel = null;
				sKey.forEach(function(part) {
					if (targetEnt.$relations[part]) {
						crtRel = targetEnt.$relations[part];
						targetEnt = targetEnt.$relations[part].targetEntity;
					} else crtRel = null;
					rels.push(crtRel);
				});
				// last part is a relation
				if (crtRel) {
					if (crtRel.isPlural) {
						newKey = sKey.join(".") + "." + syracuseEscapeMap["$uuid"];
						if (value == null) value = {
							$in: [
								[], null
							]
						};
					} else newKey = sKey.join(".") + "." + syracuseEscapeMap["$uuid"];
				} else {
					// lastPart is not a relation but a join to a reference (not child)
					if ((keysLen > 1) && (sKey[keysLen - 1]) && rels[keysLen - 2] && !rels[keysLen - 2].getIsChild()) {
						var rel = rels[keysLen - 2];
						var f = {};
						f[lastPart] = value;
						var js = db.fetchInstances(_, rel.targetEntity, {
							jsonWhere: f
						});
						value = {
							$in: js.map(function(j) {
								return j.$uuid;
							})
						};
						sKey[keysLen - 1] = "_uuid";
						newKey = sKey.join(".");
					} else {
						// case sensitive
						if (targetEnt.$properties && targetEnt.$properties[lastPart] && targetEnt.$properties[lastPart].$caseInsensitive)
							if (typeof value === "string") {
								// escape special characters of regular expressions
								var valueEsc = value.replace(/([\\\^\$\.\(\)\[\]\{\}\*\?\+])/g, "\\$1");
								filter[key] = value = {
									$regex: "^" + valueEsc + "$",
									$options: "i"
								};
							}
							// localization
						if (_isLocalizedProp(targetEnt, lastPart)) {
							if (value != null) {
								var locKey = sKey + "." + locale.current.toLowerCase();
								var t_eq = {};
								t_eq[locKey] = null;
								var t_neq = {};
								t_neq[locKey] = {
									$ne: null
								};
								//
								newKey = "$or";
								value = [{
									$and: [t_neq, _makeTerm(sKey, locale.current.toLowerCase(), value)]
								}, {
									$and: [t_eq, _makeTerm(sKey, "default", value)]
								}];
							}
						} else {
							// is last part the locale code of a localized prop ?
							if (_isLocalizedProp(targetEnt, sKey[sKey.length - 2])) {
								sKey[sKey.length - 1] = sKey[sKey.length - 1].toLowerCase();
								newKey = sKey.join(".");
							} else {
								// last part is property or technical
								if (sKey.length && syracuseEscapeMap[lastPart]) {
									sKey[sKey.length - 1] = syracuseEscapeMap[lastPart];
									newKey = sKey.join(".");
								}
							}
						}
					}
				}
			}
			if (newKey && (newKey !== key)) {
				filter[newKey] = value;
				delete filter[key];
			}
		}
	});
	return filter;
}

function getSelectFields(entity, shallow) {
	var fields = [];
	//	var meta = entity.deepMeta;
	var meta = entity;
	forEachKey(meta.$properties, function(name, prop) {
		if (prop.$isComputed || prop.$isLazy) return;
		fields.push(name);
	});
	forEachKey(meta.$relations, function(name, rel) {
		if (rel.$isComputed || rel.$isLazy) return;
		fields.push(name);
	});
	// add technical fields
	fields.splice(0, 0, "_creUser", "_creDate", "_updUser", "_updDate", "_signature", "_syncUuid", "_tick", "_endpoint");
	//
	return fields;
}

// cursor

function _nextObject(cursor, _) {
	return cursor.nextObject(_);
}

function MongodbCursor(db, entity, cursor) {
	this._cursor = cursor;
	this._entity = entity;
	this._db = db;
	this._dataFuture = _nextObject(this._cursor, !_);
}
var cursorProto = MongodbCursor.prototype;
cursorProto.next = function(_) {
	var self = this;
	var data;
	if (data = self._dataFuture(_)) {
		self._dataFuture = _nextObject(self._cursor, !_);
		data.$key = data._id;
		data.$uuid = data._id;
		data.$loaded = true;
		return self._entity.factory.createInstance(_, unescapeJson(data), self._db);
	} else return null;
};

// stores
// writable stream wrapper for grid store

function GridWritableStream(store) {
	this.store = store;
}
var gdWsProto = GridWritableStream.prototype;
gdWsProto.write = function(_, buffer, enc) {
	if (buffer == null)
	// means end
		this.store.close(_);
	else this.store.write(buffer, _);
};
// store constructor

function MongodbFileStore(db, fileName) {
	this.db = db.db;
	this.fileName = fileName;
	this.readStore = null;
	this.writeStore = null;
	this.readPosition = 0;
}
//

function _openStore(_, fileStore, mode) {
	var store = new mongodb.GridStore(fileStore.db, fileStore.fileName, mode);
	return store.open(_);
}
var storeProto = MongodbFileStore.prototype;
storeProto.getProperties = function(_) {
	var store = this.readStore || _openStore(_, this, "r");
	return {
		length: store.length,
		contentType: store.contentType,
		fileName: store.metadata && store.metadata.fileName,
		uploadDate: store.uploadDate,
		chunkSize: store.chunkSize
	};
};
storeProto.fileExists = function(_) {
	if (!this.fileName) return false;
	return mongodb.GridStore.exist(this.db, this.fileName, _);
};
storeProto.setFile = function(_, fileName) {
	this.close(_);
	//
	this.fileName = fileName;
};
// stream interface
storeProto.createReadableStream = function(_) {
	// force open a file store to reset stream read position
	config.tracer && config.tracer("mongodbDbHandle.GridFS open store for read: " + this.fileName);
	console.log("mongodbDbHandle.GridFS open store for read: " + this.fileName);
	return (new ReadableStream((new mongodb.GridStore(this.db, this.fileName, "r")).open(_).stream(true)));
};
storeProto.createWritableStream = function(_, options) {
	var store = new mongodb.GridStore(this.db, this.fileName, "w").open(_);
	//
	store.metadata = store.metadata || {};
	if (options) {
		if (options.contentType) store.contentType = options.contentType;
		if (options.fileName) store.metadata.fileName = options.fileName;
		//
		if (options.referingInstance) store.metadata.referingInstance = options.referingInstance;
	}
	var etag = store.metadata.etag || 0;
	store.metadata.etag = ++etag;
	//
	return (new GridWritableStream(store));
};
storeProto.writeMetadata = function(_, options) {
	var store = new mongodb.GridStore(this.db, this.fileName, "w+").open(_);
	//
	store.metadata = store.metadata || {};
	if (options) {
		if (options.contentType) store.contentType = options.contentType;
		if (options.fileName) store.metadata.fileName = options.fileName;
		//
		if (options.referingInstance) store.metadata.referingInstance = options.referingInstance;
	}
	var etag = store.metadata.etag || 0;
	store.metadata.etag = ++etag;
	store.close(_);
};
storeProto.deleteFile = function(_) {
	if (this.fileExists(_)) mongodb.GridStore.unlink(this.db, this.fileName, _);
};
storeProto.close = function(_) {};

function MongoDbHandle(model, dataset) {
	var self = this;
	self.model = model;
	self.dataset = dataset;
	self.baseUrl = ["/sdata", model.contract.application, model.contract.contract, dataset.database].join("/");

	self.escapeJson = function(input) {
		return escapeJson(input);
	};
	self.unescapeJson = function(input) {
		return unescapeJson(input);
	};
	// initialize the database connection
	self.connect = function(_) {
		function _mustSync(_, params) {
			var sa = Array.isArray(model.dbMeta.updateScript) ? model.dbMeta.updateScript : [{
				script: model.dbMeta.updateScript,
				version: model.dbMeta.version
			}];
			return sa.some(function(us) {
				var meta = require(us.script).metadata || {};
				if (!meta.fileId) return;
				var actualVersion = ((params[meta.fileId] || {}).version != null) ? params[meta.fileId].version : -1;
				return (actualVersion !== (us.version || 0));
			});
		}
		var self = this;
		var host = dataset.hostname;
		var port = dataset.port;
		//		var host = "localhost";
		//		var port = 27017;
		var key = host + '/' + port + '/' + dataset.database + '/' + globals.context.tenantId;
		if (connectionPool && connectionPool[key]) {
			self.db = connectionPool[key];
			config.tracer && config.tracer("Mongodb connect - pool connection for : " + host + ":" + port + "/" + dataset.database + " in state: " + self.db.state);
			if (self.db.state != "connected") self.db.open(_);
			return;
		}
		// create a connection
		//		config.tracer && config.tracer("Mongodb connect : "+dataset.hostname+":"+dataset.port);
		config.tracer && config.tracer("Mongodb connect : " + host + ":" + port);
		var server = new mongodb.Server(host, port, {});
		var dbname = globals.context.tenantId ? globals.context.tenantId + '-' + dataset.database : dataset.database;
		var db = new mongodb.Db(dbname, server, {
			w: "majority"
		});
		db = db.open(_);
		self.db = db;
		// add to pool BEFORE synchronize, because sync scripts might use adminUtil and request the admin orm
		if (connectionPool) connectionPool[key] = db;
		// synchronize start
		// force unlock resque procedure
		if (process.argv[2] === "--dbUnlock") self.unlockDatabase(_);
		// find out whether syncrhonization is necessary
		var sync = false;
		var params = db.collection("dbParam", _).find().toArray(_);
		if (params && params.length) {
			var p = params[0];
			config.tracer && config.tracer("mongodbDbHandle.connect ModelName: " + model.name + "; found dbParam: " + sys.inspect(p));
			// structure change >>> : if dbVersion is not object, assume version number is the one of the first script in update scripts array
			var pm = p[model.name] = p[model.name] || {};
			if ((pm.dbVersion != null) && (typeof pm.dbVersion !== "object")) {
				var firstScript = Array.isArray(model.dbMeta.updateScript) ? model.dbMeta.updateScript[0].script : model.dbMeta.updateScript.script;
				var meta = require(firstScript).metadata;
				var ver = pm.dbVersion;
				pm.dbVersion = {};
				pm.dbVersion[meta.fileId] = {
					version: ver,
					description: meta.description
				}
				var data = {};
				data.$set = {};
				data.$set[model.name] = {
					dbVersion: pm.dbVersion
				};
				db.collection("dbParam", _).update({}, data, {
					safe: true,
					upsert: true
				}, _);
			}
			// structure change <<<
			sync = _mustSync(_, pm);
		} else sync = true;
		if (sync) {
			config.tracer && config.tracer("Synchronization seems to be necessary. Lock the database");
			// make sure there is not another operation pending
			var start = new Date();
			var lock = self.lockDatabase(_);
			while ((lock.status !== "success") && (((new Date()) - start) < 60000)) {
				// wait some time, 50ms min
				setTimeout(~_, Math.floor(Math.random() * 10000) + 50);
				//
				lock = self.lockDatabase(_);
			}
			if (lock.status !== "success") throw new Error(locale.format(module, "databaseLockTimeout"));
			//
			try {
				// read dbParam again to find out whether synchronization is still necessary
				var pm;
				var params = db.collection("dbParam", _).find().toArray(_);
				if (params && params.length) {
					var p = params[0];
					pm = p[model.name] = p[model.name] || {};
				}
				if (pm && pm.dbVersion) {
					config.tracer && config.tracer("mongodbDbHandle.connect ModelName: " + model.name + "; found dbParam: " + sys.inspect(p));
					var sa = Array.isArray(model.dbMeta.updateScript) ? model.dbMeta.updateScript : [{
						script: model.dbMeta.updateScript,
						version: model.dbMeta.version
					}];
					// execute update for each script
					sa.forEach_(_, function(_, us) {
						var meta = require(us.script).metadata || {};
						if (!meta.fileId) return;
						var actualVersion = ((pm.dbVersion[meta.fileId] || {}).version != null) ? pm.dbVersion[meta.fileId].version : -1;
						config.tracer && config.tracer("mongodbDbHandle.connect ActualVersion: " + meta.fileId + "." + actualVersion);
						if (actualVersion !== (us.version || 0)) {
							_synchronizeDb(_, model, db, actualVersion, us);
						}
					});
				} else {
					_initializeDb(_, model, db);
					_setExpireEntities(_, model, db);
				}
			} finally {
				self.unlockDatabase(_);
			}
		}
	};
	//

	function _getUpdateScripts(model) {
		return Array.isArray(model.dbMeta.updateScript) ? model.dbMeta.updateScript : [{
			script: model.dbMeta.updateScript,
			version: model.dbMeta.version
		}];
	}
	
	self.ensureExpireIndex = function(_, entity) {
		var db = self.db;
		if (entity.$expire) {
			// check if index already exists  
			var col = db.collection(entity.className, _);
			var indexes = col.indexInformation({
				full: true
			}, _);
			var found = false;
			for (var i = 0; i < indexes.length && !found; i++) {
				found = (indexes[i].key["_expire"]);
			}
			if (!found) {
				col.ensureIndex({
					"_expire": 1
				}, {
					expireAfterSeconds: 0
				});
			}
		}
	};

	function _setExpireEntities(_, model, db) {
		var entities = model.getEntities();
		if (entities) {
			for (var name in entities) self.ensureExpireIndex(_, entities[name]);
		}
	}
	
	self.updateEntityIndex = function(_, entity) {
		var db = self.db;
		if (entity.$indexes)
			for (var idxName in entity.$indexes) {
				var indexItem = entity.$indexes[idxName];
				var fields = {};
				var isUnique = false;
				for (var i in indexItem) {
					if (i !== "$unique") fields[i] = ((indexItem[i] === "descending") || (indexItem[i] == -1)) ? -1 : 1;
				}
				//
				config.tracer && config.tracer("mongodbDbHandle.synchronize ensure index: " + entity.className + "." + sys.inspect(fields));
				db.collection(entity.className, _).ensureIndex(fields, indexItem.$unique, _);
			}
		entity.$uniqueConstraints && entity.$uniqueConstraints.forEach_(_, function(_, uc) {
			var fields = {};
			uc.forEach(function(f) {
				fields[f] = 1;
			});
			config.tracer && config.tracer("mongodbDbHandle.synchronize ensure unique index: " + entity.className + "." + sys.inspect(fields));
			db.collection(entity.className, _).ensureIndex(fields, true, _);
		});
	}

	function _initializeDb(_, model, db) {
		function _applyInitScript(_, script) {
			importHandler.jsonImport(_, self, script, {
				tracer: config.tracer,
				importMode: "update", // leave update because of include before / after, we can modify even in intialization
				$diagnoses: diag
			});
		}
		if (self._isSynchronizing) return;
		self._isSynchronizing = true;
		try {
			// db init
			if (model.dbMeta && model.dbMeta.initScript) {
				config.tracer && config.tracer("mongodbDbHandle.apply initialize script: " + model.dbMeta.initScript);
				var diag = [];
				var importHandler = require("syracuse-import/lib/jsonImport");
				if (Array.isArray(model.dbMeta.initScript)) {
					model.dbMeta.initScript.forEach_(_, function(_, scr) {
						_applyInitScript(_, scr);
					});
				} else _applyInitScript(_, model.dbMeta.initScript);
				if (diag.length) config.tracer && config.tracer("mongodbDbHandle.apply initialize script errors: " + sys.inspect(diag, null, 4));
			}
			// create indexes
			var entities = model.getEntities();
			if (entities) //
				for (var name in entities) {
					var entity = entities[name];
					self.updateEntityIndex(_, entity);
					//_ensureExpireIndex(_, entity, db);
				}
			// update dbParam
			var sa = _getUpdateScripts(model);
			var data = {};
			data.$set = {};
			data.$set[model.name] = {
				dbVersion: sa.reduce(function(prev, us) {
					var meta = require(us.script).metadata;
					return prev[meta.fileId] = {
						version: us.version,
						description: meta.description
					}
				}, {})
			};
			db.collection("dbParam", _).update({}, data, {
				safe: true,
				upsert: true
			}, _);
		} finally {
			self._isSynchronizing = true;
		}
	}
	function _synchronizeDb(_, model, db, actualVersion, updateMeta) {
		config.tracer && config.tracer("mongodbDbHandle.synchronize; ActualVersion: " + actualVersion + "; ModelVersion: " + updateMeta.version);
		// avoid dead-lock
		if (self._isSynchronizing) return;
		self._isSynchronizing = true;
		try {
			if (actualVersion === -1) return;
			// data update procedure
			if ((actualVersion !== -1) && updateMeta) {
				var updateScript = require(updateMeta.script);
				updateScript.tracer = config.tracer;
				updateScript.dataUpdate(_, self, actualVersion, updateMeta.version);
			}
			// update dbVersion
			if (updateMeta && updateMeta.version != null) {
				var scriptMeta = require(updateMeta.script).metadata;
				var data = {};
				data.$set = {};
				data.$set[model.name + ".dbVersion." + scriptMeta.fileId] = {
					version: updateMeta.version,
					description: scriptMeta.description
				};
				db.collection("dbParam", _).update({}, data, {
					safe: true,
					upsert: true
				}, _);
			}
		} finally {
			self._isSynchronizing = false;
		}
	}
	//

	function _paramsToFilter(_, parameters, entity) {
		var filter = filterHelpers.sdataFilterToJson(parameters.sdataWhere || parameters.where, config);
		// fusion jsonWhere (mostly internal filter parts) in filter (mostly sdata external filter)
		if (parameters.jsonWhere) {
			if (Object.keys(filter).length) {
				filter = {
					$and: [filter, parameters.jsonWhere]
				};
			} else filter = parameters.jsonWhere;
		}
		/*		parameters.jsonWhere && forEachKey(parameters.jsonWhere, function(key, value) {
			filter[key] = value;
		});*/
		// pager
		if (parameters.letter) {
			var sort = _paramsToOrderBy(parameters, entity);
			if (sort.length) {
				filter[sort[0][0]] = sort[0][1] === "ascending" ? {
					$gte: parameters.letter
				} : {
					$lte: parameters.letter
				};
			}
		}
		if (parameters.key) {
			var sort = _paramsToOrderBy(parameters, entity);
			if (sort.length) {
				var s = sort[0][0];
				var k = parameters.key.split(".");
				filter[s] = {};
				filter[s]["$" + k[0]] = k[1];
			}
		}
		// clone filter to avoid returning modified instance
		filter = helpers.object.clone(filter, true);
		// for relations, decorate filter 
		filter = decorateFilter(_, self, entity, filter);
		// change the $uuid key if any
		if (filter._uuid) {
			filter._id = filter._uuid;
			delete filter._uuid;
		}
		//
		return filter;
	}

	function _paramsToOrderBy(parameters, entity) {
		var sort = [];
		if (parameters.orderBy && parameters.orderBy.length) {
			parameters.orderBy.forEach(function(orderBy) {
				sort.push([orderBy.binding, (orderBy.descending ? "descending" : "ascending")]);
			});
		} else {
			if (entity.defaultOrder && entity.defaultOrder.length) {
				entity.defaultOrder.forEach(function(order) {
					sort.push([order[0], (order[1] ? "ascending" : "descending")]);
				});
			} else {
				sort.push(["$uuid", "ascending"]);
			}
		}
		return sort;
	}
	self.getFileStore = function(fileName) {
		return new MongodbFileStore(this, fileName);
	};
	//
	/// -------------
	/// ## getEntity function :
	/// ``` javascript
	/// var entity = db.getEntity(_, entityName);
	/// ```
	/// Get the class metadata as an entity
	/// 
	/// 
	self.getEntity = function(_, entityName) {
		return this.model.getEntity(_, entityName) || this.model.getEntity(_, this.model.singularize(entityName));
	};
	self.getUpdDatePropName = function() {
		return "$updDate";
	};
	// fetch instance
	self.fetchInstance = function(_, entity, uuid) {
		var filter;
		var options = {
			fields: getSelectFields(entity),
			limit: 1
		};
		if (typeof uuid === "object") {
			filter = _paramsToFilter(_, uuid, entity);
			options.sort = _paramsToOrderBy(uuid, entity);
		} else filter = {
			_id: uuid
		};
		var timing = perfmon.start(module, "mongodb.fetchInstance", entity.name + '/' + filter._id);
		config.tracer && config.tracer("mongodb.fetchInstance filter: " + sys.inspect(filter, null, 4));
		var dataArray = this.db.collection(entity.className, _).find(filter, options).toArray(_);
		timing.end();
		if (dataArray.length) {
			config.tracer && config.tracer("mongodb.fetchInstance found: " + entity.name + "\n+" + sys.inspect(uuid) + "\n+" + sys.inspect(dataArray[0]));
			dataArray[0].$key = dataArray[0]._id;
			dataArray[0].$uuid = dataArray[0]._id;
			dataArray[0].$loaded = true;
			return entity.factory.createInstance(_, unescapeJson(dataArray[0]), this);
		} else {
			config.tracer && config.tracer("mongodb.fetchInstance not found: " + entity.name + "\n+" + sys.inspect(uuid, null, 4));
			return null;
		}
	};
	// used for lazy load properties (binary or large text)
	self.fetchInstanceProperty = function(_, entity, propName, param) {
		var filter;
		var options = {
			fields: [propName],
			limit: 1
		};
		if (typeof param === "object") {
			filter = _paramsToFilter(_, param, entity);
			options.sort = _paramsToOrderBy(param, entity);
		} else filter = {
			_id: param
		};
		var timing = perfmon.start(module, "mongodb.fetchInstanceProperty", entity.name + '/' + filter._id + '/' + propName);
		var dataArray = this.db.collection(entity.className, _).find(filter, options).toArray(_);
		timing.end();
		if (dataArray.length) {
			config.tracer && config.tracer("mongodb.fetchInstanceProperty found: " + entity.name + "\n+" + param + "\n+" + sys.inspect(dataArray[0]));
			return dataArray[0][propName];
		} else {
			config.tracer && config.tracer("mongodb.fetchInstance not found: " + entity.name + "\n+" + param);
			return null;
		}
	};
	//
	self.count = function(_, entity, params) {
		var parameters = params || {};
		//
		var filter = _paramsToFilter(_, parameters, entity);
		//
		return this.db.collection(entity.className, _).count(filter, _);
	};
	//
	/// -------------
	/// ## createCursor function :
	/// ``` javascript
	/// var cursor = db.createCursor(_, entity, params, shallow);
	/// var data;
	/// while(data = cursor.next(_) {
	///   // do something with data witch is an object instance
	/// }
	/// ```
	/// Creates a cursor allowing to iterate over the objects in a collection
	/// function next(_) on the cursor returns the current instance. Returns null at the end of the cursor
	/// 
	/// ```javascript
	/// // parameters example
	/// params = {
	///   count: 20, // cursor fetch limit
	///   startIndex: 2, // skip parameter
	///   orderBy: [{binding:"name", descending: true}, {binding: title}],
	///   jsonWhere: {/* mongodb style json filter */} // or sdataWhere = sdataClause or where = parsed_expression_object
	/// }
	/// ```
	/// 
	self.createCursor = function(_, entity, params, shallow) {
		var parameters = params || {};
		config.tracer && config.tracer("mongodb.createCursor " + entity.className + " parameters: " + sys.inspect(parameters, null, 5));
		var options = {};
		if (parameters.count) options.limit = parameters.count;
		if (parameters.startIndex) options.skip = parameters.startIndex - 1;
		options.sort = _paramsToOrderBy(parameters, entity);
		options.fields = getSelectFields(entity, shallow);
		//
		var filter = _paramsToFilter(_, parameters, entity);
		//
		config.tracer && config.tracer("mongodb.createCursor filter: " + sys.inspect(filter));
		config.tracer && config.tracer("mongodb.createCursor options: " + sys.inspect(options));
		//
		return new MongodbCursor(this, entity, this.db.collection(entity.className, _).find(filter, options));
	};
	// fetch all instances acording to parameters
	self.fetchInstances = function(_, entity, params, shallow, context) {
		var timing = perfmon.start(module, "mongodb.fetchInstances", entity.name);
		var startTime = _startProfile();
		var instances = [];
		var cursor = this.createCursor(_, entity, params, shallow);
		var data;
		while (data = cursor.next(_)) instances.push(data);
		config.tracer && config.tracer("mongodb.fetchInstances found " + instances.length + " " + entity.className + "; TOOK: " + _endProfile(startTime));
		//
		timing.end();
		return instances;
	} //;
	self.saveInstance = function(_, instance) {
		var self = this;
		if (instance._meta.$sequentialStorage) {
			var funnel = _funnels[instance._meta.name];
			if (!funnel) funnel = _funnels[instance._meta.name] = flows.funnel(1);
			return funnel(_, function(_) {
				return _internalSave(_, instance, self.db);
			});
		} else return _internalSave(_, instance, self.db);
	};
	// helper function

	function _getDirtyProperty(_, data, $p, value, path) {
		if (!$p.$compute) {
			if (value && (typeof value === "object")) value = escapeJson(value);
			if ($p.$isLocalized) {
				value && Object.keys(value).forEach(function(loc) {
					data.$set[path + "." + loc] = value[loc];
				});
			} else if ($p.isExternalStorage()) {
				// !!! value is escaped, so use _uuid, instead of $uuid !!!
				if (value && value._uuid) data.$set[path + "._uuid"] = value._uuid;
				else data.$set[path] = {};
			} else data.$set[path] = value;
		}
	}

	function _getDirtyPlural(_, data, pull, $r, value, path) {
		// array storage function: value is the array to be persisted
		// need to escape the childrens
		data.$set[path] = value && escapeJson(value);
	}

	function _getDirtyProps(_, data, pull, delta, meta, path) {
		var locPath = (path ? (path + ".") : "");
		flows.eachKey(_, delta, function(_, key, value) {
			// escaped key
			var escKey = syracuseEscapeMap[key] || key;
			//
			if (key === "$index") data.$set[locPath + escKey] = value;
			if (key === "$variantType") data.$set[locPath + escKey] = value;
			if ((key === "$signature") && value) data.$set[locPath + escKey] = value;
			// is a property ?
			var $p = meta.$properties && meta.$properties[key];
			$p && _getDirtyProperty(_, data, $p, value, locPath + escKey);
			// is a relation ?
			var rel = meta.$relations && meta.$relations[key];
			if (rel && !rel.$compute && !rel.isComputed) {
				if (meta.$relations[key].isPlural) {
					_getDirtyPlural(_, data, pull, rel, value, locPath + escKey);
				} else {
					if (rel.getIsChild(value && value.$variantType) || rel.$inlineStore) {
						if (!value) {
							//										throw new Error("Cannot save null child for relation: "+key);
							data.$set[locPath + escKey] = {};
						} else {
							_getDirtyProps(_, data, pull, value, rel.getTargetEntity(value && value.$variantType), locPath + escKey);
							data.$set[locPath + escKey + "." + syracuseEscapeMap["$uuid"]] = value.$uuid;
						}
					} else {
						if (value) {
							if (value.$url) data.$set[locPath + escKey + "." + syracuseEscapeMap["$url"]] = value.$url;
							if (value.$variantType) data.$set[locPath + escKey + "." + syracuseEscapeMap["$variantType"]] = value.$variantType;
							if (value.$uuid) data.$set[locPath + escKey + "." + syracuseEscapeMap["$uuid"]] = value.$uuid;
						} else data.$set[locPath + escKey] = {};
					}
				}
			}
			//					}
			//				}
		});
	}
	// persist instance and its childs

	function _internalSave(_, instance, db) {
		var data = {
			$set: {}
		};
		var pull = {};
		//
		var _opRef = opRef++;
		config.tracer && config.tracer("mongodb.save enter: opRef=" + _opRef);
		//
		if (instance._snapshotEnabled) var saveDelta = instance.getSaveSnapshotDelta(_);
		else var saveDelta = instance.serializeInstance(_);
		config.tracer && config.tracer("mongodb.save delta: opRef=" + _opRef + "; " + sys.inspect(saveDelta));
		//
		_getDirtyProps(_, data, pull, saveDelta, instance._meta);
		// technical meta
		var updDate = new Date();
		if (instance.$created) {
			data.$set._creUser = instance.$creUser;
			data.$set._creDate = instance.$creDate = updDate;
		}
		data.$set._updUser = instance.$updUser;
		// TODO: for now, mongodb doesn't seems to accept javascript in update, so we'll send the Syracuse server date
		// It would be best if it was the mongodb server date 
		data.$set._updDate = instance.$updDate = updDate;
		// save global UUID (remove existing syncUuid only when requested - this is to avoid removal of syncUuid when
		// an instance is stored and during storing, the synchronization digest is formed and syncUuids are assigned to the instances
		if (instance.$syncUuid || instance._deleteSyncUuid) {
			instance._deleteSyncUuid = false;
			data.$set._syncUuid = instance.$syncUuid;
		}
		data.$set._tick = instance.$tick;
		data.$set._endpoint = instance.$endpoint;
		// manage time to live 
		if (instance._meta.$expire) {
			var millisec = instance._meta.$expire(_, instance);
			if (millisec && millisec > 0) {
				var curTime = (new Date()).getTime();
				data.$set._expire = instance.$expire = new Date(curTime + millisec);
			}
		}
		//
		var collection = db.collection(instance.getClassName(), _);
		var instanceFilter = (instance._meta.$lockType && (instance._meta.$lockType === "noLock")) ? {
			_id: instance.$uuid
		} : {
			_id: instance.$uuid,
			$or: [{
				_updDate: instance.$initialUpdDate
			}, {
				_updDate: null
			}]
		};
		// crnit110825
		// in current mongodb version there is an issue making an atomic $pull and $addToSet on the same array
		// http://jira.mongodb.org/browse/SERVER-1050
		// this should be fixed but for now we'll make a distinct pull operation before
		// after mongodb fixes this, add pull to data and make just one call to update
		if (pull && (Object.keys(pull).length > 0)) {
			config.tracer && config.tracer("mongodb.save data pull: opRef=" + _opRef + "; " + instance.getClassName() + ";" + sys.inspect(pull));
			collection.update(instanceFilter, pull, {
				safe: true
			}, _);
		}
		//
		config.tracer && config.tracer("mongodb.save data: opRef=" + _opRef + "; " + instance.getClassName() + ";" + sys.inspect(data, null, 4));
		// TODO: if result is 0, detail error conditions
		var result;
		if (instance.$created) {
			// creation, do not check concurrency conditions. Use update because of the common use of $set for insert and for update.
			result = collection.update({
				_id: instance.$uuid
			}, data, {
				safe: true,
				upsert: true
			}, _);
		} else {
			config.tracer && config.tracer("mongodb.save data instanceFilter: opRef=" + _opRef + "; " + sys.inspect(instanceFilter));
			// check concurrency with updDate
			result = collection.update(instanceFilter, data, {
				safe: true,
				upsert: false
			}, _);
		}
		// keep the count
		if (result) instance.$initialUpdDate = instance.$updDate;
		config.tracer && config.tracer("mongodb.save data result: opRef=" + _opRef + "; " + result);
		return result;
	}

	//
	self.deleteInstance = function(_, instance) {
		var self = this;
		if (instance._meta.$sequentialStorage) {
			var funnel = _funnels[instance._meta.name];
			if (!funnel) funnel = _funnels[instance._meta.name] = flows.funnel(1);
			return funnel(_, function(_) {
				return _deleteInstance(_, instance, self.db);
			});
		} else return _deleteInstance(_, instance, self.db);
	};
	// delete instance and its childs

	function _deleteInstance(_, instance, db) {
		config.tracer && config.tracer("mongodb.deleteInstance: " + instance.$uuid);
		return db.collection(instance.getClassName(), _).remove({
			_id: instance.$uuid
		}, {
			safe: true
		}, _);
	}

	// atomicaly create a instance lock
	self.lockInstance = function(_, instance) {
		if (!instance) return null;
		if (!instance.$uuid) return null;
		var session = globals.context.session;
		var userLogin = (session && session.getUserLogin(_)) || "anonymous";
		var ssid = (session && session.id) || processId;
		// try un upsert with _id=instance.$uuid and currect sessionId. If allready locked, returns count of 0
		var lockId = instance.$uuid;
		try {
			var coll = this.db.collection("dbLocks", _);
			var res = coll.update({
				_id: lockId,
				sessionId: ssid
			}, {
				$set: {
					sessionId: ssid,
					lockDate: new Date(),
					lockUser: userLogin
				}
			}, {
				safe: true,
				upsert: true
			}, _);
			if (res == 1) return {
				status: "success",
				id: lockId
			};
		} catch (ex) {
			// return null;
			// pk violation, allready locked
		}
		// read the lock record to return meta
		var locks = coll.find({
			_id: lockId
		}).toArray(_);
		if (locks && locks[0]) return {
			status: "locked",
			lock: locks[0]
		};
		else return {
			status: "error"
		};
	};
	self.unlockInstance = function(_, instance) {
		this.db.collection("dbLocks", _).remove({
			_id: instance.$uuid
		}, {
			safe: true
		}, _);
	};
	// atomicaly create a instance lock
	self.lockDatabase = function(_) {
		config.tracer && config.tracer("Mongodb locking database");
		var session = globals.context.session;
		var userLogin = (session && session.getUserLogin(_)) || "internal";
		var ssid = self._lockSid = self._lockSid || helpers.uuid.generate();
		// try un upsert with _id=instance.$uuid and currect sessionId. If allready locked, returns count of 0
		var lockId = "database";
		try {
			var coll = this.db.collection("dbLocks", _);
			var res = coll.update({
				_id: lockId,
				sessionId: ssid
			}, {
				$set: {
					sessionId: ssid,
					lockDate: new Date(),
					lockUser: userLogin
				}
			}, {
				safe: true,
				upsert: true
			}, _);
			if (res == 1) {
				config.tracer && config.tracer("Mongodb lock database: success");
				return {
					status: "success",
					id: lockId
				};
			}
		} catch (ex) {
			// return null;
			// pk violation, allready locked
		}
		// read the lock record to return meta
		var locks = coll.find({
			_id: lockId
		}).toArray(_);
		if (locks && locks[0]) return {
			status: "locked",
			lock: locks[0]
		};
		else return {
			status: "error"
		};
	};
	// lock the database, but retry it when it does not work out for the first time
	self.lockDatabaseRetry = function(_) {
		var start = new Date();
		var lock = self.lockDatabase(_);
		while ((lock.status !== "success") && (((new Date()) - start) < 60000)) {
			// wait some time, 50ms min
			setTimeout(~_, Math.floor(Math.random() * 10000) + 50);
			//
			lock = self.lockDatabase(_);
		}
		if (lock.status !== "success") throw new Error(locale.format(module, "databaseLockTimeout"));
		return lock;
	};
	self.unlockDatabase = function(_) {
		config.tracer && config.tracer("Mongodb unlocking database");
		this.db.collection("dbLocks", _).remove({
			_id: "database"
		}, {
			safe: true
		}, _);
		this._lockId = null;
	};
}

helpers.defineClass(MongoDbHandle);

exports.create = function(_, model, dataset) {
	var handle = new MongoDbHandle(model, dataset);
	handle.connect(_);
	return handle;
};

//
exports.setup = function(mongodbConfig, mongoConnPool) {
	config = mongodbConfig || {};
	connectionPool = mongoConnPool;
};