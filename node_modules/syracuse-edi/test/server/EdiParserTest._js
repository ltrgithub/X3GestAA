"use strict";
/* jshint -W079 */
/* jshint unused: false */
/* global QUnit: false, asyncTest: false, test: false, strictEqual: false, ok: false, start: false, stop: false */

/*global QUnit, start, ok*/
var syracuse = require('syracuse-main/lib/syracuse');
var helpers = require('syracuse-core').helpers;
var streams = require("streamline-streams");
var sys = require("util");
var config = require('config'); // must be first syracuse require
var adminHelper = require("syracuse-collaboration/lib/helpers").AdminHelper;
var parser = require("syracuse-edi/lib/tool/parser");
var serializer = require("syracuse-edi/lib/tool/serializer");
var EdiProcess = require("syracuse-edi/lib/edi_Process");
var EdiEntity = require("syracuse-edi/lib/edi_Entity");
var EdiType = require("syracuse-edi/lib/enumType").EdiType;
var testAdmin = require('syracuse-core').apis.get('test-admin');
var mongodb = require('mongodb');
var dataModel = require("syracuse-orm/lib/dataModel");
var registry = require("syracuse-sdata/lib/sdataRegistry");
var fs = require("streamline-fs");
var sqMap = require("syracuse-edi/lib/helpers").seqentialFilePropertyMap;
var mmMap = require("syracuse-edi/lib/helpers").messageMappingPropertyMap;
var SEQFILE_LIB = require('syracuse-edi/lib/enumType').SEQFILE_LIB;
var IMPORTFILE_LIB = require('syracuse-edi/lib/enumType').IMPORTFILE_LIB;
var ediDataFormat = require("syracuse-edi/lib/enumType").EDIDATEFORMAT;
var jsxml = require('js-xml');
var upath = require('path');
var dbName = "unit_test";
var traces = false;
// activate traces
if (traces) {
	var level = "error";
	var option = {
		name: "edi",
		enabled: true,
		level: level,
		transport: "console",
		mod: {
			name: "parser",
			enabled: true,
			level: level
		}
	};
	testAdmin.setTracesOn("edi.parser", option);

	option.mod.name = "serializer";
	testAdmin.setTracesOn("edi.serializer", option);
}

var globals = require('streamline-runtime').globals;
var _defDataDir = upath.join(__dirname, "../server/data/");

function _getModel() {
	return dataModel.make(registry.applications.syracuse.contracts.collaboration, dbName);
}

var endpointTest = {
	dataset: function(_) {
		return "GX3APP";
	},
	getModel: function(_) {
		return {
			getEntity: function(_, nameEntity, facet) {
				return {
					name: nameEntity,
					getPrototype: function(_, name, facet) {
						return JSON.parse(fs.readFile(_defDataDir + "context/" + name + "_" + facet + ".json", 'utf-8', _));
					}
				};
			}
		};
	}
};


function retrieveCommonPrefix(exp1, exp2) {
	var prefix = "";
	for (var i = 0; i < exp1.length && exp2.length && exp1[i] === exp2[i]; i++) {
		prefix += exp1[i];
	}
	return prefix[prefix.length - 1] === "." ? prefix.substring(0, prefix.length - 1) : prefix;
};

function createContextParser(seqFile, sqMap) {
	var mapFilesCode = {};
	var keyByFileCode = {};
	seqFile[sqMap.filesDescription] && seqFile[sqMap.filesDescription].forEach(function(item) {
		mapFilesCode[item[sqMap.filesDescriptions.fileId]] = item[sqMap.filesDescriptions.fileName];

		keyByFileCode[item[sqMap.filesDescriptions.fileName]] = {
			primary: item[sqMap.filesDescriptions.primarykey],
			foreign: item[sqMap.filesDescriptions.foreignkey],
			fatherFileId: item[sqMap.filesDescriptions.fatherFileId]
		};
	});
	var elems = [];
	var prefixExp = {};
	var currentFilename = "";
	seqFile[sqMap.elem].forEach(function(item, idx, arr) {
		var classInstname = seqFile[sqMap.classInstance] || null;
		var exp = item[sqMap.elems.expression];

		var fileName = mapFilesCode[item[sqMap.elems.fileId]];
		prefixExp[fileName] = prefixExp[fileName] == null ? exp : retrieveCommonPrefix(prefixExp[fileName], exp); // get common path for the file id
		var element = {
			isStart: currentFilename !== fileName,
			offset: item[sqMap.elems.offset] - 1,
			length: item[sqMap.elems.length],
			level: item[sqMap.elems.level],
			expression: exp,
			flag: item[sqMap.elems.flag],
			idLineElem: item[sqMap.elems.idLine],
			isEnd: item[sqMap.elems.isEnd],
			fileName: fileName,
			fileId: item[sqMap.elems.fileId]
		};
		currentFilename = fileName;
		elems.push(element);
	});
	var configParser = {
		parser: SEQFILE_LIB[seqFile[sqMap.fileType]],
		sepDecimal: seqFile[sqMap.decimalSep],
		sepField: seqFile[sqMap.fieldSep],
		sepRecord: "\n", // TODO the one in the context
		delimField: seqFile[sqMap.fieldDelimiter],
		elems: elems,
		linkKey: keyByFileCode, // primary and foreign key for the element
		exppressionPrefix: prefixExp,
		dateFormat: seqFile[sqMap.dateFormat] ? ediDataFormat[seqFile[sqMap.dateFormat]] : null

	};
	return configParser;
}

function createContextSerializer(mm, mapField, mapFilesCode, isImport) {
	// create map between file name and file code

	var serializerMethod;
	if (!isImport) {
		serializerMethod = SEQFILE_LIB[mm[mapField.fileType]];
	} else {
		serializerMethod = IMPORTFILE_LIB[mm[mapField.fileType]];
	}
	var elems = [];
	mm && mm[mapField.elem] && mm[mapField.elem].forEach(function(item, idx, arr) {
		var classInstname = mm[mapField.classInstance];
		var exp = item[mapField.elems.expression];

		elems.push({
			offset: item[mapField.elems.offset] - 1,
			length: item[mapField.elems.length],
			expression: exp, // remove the classe instance if it exists to have the property name that correspond tot he prototype arborescence
			flag: item[mapField.elems.flag],
			isEnd: item[mapField.elems.isEnd],
			fileName: mapFilesCode[item[mapField.elems.fileId]],
			level: item[mapField.elems.level],
			isMandatory: item[mapField.elems.isMandatory] === 2 ? true : false
		});
	});
	var configSerializer = {
		serializer: serializerMethod,
		sepDecimal: mm[mapField.decimalSep],
		sepField: mm[mapField.fieldSep],
		sepRecord: "\n",
		delimField: mm[mapField.fieldDelimiter],
		elems: elems,
		fileName: isImport ? "importTest_" + (new Date().getTime()) + ".csv" : null,
		dateFormat: mm[mapField.dateFormat] ? ediDataFormat[mm[mapField.dateFormat]] : null

	};
	return configSerializer;
}



globals.context = globals.context || {};
globals.context.session = {
	id: helpers.uuid.generate(),
	getUserLogin: function(_) {
		return "admin";
	},
	getUserProfile: function(_) {
		return {
			selectedLocale: function(_) {
				var db = adminHelper.getCollaborationOrm(_);

				// the metamodel is associated to the orm
				var model = db.model;

				var entity = db.model.getEntity(_, "localePreference");
				return db.fetchInstance(_, entity, {
					jsonWhere: {
						code: "en-US"
					}
				});
			},
			user: function(_) {
				// getting the administration ORM
				var db = adminHelper.getCollaborationOrm(_);

				// the metamodel is associated to the orm
				var model = db.model;

				var entity = db.model.getEntity(_, "user");
				// fetchInstance(callback, entity, filter)
				return db.fetchInstance(_, entity, {
					jsonWhere: {
						login: "admin"
					}
				});

			}
		};
	},
	getSecurityProfile: function(_) {
		return null;
	},
	getData: function(key) {
		return null;
	},
};
QUnit.module(module.id, {});
var endpoint, db;
var prototype_EDISIH1;
var prototype_EDISIH1SCOL;
var prototype_EDIS0H2;
var prototype_EDISIH1FACTURA;

function _initPrototypeCache(_, database) {
	EdiEntity.dropAllEdiCacheEntity(_, {
		db: database
	});
	EdiProcess.removeAllEdiProcess(_, {
		db: database
	});
	prototype_EDISIH1 = JSON.parse(fs.readFile(_defDataDir + "context/EDISIH1_$details.json", 'utf-8', _));
	prototype_EDISIH1SCOL = JSON.parse(fs.readFile(_defDataDir + "context/EDISIH1SCOL_$details.json", 'utf-8', _));
	prototype_EDIS0H2 = JSON.parse(fs.readFile(_defDataDir + "context/EDISOH2_$details.json", 'utf-8', _));
	prototype_EDISIH1FACTURA = JSON.parse(fs.readFile(_defDataDir + "context/EDISIH1FACTURA_$details.json", 'utf-8', _));

}

function sortInput(input, opt) {
	var sortInput = {};
	Object.keys(opt.linkKey).forEach(function(filenamePattern) {
		// looking for the right file
		var found = null;
		var regexp = new RegExp(filenamePattern);

		for (var i = 0; i < Object.keys(input).length && !found; i++) {
			found = regexp.test(Object.keys(input)[i]) ? Object.keys(input)[i] : null;
		}
		// add found data
		if (found) {
			sortInput[found] = input[found];
		}
	});
	return sortInput;

}
var uuidJSonSOH;


function _testError(_, data, code) {
	console.log("------------------ TEST " + code)
	try {
		parser.preprocessMapping(_, data)
		strictEqual(true, false, "No error for " + code);
	} catch (e) {
		strictEqual(e.code, code, "Error code " + code)
		console.log("Error " + e);
	}
}

asyncTest("preprocess test", 21, function(_) {
	var mappings = [{
		xml: "A.L",
		json: "u",
		type: "base:xs:string,maxLength:50"
	}, {
		xml: "A.B.C[0..n].D",
		json: "v.w",
		type: "base:xs:string"
	}, {
		xml: "A.B.C[0..n].DE",
		json: "v.x",
		type: "base:xs:decimal"
	}, {
		xml: "A.F[0..n].G.H",
		json: "y.z"
	}, {
		xml: "A.F[0..n].I[0..n].J",
		json: "y.t.s"
	}];

	var r1 = parser.preprocessMapping(_, mappings)
	strictEqual(JSON.stringify(r1), JSON.stringify({
		"": {
			toXML: false,
			depth: 3
		},
		"A.L": ["u", "50"],
		"A.B.C": {
			"": "v",
			"D": ["w"],
			"DE": ["x", "0", "decimal"]
		},
		"A.F": {
			"": "y",
			"G.H": ["z"],
			"I": {
				"": "t",
				"J": ["s"]
			}
		}
	}), "Transformation to JSON mapping")
	mappings.push({}); // add empty mapping (should not change anything)
	var r1 = parser.preprocessMapping(_, mappings, true)
	strictEqual(JSON.stringify(r1), JSON.stringify({
		"": {
			toXML: true,
			depth: 1
		},
		"u": ["A.L", "50"],
		"v": {
			"": "A.B.C",
			"w": ["D"],
			"x": ["DE", "0", "decimal"]
		},
		"y": {
			"": "A.F",
			"z": ["G.H"],
			"t": {
				"": "I",
				"s": ["J"]
			}
		}
	}), "Transformation to XML mapping")
	console.log(JSON.stringify(r1));
	_testError(_, [{
		xml: "A"
	}], "EMPTYJSON")
	_testError(_, [{
		json: "A"
	}], "EMPTYXML")
	_testError(_, [{
		json: "",
		xml: "A"
	}], "EMPTYJSON")
	_testError(_, [{
		json: "A",
		xml: ""
	}], "EMPTYXML")
	_testError(_, [{}], "NOMAPPING")
	_testError(_, [{
		xml: "A.B[0..n][0..m].C",
		json: "u.v.w"
	}], "EMPTYPARTBASE");
	_testError(_, [{
		xml: "A.B[0..n].B[0..m]",
		json: "u.v.w"
	}], "EMPTYPARTBASEEND");
	_testError(_, [{
		xml: "A.B[0..n].B[0..m].C",
		json: "u.v."
	}], "EMPTYPARTDESTEND")
	_testError(_, [{
		xml: "A.B[0..n].B[0..m].C",
		json: "u..v."
	}], "EMPTYPARTDEST")
	_testError(_, [{
		xml: "A.B[0..n].B[0..m].C",
		json: "u.vw"
	}], "DESTLESSGROUPS")
	_testError(_, [{
		xml: "A.B[0..n].B[0..m].C",
		json: "u.v.w.x"
	}], "DESTMOREGROUPS")
	_testError(_, [{
		xml: "A.B",
		json: "u"
	}, {
		xml: "A.B[0..n].B",
		json: "u.v"
	}], "FULLTOGROUP")
	_testError(_, [{
		xml: "A.B[0..n].B",
		json: "u.v"
	}, {
		xml: "A.B",
		json: "u"
	}], "GROUPTOFULL")
	_testError(_, [{
		xml: "A.B[0..n].B",
		json: "u.v"
	}, {
		xml: "A.B[0..n].C",
		json: "v.v"
	}], "GROUPCHANGE")
	_testError(_, [{
		xml: "A",
		json: "u"
	}, {
		xml: "A",
		json: "v"
	}], "FULLCHANGE")
	_testError(_, [{
		xml: "A.C",
		json: "u"
	}, {
		xml: "A.C.D",
		json: "v"
	}], "CONTAINS");
	_testError(_, [{
		xml: "A.C.D",
		json: "u"
	}, {
		xml: "A.C",
		json: "v"
	}], "CONTAINED");
	_testError(_, [{
		xml: "A[0..n].B",
		json: "u.v"
	}], "ROOTREPETITION");
	_testError(_, [{
		xml: "A.B[0..n].B",
		json: "u.v"
	}, {
		xml: "AA.B[0..n].B",
		json: "w.v"
	}], "ROOTDIFFERENT");

	start();
});

asyncTest("converter test", 21, function(_) {
	var mappings = [{
		xml: "A.L",
		json: "u",
		type: "base:xs:string,maxLength:10"
	}, {
		xml: "A.B.C[0..n].D",
		json: "v.w",
		type: "base:xs:string"
	}, {
		xml: "A.B.C[0..n].DE",
		json: "v.x",
		type: "base:xs:decimal"
	}, {
		xml: "A.F[0..n].G.H",
		json: "y.z"
	}, {
		xml: "A.F[0..n].I[0..n].J",
		json: "y.t.s"
	}];
	var map = parser.preprocessMapping(_, mappings, true);
	var converted = parser.convert({
			u: "abcdefghijklmnop",
			nothing: 5,
			nil: {},
			v: {
				w: "a",
				x: 1.5
			},
			y: [{
				z: 1
			}, {
				z: 2,
				t: {
					s: 20
				}
			}]
		}, map)
		// string will be truncated!
	strictEqual(JSON.stringify(converted), JSON.stringify({
		"A": {
			"F": [{
				"G": {
					"H": 1
				}
			}, {
				"I": [{
					"J": 20
				}],
				"G": {
					"H": 2
				}
			}],
			"B": {
				"C": [{
					"DE": "1.5",
					"D": "a"
				}]
			},
			"L": "abcdefghij"
		}
	}), "Conversion to jsxml")
	var map2 = parser.preprocessMapping(_, mappings);
	var convertedback = parser.convert(converted, map2);
	// string will be truncated!
	strictEqual(JSON.stringify(convertedback), JSON.stringify({
		u: "abcdefghij",
		v: [{
			w: "a",
			x: 1.5
		}],
		y: [{
			z: 1
		}, {
			z: 2,
			t: [{
				s: 20
			}]
		}]
	}), "Conversion from jsxml")
	var A = {
		A: undefined
	};
	A.A = A;
	var convertedrec = parser.convert(A, map2);
	strictEqual(JSON.stringify(convertedrec), '{}', "recursive structure")
		// test with type conversions
	var mappings = [{
		xml: "A.A",
		json: "a",
		type: "base:xs:integer"
	}, {
		xml: "A.B",
		json: "b",
		type: "base:xs:decimal"
	}, {
		xml: "A.C",
		json: "c",
		type: "base:xs:date"
	}, {
		xml: "A.D",
		json: "d",
		type: "base:xs:dateTime"
	}, {
		xml: "A.E",
		json: "e",
		type: "base:xs:boolean"
	}];
	var map = parser.preprocessMapping(_, mappings, true);
	var converted = parser.convert({
		a: 1,
		b: 1.5,
		c: "2011-10-10Z",
		d: new Date("2011-10-10T05:05Z"),
		e: true
	}, map);
	strictEqual(JSON.stringify(converted), '{"A":{"E":"true","D":"2011-10-10T05:05:00Z","C":"2011-10-10","B":"1.5","A":"1"}}', "type example 1")
	var converted = parser.convert({
		a: "1.5",
		b: +0,
		c: new Date("2011-10-10Z"),
		d: "2011-10-10T05:05:05Z",
		e: "false"
	}, map);
	strictEqual(JSON.stringify(converted), '{"A":{"E":"false","D":"2011-10-10T05:05:05Z","C":"2011-10-10","B":"0","A":"1"}}', "type example 2")
	var map = parser.preprocessMapping(_, mappings, false);
	var converted = parser.convert({
		A: {
			A: "1",
			B: "+1.5E2",
			C: "2011-10-10Z",
			D: "2011-10-10T05:05:05Z",
			E: "1"
		}
	}, map);
	strictEqual(converted.e, true, "conversion from XML: boolean 1")
	strictEqual(converted.d.getTime(), new Date("2011-10-10T05:05:05.000Z").getTime(), "conversion from XML: datetime 1")
	strictEqual(converted.c.getTime(), new Date("2011-10-10Z").getTime(), "conversion from XML: date 1")
	strictEqual(converted.b, 150, "conversion from XML: decimal 1")
	strictEqual(converted.a, 1, "conversion from XML: integer 1")
	var converted = parser.convert({
		A: {
			A: "-15",
			B: "-1.5",
			C: "2011-10-10Z",
			D: "2011-10-10T05:05:05Z",
			E: "true"
		}
	}, map);
	strictEqual(converted.e, true, "conversion from XML: boolean 2")
	strictEqual(converted.d.getTime(), new Date("2011-10-10T05:05:05.000Z").getTime(), "conversion from XML: datetime 2")
	strictEqual(converted.c.getTime(), new Date("2011-10-10Z").getTime(), "conversion from XML: date 2")
	strictEqual(converted.b, -1.5, "conversion from XML: decimal 2")
	strictEqual(converted.a, -15, "conversion from XML: integer 2")
	start();
})


asyncTest("init test environment ", function(_) {
	endpoint = testAdmin.modifyCollaborationEndpoint(dbName);
	db = dataModel.getOrm(_, _getModel(), endpoint.datasets[dbName]);
	EdiEntity.dropAllEdiCacheEntity(_, {
		db: db
	});
	EdiProcess.removeAllEdiProcess(_, {
		db: db
	});
	start();
});
asyncTest("parser fixed size ", function(_) {

	var fileBuff = {};
	fileBuff["LINFAC"] = fs.readFile(_defDataDir + "ediFiles/LINFAC", 'utf-8', _).replace(/\r\n/g, "\n");;

	var seqFile = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSIH.json", 'utf-8', _));
	var prototype = JSON.parse(fs.readFile(_defDataDir + "context/BadProto.json", 'utf-8', _));


	var configParser = createContextParser(seqFile, sqMap);

	var res = parser.mapParse["fixedLength"](fileBuff, configParser, prototype);

	strictEqual(JSON.stringify(res), "{}", "generate instance json from LINFAC (with not all field  value), good seqFile and bad prototype");

	fileBuff["CABFAC"] = fs.readFile(_defDataDir + "ediFiles/CABFAC1", 'utf-8', _).replace(/\r\n/g, "\n");;

	seqFile = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSIH.json", 'utf-8', _));
	prototype = JSON.parse(fs.readFile(_defDataDir + "context/BadProto.json", 'utf-8', _));

	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);
	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","SIVTYP":"FAC","ACCDAT":"2014-02-24"}}', "generate instance json from LINFAC (with not all field  value) and CABFAC, good seqFile and bad prototype");
	seqFile = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSIH.json", 'utf-8', _));
	prototype = JSON.parse(fs.readFile(_defDataDir + "context/fakePrototype.json", 'utf-8', _));

	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);
	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"AMTNOT":66000,"SIVTYP":"FAC","ACCDAT":"2014-02-24","ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000}]}}', "generate instance json from LINFAC (with not all field  value) and CABFAC, good seqFile and good prototype (not describe all properties)");
	prototype = JSON.parse(fs.readFile(_defDataDir + "context/fullPrototype.json", 'utf-8', _));

	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);

	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"AMTNOT":66000,"SIVTYP":"FAC","ACCDAT":"2014-02-24","ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000}]}}', "generate instance json from LINFAC (with not all field  value) and CABFAC, good seqFile and full prototype");

	fileBuff["CABFAC"] = fs.readFile(_defDataDir + "ediFiles/CABFACIncomplete.txt", 'utf-8', _).replace(/\r\n/g, "\n");;

	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);

	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000}]}}', "generate instance json from LINFAC  and CABFAC (incomplete), good seqFile and good prototype");


	fileBuff["LINFAC"] = fs.readFile(_defDataDir + "ediFiles/LINFACIncomplete.txt", 'utf-8', _).replace(/\r\n/g, "\n");;

	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);

	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000}]}}', "generate instance json from LINFAC (with not all field  value) and CABFAC (incomplete), good seqFile and good prototype");

	fileBuff["LINFAC"] = fs.readFile(_defDataDir + "ediFiles/LINFACIncomplete2.txt", 'utf-8', _).replace(/\r\n/g, "\n");;

	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);
	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000}]}}', "generate instance json from LINFAC (with not all field value version 2) and CABFAC (incomplete), good seqFile and good prototype");

	prototype = JSON.parse(fs.readFile(_defDataDir + "context/fakePrototype.json", 'utf-8', _));

	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);

	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","QTY":2,"GROPRI":1000000}]}}', "generate instance json from LINFAC (with not all field  value) and CABFAC (incomplete), good seqFile and good prototype");

	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype, res);

	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","QTY":2,"GROPRI":1000000}]}}', "add to an existent instance json from LINFAC (with not all field  value) and CABFAC (incomplete), good seqFile and good prototype ");


	// test array of object or array
	fileBuff["LINFAC"] = fs.readFile(_defDataDir + "ediFiles/LINFACSCOL.txt", 'utf-8', _).replace(/\r\n/g, "\n");;
	seqFile = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSIHArray.json", 'utf-8', _));
	var mmFile = JSON.parse(fs.readFile(_defDataDir + "context/messageMappingSOHREal_sep_field.json", 'utf-8', _));

	prototype = JSON.parse(fs.readFile(_defDataDir + "context/EDISIH1SCOL_$details.json", 'utf-8', _));

	var configParser = createContextParser(seqFile, sqMap);
	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);

	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000,"SCOL":[{"TEST":"VALT"}]},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000,"SCOL":[{"TEST":"VALY"}]}]}}', " parse object of array of object or array ok");

	fileBuff["CABFAC"] = fs.readFile(_defDataDir + "ediFiles/CABFACMultiple.txt", 'utf-8', _);
	fileBuff["LINFAC"] = fs.readFile(_defDataDir + "ediFiles/LINFACMultiple.txt", 'utf-8', _);
	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);


	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"AMTNOT":66000,"SIVTYP":"FAC","ACCDAT":"2014-02-24","ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000}]},"FCC11014VEN00000012":{"NUM":"FCC11014VEN00000012","BPR":"ESP0002","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"AMTNOT":66000,"SIVTYP":"FAC","ACCDAT":"2014-02-24","ESIH1SID":[{"NUM":"FCC11014VEN00000012","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000012","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000},{"NUM":"FCC11014VEN00000012","SIDLIN":1000,"ITMREF":"CDROM2","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000012","SIDLIN":2000,"ITMREF":"CDROM2","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000}]}}', " parse multiple object in 1 header file and 1 body file");

	fileBuff["CABFAC1"] = fs.readFile(_defDataDir + "ediFiles/CABFACMultiple1.txt", 'utf-8', _).replace(/\r\n/g, "\n");;
	fileBuff["CABFAC2"] = fs.readFile(_defDataDir + "ediFiles/CABFACMultiple2.txt", 'utf-8', _).replace(/\r\n/g, "\n");;

	fileBuff["LINFAC1"] = fs.readFile(_defDataDir + "ediFiles/LINFACMultiple1.txt", 'utf-8', _).replace(/\r\n/g, "\n");;
	fileBuff["LINFAC32"] = fs.readFile(_defDataDir + "ediFiles/LINFACMultiple2.txt", 'utf-8', _).replace(/\r\n/g, "\n");;
	res = parser.mapParse["fixedLength"](sortInput(fileBuff, configParser), configParser, prototype);

	strictEqual(JSON.stringify(res), '{"FCC11014VEN00000014":{"NUM":"FCC11014VEN00000014","BPR":"ESP0001","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"AMTNOT":66000,"SIVTYP":"FAC","ACCDAT":"2014-02-24","ESIH1SID":[{"NUM":"FCC11014VEN00000014","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000014","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000}]},"FCC11014VEN00000012":{"NUM":"FCC11014VEN00000012","BPR":"ESP0002","CPY":"110","FCY":"C110","BPAPAY":"A01","CUR":"EUR","PTE":"CHEQ100COMPTANT","AMTATI":75880,"AMTNOT":66000,"SIVTYP":"FAC","ACCDAT":"2014-02-24","ESIH1SID":[{"NUM":"FCC11014VEN00000012","SIDLIN":1000,"ITMREF":"CDROM","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000012","SIDLIN":2000,"ITMREF":"CDROM1","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000},{"NUM":"FCC11014VEN00000012","SIDLIN":1000,"ITMREF":"CDROM2","ITMDES":"CD-ROM 16X","SAU":"Un","QTY":3,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":30000},{"NUM":"FCC11014VEN00000012","SIDLIN":2000,"ITMREF":"CDROM2","ITMDES":"CD-ROM1 16X","SAU":"Un","QTY":2,"GROPRI":1000000,"NETPRI":1000000,"AMTLIN":20000}]}}', " parse multiple object in multiple header file and multiple line file");

	var mmFile = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSORDERFlagFixedLength.json", 'utf-8', _));
	var prototype = JSON.parse(fs.readFile(_defDataDir + "context/protoSORDERTenor.json", 'utf-8', _));

	var configParser = createContextParser(mmFile, sqMap);
	var fileBuff = {};
	fileBuff["SORDERTENOR.edi"] = fs.readFile(_defDataDir + "ediFiles/SORDERTERNOR.edi", 'utf-8', _).replace(/\r\n/g, "\n");;
	var res = parser.mapParse["fixedLength"](fileBuff, configParser, prototype);

	strictEqual(JSON.stringify(res), '{"":{"BLANK":"250","CUSORDREF":"00243060","EDISOHTYP":"0","EDIFUNCTION":"9","CUR":"EU","EDIDLVDAT":"R2015040","EDIDLVTIME":"7","EDIBPCCOD":"3020400233805","EXCEDICOD":"3011005400105","EDIBPCINV":"3020400233805","SOHR_NOTES":[{"TXTID":"GEN","TEXT":"ATTENTION : IL EST IMPERATIF DE RESPECTER L\'HORAIRE DE LIVRAISON. TOUT MANQUANT OU RETARD OCCASIONNERONT DES PENALITES."}],"SOHR_SOP":[{"EANCOD":"9999999999999","EDISAU":"PCE","DEMDLVDATLIN":"2015-04-25"},{"EANCOD":"9999999999999","EDISAU":"PCE","DEMDLVDATLIN":"2015-04-25"}],"SOHSOP":[{"ITMDES":"CARGO TABLE GRIS ORAGE","QTY":2},{"ITMDES":"CARGO TABLE PIMENT","QTY":1}]}}', " parse fixed length file with flag for line");
	start();
});
asyncTest("serialized fixed size ", function(_) {


	var json = {
		"ESIH1SID": [{
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 1000,
			"ITMREF": "CDROM",
			"ITMDES": "CD-ROM 16X",
			"SAU": "Un",
			"QTY": 3,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 30000
		}, {
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 2000,
			"ITMREF": "CDROM1",
			"ITMDES": "CD-ROM1 16X",
			"SAU": "Un",
			"QTY": 2,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 20000
		}],
		"NUM": "FCC11014VEN00000014",
		"BPR": "ESP0001",
		"CPY": "110",
		"FCY": "C110",
		"BPAPAY": "A01",
		"CUR": "EUR",
		"PTE": "CHEQ100COMPTANT",
		"AMTATI": 75880,
		"AMTNOT": 66000,
		"SIVTYP": "FAC",
		"ACCDAT": "2014-02-24"
	};
	var seqFile = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSIH.json", 'utf-8', _));
	var configParser = createContextSerializer(seqFile, sqMap, {
		C: "CABFAC",
		L: "LINFAC"
	});

	var fileBuff = {};
	var prototype = JSON.parse(fs.readFile(_defDataDir + "context/fullPrototype.json", 'utf-8', _));

	fileBuff["CABFAC"] = fs.readFile(_defDataDir + "ediFiles/CABFAC", 'utf-8', _).replace(/\r\n/g, "\n").replace(/\r\n/g, "\n");;

	fileBuff["LINFAC"] = fs.readFile(_defDataDir + "ediFiles/LINFAC", 'utf-8', _).replace(/\r\n/g, "\n").replace(/\r\n/g, "\n");;

	var res = serializer.mapSerialize["fixedLength"](json, configParser, prototype);
	strictEqual(res !== null, true, "with correct json and good proto,  generate string for files ok ");

	strictEqual(res["CABFAC"], fileBuff["CABFAC"], "with correct json and good proto,  generate cabfac ok") // space at the end are remove automatically on certains IDE;
	strictEqual(res["LINFAC"], fileBuff["LINFAC"], "with correct json and good proto,  generate cabfac ok") // space at the end are remove automatically on certains IDE;


	var json = {
		"ESIH1SID": [{
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 1000,
			"ITMREF": "CDROM",
			"ITMDES": "CD-ROM 16X",
			"SAU": "Un",
			"QTY": 3,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 30000
		}, {
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 2000,
			"ITMREF": "CDROM1",
			"ITMDES": "CD-ROM1 16X",
			"SAU": "Un",
			"QTY": 2,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 20000
		}, {
			"NUM": "",
			"SIDLIN": "",
			"ITMREF": "",
			"ITMDES": "",
			"SAU": "",
			"QTY": "",
			"GROPRI": "",
			"NETPRI": "",
			"AMTLIN": ""
		}],
		"NUM": "FCC11014VEN00000014",
		"BPR": "ESP0001",
		"CPY": "110",
		"FCY": "C110",
		"BPAPAY": "A01",
		"CUR": "EUR",
		"PTE": "CHEQ100COMPTANT",
		"AMTATI": 75880,
		"AMTNOT": 66000,
		"SIVTYP": "FAC",
		"ACCDAT": "2014-02-24"
	};
	var res = serializer.mapSerialize["fixedLength"](json, configParser, prototype);
	strictEqual(res !== null, true, "with correct json and good proto,  generate string for files ok with empty line ");

	strictEqual(res["CABFAC"], fileBuff["CABFAC"], "with correct json and good proto,  generate cabfac ok with empty line ") // space at the end are remove automatically on certains IDE;


	strictEqual(res["LINFAC"], fileBuff["LINFAC"], "with correct json and good proto,  generate linfac ok with empty line ") // space at the end are remove;

	var json = {
		"ESIH1SID": [{
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 1000,
			"ITMREF": "CDROM",
			"ITMDES": "CD-ROM 16X",
			"SAU": "Un",
			"QTY": 3,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 30000,
			"SCOL": [{
				"TEST": "VALT"
			}]
		}, {
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 2000,
			"ITMREF": "CDROM1",
			"ITMDES": "CD-ROM1 16X",
			"SAU": "Un",
			"QTY": 2,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 20000,
			"SCOL": [{
				"TEST": "VALY"
			}]
		}],
		"NUM": "FCC11014VEN00000014",
		"BPR": "ESP0001",
		"CPY": "110",
		"FCY": "C110",
		"BPAPAY": "A01",
		"CUR": "EUR",
		"PTE": "CHEQ100COMPTANT",
		"AMTATI": 75880,
		"AMTNOT": 66000,
		"SIVTYP": "FAC",
		"ACCDAT": "2014-02-24"
	};
	var prototype = JSON.parse(fs.readFile(_defDataDir + "context/BadProto.json", 'utf-8', _));
	try {
		var res = serializer.mapSerialize["fixedLength"](json, configParser, prototype);
		ok(false, "ERROR CASE : can't serialize , some properties doesn't match prototype ");
	} catch (e) {
		ok(true, "ERROR CASE : can't serialize , some properties doesn't match prototype  mess:" + e.message);

	}
	prototype = JSON.parse(fs.readFile(_defDataDir + "context/fullPrototype.json", 'utf-8', _));
	json.NONEXISTPROP = "test";
	var res = serializer.mapSerialize["fixedLength"](json, configParser, prototype);
	strictEqual(res !== null, true, "with correct json and good proto,  generate string for files ok ");

	strictEqual(res["CABFAC"], fileBuff["CABFAC"], "with non exists property json and good proto,  generate cabfac ok") // space at the end are remove automatically on certains IDE;
	strictEqual(res["LINFAC"], fileBuff["LINFAC"], "with non exists property json and good proto,  generate cabfac ok") // space at the end are remove automatically on certains IDE;


	fileBuff["LINFAC"] = fs.readFile(_defDataDir + "ediFiles/LINFACSCOL.txt", 'utf-8', _).replace(/\r\n/g, "\n");;
	seqFile = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSIHArray.json", 'utf-8', _));
	prototype = JSON.parse(fs.readFile(_defDataDir + "context/EDISIH1SCOL_$details.json", 'utf-8', _));

	var configParser = createContextSerializer(seqFile, sqMap, {
		C: "CABFAC",
		L: "LINFAC"
	});
	var res = serializer.mapSerialize["fixedLength"](json, configParser, prototype);

	console.log("LINFAC  '" + res["LINFAC"] + "'");
	strictEqual(res["CABFAC"], fileBuff["CABFAC"], "with CABFAC object or array of object of array ok") // space at the end are remove automatically on certains IDE;
	strictEqual(res["LINFAC"], fileBuff["LINFAC"], "with LINFAC object or array of object of array ok") // space at the end are remove automatically on certains IDE;
	start();

});
asyncTest("serialize import file ", function(_) {

	var json = JSON.parse(fs.readFile(_defDataDir + "context/jsonImport.json", 'utf-8', _));


	var messageMapping = JSON.parse(fs.readFile(_defDataDir + "context/messageMappingSOH.json", 'utf-8', _));

	var configParser = createContextSerializer(messageMapping, mmMap, {}, true);

	var prototype = JSON.parse(fs.readFile(_defDataDir + "context/fullPrototypeImport.json", 'utf-8', _));
	var res = serializer.mapSerialize["delimited"](json, configParser, prototype);

	var result = '"ASN","CDE-2010-0491","LS","20100921","","ASN","EUR",""\n' +
		'"CD100","Camion semi remorque rouge","Un","300000"\n' +
		'"ARTICLE2","Désignation article 2","Un",""\n' +
		'"ARTICLE2","Désignation article 2","Un",""\n' +
		'"ARTICLE4","ARTICLE4","Un",""\n';


	strictEqual(res[configParser.fileName], result, "serialize SOH in delimited file ok");

	res = serializer.mapSerialize["sepField"](json, configParser, prototype);
	result = "ASN,CDE-2010-0491,LS,20100921,ASN,EUR,CD100,Camion semi remorque rouge,Un,300000ARTICLE2,Désignation article 2,Un,ARTICLE2,Désignation article 2,Un,ARTICLE4,ARTICLE4,Un,";

	strictEqual(res[configParser.fileName], result, " serialize SOH with sepField ok ");

	res = serializer.mapSerialize["sepRecord"](json, configParser, prototype);
	result = "ASN,CDE-2010-0491,LS,20100921,ASN,EUR,CD100,Camion semi remorque rouge,Un,300000\nARTICLE2,Désignation article 2,Un,\nARTICLE2,Désignation article 2,Un,\nARTICLE4,ARTICLE4,Un,\n";

	strictEqual(res[configParser.fileName], result, " serialize SOH with sepRecord ok ");


	start();
});

asyncTest("parser", function(_) {
	_initPrototypeCache(_, db);
	// store in mongodb all needed to perform the test
	var seqFileJson = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSIH.json", 'utf-8', _));

	var configParser = createContextParser(seqFileJson, sqMap);
	var fileBuff = {};
	fileBuff["CABFAC"] = fs.readFile(_defDataDir + "ediFiles/CABFAC", 'utf-8', _).replace(/\r\n/g, "\n");;
	fileBuff["LINFAC"] = fs.readFile(_defDataDir + "ediFiles/LINFAC", 'utf-8', _).replace(/\r\n/g, "\n");;
	var jsonSIH = {
		"NUM": "FCC11014VEN00000014",
		"BPR": "ESP0001",
		"CPY": "110",
		"FCY": "C110",
		"BPAPAY": "A01",
		"CUR": "EUR",
		"PTE": "CHEQ100COMPTANT",
		"AMTATI": 75880,
		"AMTNOT": 66000,
		"SIVTYP": "FAC",
		"ACCDAT": "2014-02-24",
		"ESIH1SID": [{
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 1000,
			"ITMREF": "CDROM",
			"ITMDES": "CD-ROM 16X",
			"SAU": "Un",
			"QTY": 3,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 30000
		}, {
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 2000,
			"ITMREF": "CDROM1",
			"ITMDES": "CD-ROM1 16X",
			"SAU": "Un",
			"QTY": 2,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 20000
		}]
	};
	var protocolJson = JSON.parse(fs.readFile(_defDataDir + "context/protocol.json", 'utf-8', _));



	var res = parser.parse(_, {
		configParser: configParser,
		prototype: prototype_EDISIH1,
		input: fileBuff,
		db: db
	});
	strictEqual(JSON.stringify(res.FCC11014VEN00000014), JSON.stringify(jsonSIH), "parse ok ");

	var seqFileJsonReal = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSIHReal.json", 'utf-8', _)); // for some reasons the collection name in prototype is not the same as we have in the decribe we manage both case.

	var configParser = createContextParser(seqFileJsonReal, sqMap);



	res = parser.parse(_, {
		configParser: configParser,
		prototype: prototype_EDISIH1,
		input: fileBuff,
		db: db
	});
	var jsonSIH = {
		"NUM": "FCC11014VEN00000014",
		"BPR": "ESP0001",
		"CPY": "110",
		"FCY": "C110",
		"BPAPAY": "A01",
		"CUR": "EUR",
		"PTE": "CHEQ100COMPTANT",
		"AMTATI": 75880,
		"AMTNOT": 66000,
		"SIVTYP": "FAC",
		"ACCDAT": "2014-02-24"
	};
	strictEqual(JSON.stringify(res.FCC11014VEN00000014), JSON.stringify(jsonSIH), "parse with seqFile that describe collection that not correspond to the prototype ok ");


	start();
});

asyncTest(" serialize edi file", function(_) {
	_initPrototypeCache(_, db);
	// store in mongodb all needed to perform the test
	var seqFileJson = JSON.parse(fs.readFile(_defDataDir + "context/seqFileSIH.json", 'utf-8', _));

	var configParser = createContextSerializer(seqFileJson, sqMap, {
		C: "CABFAC",
		L: "LINFAC"
	}, false);
	var protocolJson = JSON.parse(fs.readFile(_defDataDir + "context/protocol.json", 'utf-8', _));
	var fileBuff = {};
	fileBuff["CABFAC"] = fs.readFile(_defDataDir + "ediFiles/CABFAC", 'utf-8', _).replace(/\r\n/g, "\n");;
	fileBuff["LINFAC"] = fs.readFile(_defDataDir + "ediFiles/LINFAC", 'utf-8', _).replace(/\r\n/g, "\n");;

	var jsonSIH = {
		"NUM": "FCC11014VEN00000014",
		"BPR": "ESP0001",
		"CPY": "110",
		"FCY": "C110",
		"BPAPAY": "A01",
		"CUR": "EUR",
		"PTE": "CHEQ100COMPTANT",
		"AMTATI": 75880,
		"AMTNOT": 66000,
		"SIVTYP": "FAC",
		"ACCDAT": "2014-02-24",
		"ESIH1SID": [{
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 1000,
			"ITMREF": "CDROM",
			"ITMDES": "CD-ROM 16X",
			"SAU": "Un",
			"QTY": 3,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 30000
		}, {
			"NUM": "FCC11014VEN00000014",
			"SIDLIN": 2000,
			"ITMREF": "CDROM1",
			"ITMDES": "CD-ROM1 16X",
			"SAU": "Un",
			"QTY": 2,
			"GROPRI": 1000000,
			"NETPRI": 1000000,
			"AMTLIN": 20000
		}]
	};


	var res = serializer.serialize(_, {
		configSerializer: configParser,
		prototype: prototype_EDISIH1,
		json: jsonSIH,
		action: "edi",
		db: db
	});

	strictEqual(Object.keys(res).length, 2, "number of file ok");
	strictEqual(res["CABFAC"], fileBuff["CABFAC"], "cabfac ok ");
	strictEqual(res["LINFAC"], fileBuff["LINFAC"], "linfac ok ");



	try {
		var res = serializer.serialize(_, {
			configSerializer: configParser,
			json: jsonSIH,
			action: "edi",
			db: db
		});
		ok(false, "ERROR CASE : serialize on a non exists representation");

	} catch (e) {
		ok(true, "ERROR CASE : serialize on a non exists representation mess:" + e.stack);
	}




	start();
});

asyncTest(" serialize import files", function(_) {
	_initPrototypeCache(_, db);

	// store in mongodb all needed to perform the test
	var messageMappingJson = JSON.parse(fs.readFile(_defDataDir + "context/messageMappingSOH.json", 'utf-8', _));

	var configParser = createContextSerializer(messageMappingJson, mmMap, {
		C: "CABFAC",
		L: "LINFAC"
	}, true);
	var protocolJson = JSON.parse(fs.readFile(_defDataDir + "context/protocol.json", 'utf-8', _));
	var fileBuff = fs.readFile(_defDataDir + "context/importSOH.csv", 'utf-8', _).replace(/\r\n/g, "\n");;

	var jsonSOH = JSON.parse(fs.readFile(_defDataDir + "context/jsonImport.json", 'utf-8', _));


	var res = serializer.serialize(_, {
		configSerializer: configParser,
		json: jsonSOH,
		prototype: prototype_EDIS0H2,
		action: "import",
		db: db
	});
	strictEqual(res !== null, true, "serialization ok ");

	strictEqual(res[Object.keys(res)[0]], fileBuff, "import file content ok");


	try {
		var res = serializer.serialize(_, {
			configSerializer: configParser,
			json: jsonSOH,
			action: "import",
			db: db
		});
		ok(false, "ERROR CASE : serialize on a non exists representation");

	} catch (e) {
		ok(true, "ERROR CASE : serialize on a non exists representation mess:" + e.message);
	}

	start();
});

asyncTest("parse xml file", function(_) {


	start();
});

asyncTest("clean edi entity mongodb", 0, function(_) {
	EdiEntity.dropAllEdiCacheEntity(_, {
		db: db
	});
	EdiProcess.removeAllEdiProcess(_, {
		db: db
	});

	start();
});