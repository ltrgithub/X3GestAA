"use strict";
//This file must be independent of Syracuse modules because it will be used by the patch integration process before Syracuse has been started.
//dependencies to load balancer allowed
var fs = require('streamline-fs');
var util = require('util');
var crypto = require('crypto');
var path = require('path');
var os = require('os');
var locale = require("syracuse-core/lib/locale");
var relNumberCmp = require("syracuse-core/lib/helpers").relNumberCmp;
var tracer = require('syracuse-trace/lib/helper').getTracer('patch');
// polling timeout (10 minutes)
var PATCH_INTEGRATION_TIMEOUT = 600000;
var PATCH_INTEGRATION_POLLING = 5000;

// file for version information at customer's site (redundant information - also in syracuse-load/lib/balancer._js)
exports.VERSION_FILE = "version.json";

// this variable contains the data set by the wait function and read by the final patch integration function (finalIntegration).
var waitData;

// file containing extra functions which should be executed before restarting the program
exports.EXTRA_FUNCTIONS = 'extra.fkt';
//file containing extra functions which should be executed before restarting the program
exports.EXTRA_FUNCTIONS_OLD = 'extra_o.fkt';
// excluded files and directories
exports.RELEASE_DIRECTORY = "rel";
exports.TEMP_DIRECTORY = "temp";
// special directory with release contents for patch which exceeds release border
exports.SPECIAL_DIRECTORY = "special";
// base directory of installation
exports.BASE_DIRECTORY = path.join(__dirname, "../../../");
// excludes for normal copying
var EXCLUDES_COPY = {};
EXCLUDES_COPY[exports.RELEASE_DIRECTORY] = "";
EXCLUDES_COPY[exports.SPECIAL_DIRECTORY] = "";
EXCLUDES_COPY[exports.TEMP_DIRECTORY] = "";
// top level excludes for checksum verification
var EXCLUDES_CHECKSUM = {};
EXCLUDES_CHECKSUM[exports.RELEASE_DIRECTORY] = "";
EXCLUDES_CHECKSUM[exports.TEMP_DIRECTORY] = "";
EXCLUDES_CHECKSUM["nodelocal.js"] = "";
EXCLUDES_CHECKSUM[exports.VERSION_FILE] = "";
EXCLUDES_CHECKSUM["license.json"] = "";

/// Description: auxiliary functions for IO and streamline wrappers

// writes content into file and creates directory if necessary
// parameters: content may be buffer or string. If it is a string, encoding 'utf8' will be assumed 

function writeFile(targetFile, content, _) {
	// tracer.debug && tracer.debug("WRITEFILE "+targetFile)
	try { // delete file first (to avoid problems with lower/upper case renaming under Windows)
		fs.unlink(targetFile, _);
	} catch (e) {
		tracer.error && tracer.error("Error unlink " + e);
	}
	try {
		fs.writeFile(targetFile, content, _);
	} catch (e) {
		if (e.code === 'ENOENT') { // maybe directory does not exist
			mkdirs(targetFile, _);
			fs.writeFile(targetFile, content, _);
		} else if (e.code === 'EACCES' || e.code === 'EPERM') { // no write permission
			try {
				fs.chmod(targetFile, '666', _);
			} catch (e) { // ignore 
				tracer.error && tracer.error("Error chmod2 " + e);
			}
			fs.writeFile(targetFile, content, _);
		} else throw e;
	}
}
exports.writeFile = writeFile;

/// creates a write stream for a file and creates intermediate directories

function createWriteStream(targetFile, _) {
	try {
		return fs.createWriteStream(targetFile);
	} catch (e) {
		if (e.code === 'ENOENT') { // maybe directory does not exist
			mkdirs(targetFile, _);
			return fs.createWriteStream(targetFile);
		} else throw e;
	}
}

exports.createWriteStream = createWriteStream;

/// opens a file for writing and creates intermediate directories

function open(targetFile, _) {
	try {
		return fs.open(targetFile, "w", _);
	} catch (e) {
		if (e.code === 'ENOENT') { // maybe directory does not exist
			mkdirs(targetFile, _);
			return fs.open(targetFile, "w", _);
		} else if (e.code === 'EPERM' || e.code === 'EACCES') {
			fs.chmod(targetFile, '666', _);
			return fs.open(targetFile, "w", _);
		} else throw e;
	}
}

exports.open = open;

/// mkdirs
/// create directory including all intermediate directories. Accepts backslash as directory separator

function mkdirs(targetFile, _) {
	var segs = targetFile.split(/[\\\/]+/);
	var p = '';
	var i = 0;
	while (i < segs.length - 1) {
		var seg = segs[i];
		p += (i ? '/' : '') + seg;
		if (p !== '' && !exists(p, _)) fs.mkdir(p, _);
		i++;
	}
}

// internal recursive function called by copyRec

function _copyRecInt(start, target, exclude, fast, _) {
	if (!exists(target, _)) { // create target directory
		fs.mkdir(target, _);
		fast = false;
	}
	if (!(fs.stat(target, _).isDirectory())) { // target exists and is no directory: delete it first
		fs.unlink(target, _);
		fs.mkdir(target, _);
		fast = false;
	}
	return _copyContents(start, target, exclude, fast, _); // copy contents of directory
}

/// copyRec
/// copy start directory recursively into target directory
/// parameters: start: directory (must exist and be a directory)
///			 target: target which should contain the contents of start in the end (except for excluded files). Will be created if necessary
///			 exclude: object with file names (without paths) which should be regarded as non existing in the start directory (will be removed from target directory if they exist)
///          fast: just copy files and directories whose checksums do not fit. If the checksums fit, do not copy anything even if files do not exist in target directory.

function copyRec(start, target, fast, _) {
	return _copyRecInt(start, target, EXCLUDES_COPY, fast, _); // copy contents of directory
}
exports.copyRec = copyRec;

/// _copyContents
/// copy contents start directory recursively into target directory
/// parameters: start: directory (must exist and be a directory)
///			 target: target which should contain the contents of start in the end (except for excluded files)
///			 exclude: object with file names (without paths) which should be excluded from copy action
///          fast: just copy files and directories whose checksums do not fit
///

function _copyContents(start, target, exclude, fast, _) {
	var checksumsStart;
	var checksumsTarget = {};
	var changes = false;
	var startFiles;
	if (fast) {
		checksumsStart = JSON.parse(fs.readFile(start + "/.checksums", "utf8", _));
		startFiles = Object.keys(checksumsStart);
		if (exclude && exists(start + "/" + exports.VERSION_FILE, _)) // root directory: copy version file
			startFiles.unshift(exports.VERSION_FILE); // version file should be copied as last file
		try {
			checksumsTarget = JSON.parse(fs.readFile(target + "/.checksums", "utf8", _));
		} catch (e) {
			fast = false;
			startFiles.push(".checksums");
		} // 
	} else {
		startFiles = fs.readdir(start, _);
	}
	var targetFiles = fs.readdir(target, _);
	var targetFilesHash = {};
	var i = targetFiles.length;
	while (--i >= 0) {
		targetFilesHash[targetFiles[i]] = "";
	}
	i = startFiles.length;
	while (--i >= 0) {
		var filename = startFiles[i];
		if (exclude && (filename in exclude || filename.lastIndexOf('.bbb') === filename.length - 4)) continue; // exclude this file
		var startFile = start + "/" + filename;
		var targetFile = target + "/" + filename;
		if (fast && checksumsStart[filename]) {
			if (checksumsStart[filename] === checksumsTarget[filename]) { // checksums match: do not copy
				delete targetFilesHash[filename];
				continue;
			}
			if (checksumsStart[filename].substr(0, 2) === "D ") { // fast copy subdirectory
				_copyRecInt(startFile, targetFile, null, true, _);
				delete targetFilesHash[filename];
				continue;
			}
		}
		changes = true;
		if (fs.stat(startFile, _).isDirectory()) { // start is directory: copy recursively
			_copyRecInt(startFile, targetFile, null, false, _);
		} else { // start file is no directory
			if (filename in targetFilesHash && fs.stat(targetFile, _).isDirectory()) rmdirRec(targetFile, _);
			// copy file
			var buffer = fs.readFile(startFile, _);
			var stat = fs.stat(startFile, _);
			// use permissions of original file to set permissions of copied file
			var permissions = (stat.mode * 1 & 0xFFF).toString(8);
			tracer.debug && tracer.debug("Changed (" + permissions + ") " + startFile);
			writeFile(targetFile, buffer, _);
			fs.chmod(targetFile, permissions, _);
			buffer = null;
		}
		// target file has been handled
		delete targetFilesHash[filename];
	}
	if (exclude) { // root directory: do not delete nodelocal.js and files ending in .bbb
		delete targetFilesHash["nodelocal.js"];
		for (var file in targetFilesHash) {
			if (file.lastIndexOf('.bbb') === file.length - 4) delete targetFilesHash[file];
		}
		if (target.indexOf("..") >= 0) { // do not delete exclude directories when copying into parent directory
			for (var key in exclude) {
				delete targetFilesHash[key];
			}
		}
	}
	targetFiles = Object.keys(targetFilesHash);
	i = targetFiles.length;
	while (--i >= 0) {
		var targetFile = target + "/" + targetFiles[i];
		if (fs.stat(targetFile, _).isDirectory()) rmdirRec(targetFile, _);
		else {
			unlinkP(targetFile, _);
		}
		changes = true;
	}
	if (fast && changes) { // update checksums file only when there are changes
		writeFile(target + "/.checksums", JSON.stringify(checksumsStart), _);
	}

}

function unlinkP(filename, _) {
	try {
		fs.unlink(filename, _);
	} catch (e) {
		if (e.code === 'EPERM') {
			try {
				fs.chmod(filename, '666', _); // allow write access for everybody;
			} catch (e) { // ignore
				tracer.error && tracer.error("Error chmod of " + filename, e);
			}
			fs.unlink(filename, _);
		} else throw e;
	}
}
exports.unlinkP = unlinkP;

//elevates access rights at least to 664, keeps extra bits

function elevate(access_rights) {
	if (!access_rights) return "664";
	var length = access_rights.length;
	var changed = "" + (access_rights.substr(length - 3, 1) * 1 | 0x06); // own
	changed += (access_rights.substr(length - 2, 1) * 1 | 0x06); // group
	changed += (access_rights.substr(length - 1, 1) * 1 | 0x04); // other
	return access_rights.substr(0, length - 3) + changed;
}

exports.elevate = elevate;

// asynchronous file existence test for streamline

function exists(path, _) {
	return fs.exists(path, _);
}

exports.exists = exists;

// delete directory recursively

function rmdirRec(path, _) {
	var files = fs.readdir(path, _);
	var i = files.length;
	/* Loop through and delete everything in the sub-tree after checking it */
	while (--i >= 0) {
		var currFile = path + "/" + files[i];

		if (fs.stat(currFile, _).isDirectory()) // Recursive function back to the beginning
			rmdirRec(currFile, _);
		else {
			unlinkP(currFile, _);
		}
	}
	return fs.rmdir(path, _);
};

exports.rmdirRec = rmdirRec;

/// getChecksumContent
/// reads and parses the contents of the .checksums file in the specified directory
/// returns empty object if file does not exist yet

function getChecksumContent(directory, path, _) {
	var buffer = null;
	try {
		buffer = fs.readFile(directory + "/" + path + "/.checksums", _);
		try {
			var result = JSON.parse(buffer.toString("utf8"));
			return result;
		} catch (e) {
			throw new Error(locale.format(module, "invalidChecksumFile", path, "" + e));
		}
	} catch (e) {
		if (e.code === 'ENOENT') {
			var result = {};
			return result; // file does not exist yet				
		} else throw new Error(locale.format(module, "invalidChecksumFile", path, "" + e));
	}

}

exports.getChecksumContent = getChecksumContent;

/// updateChecksums
/// After applying the patch, the contents of some directories have changed. 
/// The object 'list' contains the names of these directories as keys and the contents of their checksum files as values.
/// This function adds the missing intermediate directories and updates all checksum files
/// return value: checksum of top level directory

function updateChecksums(directory, list, _) {
	var current = Object.keys(list);
	var parents = [];
	var i;
	// add intermediate directories. Strategy: loop over all directories and add parent directory (and its checksums contents) if it does not exist yet.
	// after the loop over directories has finished, start a new loop of the added parent directories	
	while (current.length > 0) {
		i = current.length;
		while (--i >= 0) {
			var parent = path.dirname(current[i]);
			if (!(parent in list)) { // parent directory not yet in the list
				parents.push(parent);
				list[parent] = getChecksumContent(directory, parent, _);
			}
		}
		current = parents;
		parents = [];
	}

	// sort with ascending length so that no directory will be scanned before any of its subdirectories
	// in order to assure that root directory "." is really the last, a special comparison puts the char code of "." (46) to be the lowest value.
	var dirs = Object.keys(list).sort(function(a, b) {
		return (a.length - b.length || a.length === 1 && Math.abs(a.charCodeAt(0) - 46) - Math.abs(b.charCodeAt(0) - 46) || 0);
	});
	i = dirs.length;
	while (--i >= 0) {
		// write checksum file of current directory
		var dir = dirs[i];
		var buffer = new Buffer(JSON.stringify(list[dir]), "utf8");
		var sha1 = get_sha1_binary(buffer);
		writeFile(directory + "/" + dir + "/.checksums", buffer, _);
		if (dir !== ".") {
			// update contents of checksums of parent directory. The ordering of dirs assures that the checksums of the parent directory will be written later
			list[path.dirname(dir)][path.basename(dir)] = "D " + sha1;
		} else {
			return sha1;
		}
	}
	return "";
}

// files which should be excluded from checksum generation in any directory (in contrast to EXCLUDES_CHECKSUM which is just for the top level directory

function excludeFile(file) {
	if (file.substr(-4) === ".bbb" || file === ".checksums") {
		return true;
	}
	return false;
}

exports.updateChecksums = updateChecksums;

/// creates checksum files. Assumes that all line endings are unix style endings (therefore no distinction between binary and text files)
/// parameters: directory: directory to search
///			 errors: if not null, checksum files will be assumed to exist and to be correct, and mismatches will be appended to array
/// return value: checksum of current directory (i. e. of .checksums file)

function makeChecksums(path, _) {
	return makeChecksumsInt(path, EXCLUDES_CHECKSUM, _);
}

// recursive function: extra excludes parameter

function makeChecksumsInt(path, exclude, _) {
	var files = fs.readdir(path, _);
	var contents = {};
	var i = files.length;
	while (--i >= 0) {
		var file = files[i];
		// do not include checksums file and backup files
		if (excludeFile(file) || (exclude && file in exclude)) {
			continue;
		}
		var filePath = path + "/" + file;
		var currFile = fs.stat(filePath, _);
		if (currFile.isDirectory()) { // Recursive function back to the beginning
			var sha1 = makeChecksumsInt(filePath, null, _);
			contents[file] = "D " + sha1;
		} else {
			var buffer = fs.readFile(filePath, _);
			var sha1 = get_sha1_binary(buffer);
			contents[file] = "F " + sha1;
		}
	}
	var buf = new Buffer(JSON.stringify(contents), "utf8");
	writeFile(path + "/.checksums", buf, _);
	return get_sha1_binary(buf);
}

exports.makeChecksums = makeChecksums;

/// check checksums and compares them with total checksum in version file

function checkChecksumsV(path, _) {
	var errors = [];
	var sha1 = checkChecksums(path, errors, EXCLUDES_CHECKSUM, _);
	try {
		var version = readVersionFile(path, _);
		if (sha1 != version.sha1) errors.push(locale.format(module, "checksumMismatch"));
	} catch (e) {
		errors.push(locale.format(module, "versionFileError", "" + e));
	}
	return errors;
}

exports.checkChecksumsV = checkChecksumsV;

/// checks checksum files. Assumes that all line endings are unix style endings (therefore no distinction between binary and text files)
/// parameters: directory: directory to search
///			 errors: if not null, checksum files will be assumed to exist and to be correct, and mismatches will be appended to array
/// return value: checksum of current directory (i. e. of .checksums file)

function checkChecksums(path, errors, exclude, _) {
	try {
		var files = fs.readdir(path, _);
		var contents = {};
		var result = "";
		try {
			var buf = fs.readFile(path + "/.checksums", "utf8", _);
			contents = JSON.parse(buf);
			result = get_sha1_binary(buf);
		} catch (e) {
			errors.push(e.toString());
			return result;
		}
		for (var i = 0; i < files.length; i++) {
			var file = files[i];
			var filePath = path + "/" + file;
			// do not include checksums file and backup files
			if (excludeFile(file) || (exclude && file in exclude)) continue;
			if (!(file in contents)) {
				errors.push(locale.format(module, "newFile", filePath));
				continue;
			}
			var currFile = fs.stat(filePath, _);
			if (currFile.isDirectory()) { // Recursive function back to the beginning
				if (contents[file].substr(0, 2) != "D ") errors.push(locale.format(module, "noDirectoryExpected", filePath));
				else {
					var sha1 = checkChecksums(path + "/" + file, errors, null, _);
					if (contents[file] !== "D " + sha1) {
						errors.push(locale.format(module, "wrongChecksum", filePath));
					}
				}
			} else {
				var buffer = fs.readFile(filePath, _);
				var sha1 = get_sha1_binary(buffer);
				if (contents[file] !== "F " + sha1) {
					if (contents[file].substr(0, 2) != "F ") errors.push(locale.format(module, "directoryExpected", filePath));
					else errors.push(locale.format(module, "wrongChecksum", filePath));
				}
			}
			delete contents[file];
		}
		// errors for remaining files
		for (var file in contents) {
			errors.push(locale.format(module, "deletedFile", path + "/" + file));
		}
	} catch (err) {
		errors.push(locale.format(module, "otherError", "" + err));
	}
	return result;
}

exports.checkChecksums = checkChecksums;

/// get sha1 sum of binary content

function get_sha1_binary(buffer) {
	var shasum = crypto.createHash('sha1');
	// GIT specific header
	shasum.update("blob " + buffer.length + "\0", "utf8");
	shasum.update(buffer);
	var digest = shasum.digest('hex');
	return digest;
}

exports.get_sha1_binary = get_sha1_binary;

/// tests SHA1 checksums for equality or whether one is an abbreviation of the other

function equal_sha1(sum, sumGit) {
	if (sum === undefined || sumGit === undefined) return false;
	if (sum.substr(0, 2) === "F ") sum = sum.substr(2); // strip directory/file information in checksums file
	return sum === sumGit || sum.substr(0, sumGit.length) == sumGit;
}

exports.equal_sha1 = equal_sha1;

/// check whether checksum is correct for binary file
/// input

function check_sha1_binary(buffer, sha1) {
	return (get_sha1_binary(buffer).substr(0, sha1.length) === sha1);
}
exports.check_sha1_binary = check_sha1_binary;



// find next: create next release number which is greater than the highest with same prefix.
// the pattern is a release number but contains an X which should be replaced with the next number.
// the list is an array of release number which is ordered according to release number ordering (highest number last)
// returns the highest found release number and the new release number as an array

function nextRelease(list, pattern) {
	var r = /(^|\.)X(?:\.|$)/.exec(pattern);
	if (!r) throw new Error("Wrong pattern");
	var index = r.index + r[1].length;
	var left = pattern.substr(0, index);
	var right = pattern.substr(index + 1);
	if (!/^\d+(\.\d+)*$/.test(left + "0" + right)) throw new Error("Invalid characters in pattern");
	var newNumber = 0;
	var item = "";
	if (list.length) {
		// find release before this release
		var index = left.indexOf('.');
		var i = list.length - 1;
		var beginning = left.substr(0, index + 1);
		item = list[i];
		while (index >= 0 && i >= 0) {
			if (item.substr(0, index + 1) <= beginning) {
				index = left.indexOf('.', index + 1);
				if (index >= 0) {
					beginning = left.substr(0, index + 1);
					continue;
				} else break;
			}
			if (--i >= 0) item = list[i];
		}
		if (i < 0) throw new Error("No previous version found");
		r = /\d+/.exec(item.substr(beginning.length));
		if (r) newNumber = (1 + 1 * r[0]);
	}
	return [item, left + newNumber + right];
}

exports.nextRelease = nextRelease;

function readVersionFile(path, _) {
	return JSON.parse(fs.readFile(path + "/" + exports.VERSION_FILE, "utf8", _));
}

exports.readVersionFile = readVersionFile;


exports.waitfunctionCb = function(cb) {
	waitfunction(cb);
};

// this function is invoked for determining the correct time to start Syracuse for patch replaying
// when there is at most one active server, it just finishes. Otherwise it waits until (by database voting) it is clear
// that this is the first server or waits until the first server has finished the Syracuse initialization.
// the result contains some status data, they are also written to a module global variable in order to 

function waitfunction(_) {
	tracer.info && tracer.info("Start of wait function");
	var dbDriver = require('syracuse-load/lib/dbDriver');
	var db;

	try {
		var ballot = require('syracuse-patch/lib/ballot');
		db = dbDriver.open(null, _);
		var localhostname = os.hostname();
		try {
			var version = readVersionFile(exports.BASE_DIRECTORY, _);
		} catch (e) {
			tracer.error && tracer.error("Version file not readable", e);
			throw new Error(locale.format(module, "versionFileError", "" + e));
		}
		tracer.debug && tracer.debug("local code version " + util.format(version));
		var hostsCollection = dbDriver.createCollection(db, 'Host', _);
		var count = dbDriver.count(hostsCollection, {
			"active": true
		}, _);
		tracer.info && tracer.info("Number of active servers: " + count);
		if (count <= 1) { // no voting when there are no more than 1 active servers
			tracer.debug && tracer.debug("No voting");
			var result = {
				vote: false,
				version: version
			};
			waitData = result;
			return result;
		}
		tracer.debug && tracer.debug("Voting");
		// voting
		var data = {
			release: version.relNumber,
			patch: version.patchNumber
		};
		var localVersion = version.relNumber + "-" + version.patchNumber;
		// into file
		var timestamp = ballot.writeVote(db, data, _);
		// into database
		// write all temporary and obtained data into result object: database instance, timestamp, file voting data, hosts colection,
		// version (of temp directory), is this server the winner of the database voting?
		tracer.info && tracer.info("vote " + localVersion);
		var result = {
			timestamp: timestamp,
			data: data,
			version: version,
			winner: false,
			vote: true
		};
		dbDriver.update(hostsCollection, {
			hostname: localhostname
		}, {
			patchStatus: localVersion + ";" + timestamp
		}, _);
		ballot.wait(1000, _);
		// read database votes
		var hosts = dbDriver.find(hostsCollection, {}, _);
		// collect database votes
		tracer.debug && tracer.debug("Votes from all servers " + util.format(hosts));
		// first step: find highest version
		var highestVersion = localVersion; // highest version of all versions in database
		hosts.forEach(function(host) {
			var patchStatus = host.patchStatus || "";
			var parts = patchStatus.split(";");
			if (!host.deactivated && relNumberCmp(parts[0], highestVersion) > 0) {
				highestVersion = parts[0];
			};
		});
		tracer.info && tracer.info("Highest version " + highestVersion);
		// choose server with first database vote
		var firstTimeStamp = timestamp * 1;
		var localHostData; // data of local host
		var firstServer = localhostname; // name of server with first database vote
		var stillStartedServer = false; // is there still a non-finished server?
		hosts.forEach(function(host) {
			if (host.hostname === localhostname) localHostData = host;
			var patchStatus = host.patchStatus || "";
			tracer.info && tracer.info("Host " + host.hostname + " patchStatus " + patchStatus);
			var parts = patchStatus.split(";");
			if (!host.deactivated && parts[0] === highestVersion && parts[1] && !parts[2]) {
				var otherTimeStamp = +parts[1];
				tracer.debug && tracer.debug("Timestamps " + firstTimeStamp + " " + otherTimeStamp + " diff " + (firstTimeStamp - otherTimeStamp));
				if (firstTimeStamp - otherTimeStamp < ballot.EXPIRE_TIME && (otherTimeStamp < firstTimeStamp || (otherTimeStamp === firstTimeStamp && host.hostname < firstServer))) {
					firstServer = host.hostname;
					firstTimeStamp = otherTimeStamp;
					tracer.info && tracer.info("Server changed " + firstServer);
				}
				if (highestVersion === parts[0] && !parts[2] && host.started) stillStartedServer = true;
			}
		});
		if (!localHostData) throw new Error(locale.format(module, "noLocal", localhostname));
		if (!firstServer) throw new Error(locale.format(module, "noWinner"));
		// if highest version is bigger than local version or current host not active: do not do anything
		if (relNumberCmp(localVersion, highestVersion) < 0) {
			tracer.info && tracer.info("Local version (" + localVersion + ") is less than highest version: " + highestVersion);
			process.exit(0);
			return null;
		}
		if (localHostData.deactivated) {
			tracer.info && tracer.info("Local host is inactive");
			process.exit(0);
			return null;
		}
		if (firstServer === localhostname) {
			// this is the server which has voted first
			// is there a server with same version which is still started? Wait for some more seconds, allow server to stop. 
			// Normally server should be stopped then (because of 3000ms polling during stop sessions). 
			// Do not test whether server really has stopped after that time because this would be very complicated.
			tracer.info && tracer.info("Winner of database voting");
			if (stillStartedServer) {
				tracer.debug && tracer.debug("Wait some seconds to allow other servers to stop");
				ballot.wait(5000, _);
			}
			result.winner = true;
		} else {
			// this is not the server which has voted first
			// wait for first server (polling)
			if (!_polling(hostsCollection, firstServer, ";END", _)) throw new Error(locale.format(module, "firstTooLong"));
		}
		// start Syracuse environment and patch copying		
		waitData = result;
		return result;
	} finally {
		tracer.debug && tracer.debug("Close db");
		if (db) dbDriver.close(db);
	}
}

exports.waitfunction = waitfunction;

function _polling(hostsCollection, firstServer, searchString, _) {
	tracer.debug && tracer.debug("Poll for server " + firstServer + " and '" + searchString + "'");
	// wait for first server (polling)
	var dbDriver = require('syracuse-load/lib/dbDriver');
	var ballot = require('syracuse-patch/lib/ballot');
	var totalPolling = PATCH_INTEGRATION_TIMEOUT;
	while (totalPolling >= 0) {
		totalPolling -= PATCH_INTEGRATION_POLLING;
		ballot.wait(PATCH_INTEGRATION_POLLING, _);
		var hosts = dbDriver.find(hostsCollection, {
			hostname: firstServer
		}, _);
		var finished = false;
		hosts.forEach(function(host) {
			if (host.patchStatus.indexOf(searchString) >= 0) finished = true;
		});
		tracer.debug && tracer.debug("Polling finished? " + finished);
		if (finished) return true;
	}
	tracer.warn && tracer.warn("Polling exceeded");
	// maximal polling time exceeded: Error
	return false;
}

//patch integration on temp directory: perform special actions and copy files into normal folder
//waitData: object returned from waitfunction;
exports.finalIntegration = function(_) {
	tracer.info && tracer.info("Final patch integration " + util.format(waitData));
	if (!waitData) return false; // no waitData: do nothing
	var dbDriver = require('syracuse-load/lib/dbDriver');
	var db;

	function _markEnd(suffix, _) {
		tracer.debug && tracer.debug("Mark end " + suffix);
		var patchStatus = waitData.version.relNumber + "-" + waitData.version.patchNumber + ";" + waitData.timestamp + ";" + suffix;
		dbDriver.update(hostsCollection, {
			hostname: localhostname
		}, {
			patchStatus: patchStatus
		}, _);
	}
	try {
		var version = waitData.version;
		var localhostname = os.hostname();
		var copying = true;
		if (waitData.vote) {
			var ballot = require('syracuse-patch/lib/ballot');
			db = dbDriver.open(null, _);
			var hostsCollection = dbDriver.createCollection(db, 'Host', _);
			if (waitData.winner) { // the winner of the database voting notifies about end of database update
				_markEnd("END", _);
			}
			// find out whether this host should do the integration
			var firstServer = ballot.readVotes(waitData.data, waitData.timestamp, _);
			if (firstServer !== localhostname) { // wait until first server has finished (polling)
				if (!_polling(hostsCollection, firstServer, ";END_END", _)) {
					_markEnd("END_END1", _);
					throw new Error(locale.format(module, "firstTooLong"));
				}
				copying = false;
			} else {
				// this is the first server
				tracer.info && tracer.info("Winner");
				// test whether copying has already been done
				try {
					var targetVersion = readVersionFile(exports.BASE_DIRECTORY + '/..', _);
				} catch (e) {
					tracer.error && tracer.error("Version file not readable", e);
					throw new Error(locale.format(module, "versionFileError", "" + e));
				}
				tracer.debug && tracer.debug("Target version " + util.format(targetVersion));
				if (targetVersion.relNumber === waitData.version.relNumber && targetVersion.patchNumber * 1 === waitData.version.patchNumber * 1) {
					// patch has already been integrated		
					copying = false;
				}
			}
		}
		tracer.info && tracer.info("Copy? " + copying);
		if (copying) {
			// perform extra actions
			var oldActions;
			var newActions;
			if (exports.exists(exports.BASE_DIRECTORY + "/" + exports.EXTRA_FUNCTIONS_OLD, _)) oldActions = fs.readFile(exports.BASE_DIRECTORY + "/" + exports.EXTRA_FUNCTIONS_OLD, "utf8", _).split(/\r\n|\r|\n/);
			else oldActions = [];
			if (exports.exists(exports.BASE_DIRECTORY + "/" + exports.EXTRA_FUNCTIONS, _)) newActions = fs.readFile(exports.BASE_DIRECTORY + "/" + exports.EXTRA_FUNCTIONS, "utf8", _).split(/\r\n|\r|\n/);
			else newActions = [];
			var oldActionsHash = {};
			var i = oldActions.length;
			while (--i >= 0) {
				oldActionsHash[oldActions[i]] = "";
			}
			i = 0;
			while (i < newActions.length) {
				var action = newActions[i++];
				if (!action || action in oldActionsHash) continue;
				var parts = action.split(" ");
				if (parts.length != 2) {
					tracer.error && tracer.error("Invalid action " + action);
				} else {
					tracer.debug && tracer.debug("Perform action " + action);
					var mod = require(exports.BASE_DIRECTORY + "/" + parts[0]);
					if (parts[1] in mod) {
						var result = mod[parts[1]](_);
						tracer.info && tracer.info("Action " + action + " performed, result " + result);
					} else {
						tracer.warn && tracer.warn("Action " + action + " not available");
					}
				}
			}
			exports.copyRec(exports.BASE_DIRECTORY, exports.BASE_DIRECTORY + "/..", true, _);

			// new release: copy data to release directory


			if (version.patchNumber === "0") {
				exports.copyRec(exports.BASE_DIRECTORY, exports.BASE_DIRECTORY + "/../" + exports.RELEASE_DIRECTORY, true, _);
			} else if (fs.exists(exports.BASE_DIRECTORY + "/" + exports.SPECIAL_DIRECTORY, _)) {
				tracer.info && tracer.info("Special directory available");
				// patch which exceeds release border: copy special directory into release directory and delete special directory
				try {
					var version1 = JSON.parse(fs.readFile(exports.BASE_DIRECTORY + "/" + exports.SPECIAL_DIRECTORY + "/" + exports.VERSION_FILE, "utf8", _));
					tracer.debug && tracer.debug("Contents of special version file " + util.format(version1) + " VERSION " + util.format(version));
					if (version1.relNumber === version.relNumber && version1.patchNumber === "0") {
						tracer.info && tracer.info("Copy special directory");
						exports.copyRec(exports.BASE_DIRECTORY + "/" + exports.SPECIAL_DIRECTORY, exports.BASE_DIRECTORY + "/../" + exports.RELEASE_DIRECTORY, true, _);
					}
				} catch (e) {
					tracer.error && tracer.error("Error in special directory", e);
				}
				rmdirRec(exports.BASE_DIRECTORY + "/" + exports.SPECIAL_DIRECTORY, _);
			}
		}
		// mark as finished
		if (waitData.vote) {
			_markEnd(copying ? "END_END" : "END_END0", _);
		}
		return true;
	} finally {
		if (db) dbDriver.close(db);
	}
};

// collect the results of the checksum check for all directories

function checkChecksumsAll(_) {
	var future = [];
	future[0] = checkChecksumsV(exports.BASE_DIRECTORY, !_);
	if (exists(exports.BASE_DIRECTORY + "/" + exports.RELEASE_DIRECTORY, _)) future[1] = checkChecksumsV(exports.BASE_DIRECTORY + "/" + exports.RELEASE_DIRECTORY, !_);
	if (exists(exports.BASE_DIRECTORY + "/" + exports.TEMP_DIRECTORY, _)) future[2] = checkChecksumsV(exports.BASE_DIRECTORY + "/" + exports.TEMP_DIRECTORY, !_);
	var result = {
		work: _output(future[0], _),
		rel: _output(future[1], _),
		temp: _output(future[2], _)
	};
	return result;
}

exports.checkChecksumsAll = checkChecksumsAll;

// get the error messages for one check

function _output(future, _) {
	if (future) {
		var errors = future(_);
		if (errors.length > 0) {
			return errors;
		}
	}
	return undefined;
}

//deletes file and empty directories. Also removes entries from `checksums` object if given
//Only deletes subdirectories of targetDirectory

function deleteFileDir(targetDirectory, file_from, checksums, _) {
	if (fs.stat(targetDirectory + "/" + file_from, _).isDirectory()) {
		rmdirRec(targetDirectory + "/" + file_from, _);
	} else {
		fs.unlink(targetDirectory + "/" + file_from, _);
	}
	var directory = path.dirname(file_from);
	var list;
	if (checksums) {
		var checksumsObject = checksums[directory];
		delete checksumsObject[path.basename(file_from)];
		list = Object.keys(checksumsObject);
	} else {
		list = fs.readdir(targetDirectory + "/" + directory, _);
	}
	while (list && directory !== "" && list.length === 0) {
		if (checksums) {
			var checksumsFile = targetDirectory + "/" + directory + "/.checksums";
			if (exists(checksumsFile, _)) {
				try {
					fs.unlink(checksumsFile, _);
				} catch (e) {
					tracer.error && tracer.error("Error when deleting checksum file", e);
				}
			}
		}
		fs.rmdir(directory === '.' ? targetDirectory : targetDirectory + "/" + directory, _);
		var dirname = path.basename(directory);
		directory = path.dirname(directory);
		if (directory === ".") directory = "";
		if (checksums) {
			checksumsObject = checksums[directory];
			if (checksumsObject) {
				delete checksumsObject[dirname];
				list = Object.keys(checksumsObject);
			} else {
				list = null;
			}
		} else {
			list = fs.readdir(targetDirectory + "/" + directory, _);
		}
	}
}
exports.deleteFileDir = deleteFileDir;