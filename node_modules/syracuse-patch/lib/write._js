var fs = require('fs');
var util = require('util');
var crypto = require('crypto');
var path = require('path');
var child_process = require('child_process');
var locale = require("syracuse-core/lib/locale");
var config = require('syracuse-main/lib/nodeconfig').config

// file for version information at customer's site (redundant information - also in syracuse-load/lib/balancer._js)
exports.VERSION_FILE = "version.json"; 
// file containing extra functions which should be executed before restarting the program
exports.EXTRA_FUNCTIONS = 'extra.fkt';
//file containing extra functions which should be executed before restarting the program
exports.EXTRA_FUNCTIONS_OLD = 'extra_o.fkt';
// semaphore file
exports.SEMAPHORE = 'semaphore.bbb';
// excluded files and directories
exports.RELEASE_DIRECTORY = "rel";
exports.TEMP_DIRECTORY = "temp";
// base directory of installation
exports.BASE_DIRECTORY = path.join(__dirname, "../../../");
// excludes for normal copying
var EXCLUDES_COPY = {};
EXCLUDES_COPY[exports.RELEASE_DIRECTORY] = ""
EXCLUDES_COPY[exports.TEMP_DIRECTORY] = ""
// top level excludes for checksum verification
var EXCLUDES_CHECKSUM = {};
EXCLUDES_CHECKSUM[exports.RELEASE_DIRECTORY] = ""
EXCLUDES_CHECKSUM[exports.TEMP_DIRECTORY] = ""
EXCLUDES_CHECKSUM["nodelocal.js"] = ""
EXCLUDES_CHECKSUM[exports.VERSION_FILE] = ""
EXCLUDES_CHECKSUM["license.json"] = ""	
	
/// Description: auxiliary functions for IO and streamline wrappers


// writes content into file and creates directory if necessary
// parameters: content may be buffer or string. If it is a string, encoding 'utf8' will be assumed 
function writeFile(targetFile, content, _) {
	// console.log("WRITEFILE "+targetFile)
	try {
		fs.writeFile(targetFile, content, _);
	} catch (e) {
		if (e.code === 'ENOENT') { // maybe directory does not exist
			mkdirs(targetFile, _);
			fs.writeFile(targetFile, content, _);
		} else 
			throw e;
	}
}

exports.writeFile = writeFile;

/// creates a write stream for a file and creates intermediate directories
function createWriteStream(targetFile, _) {
	try {
		return fs.createWriteStream(targetFile);
	} catch (e) {
		if (e.code === 'ENOENT') { // maybe directory does not exist
			mkdirs(targetFile, _);
			return fs.createWriteStream(targetFile);
		} else 
			throw e;
	}
}

exports.createWriteStream = createWriteStream 
	
/// opens a file for writing and creates intermediate directories
function open(targetFile, _) {
	try {
		return fs.open(targetFile, "w", _);
	} catch (e) {
		if (e.code === 'ENOENT') { // maybe directory does not exist
			mkdirs(targetFile, _);
			return fs.open(targetFile, "w", _);
		} else 
			throw e;
	}
}



exports.open = open; 

/// mkdirs
/// create directory including all intermediate directories. Accepts backslash as directory separator
function mkdirs(targetFile, _) {
	var segs = targetFile.split('\/');
	var p = '';
	var i = 0;
	while (i < segs.length-1) {
		var seg = segs[i];
		p += (i ? '/' : '') + seg;
		if (!exists(p, _))
			fs.mkdir(p, _);
		i++;
	}	
}




// internal recursive function called by copyRec
function _copyRecInt(start, target, exclude, fast, _) {
	if (!exists(target, _)) { // create target directory
		fs.mkdir(target, _);
		fast = false;
	}
	if (!(fs.stat(target, _).isDirectory())) { // target exists and is no directory: delete it first
		fs.unlink(target, _);
		fs.mkdir(target, _);
		fast = false;
	}	
	return _copyContents(start, target, exclude, fast, _); // copy contents of directory
}

/// copyRec
/// copy start directory recursively into target directory
/// parameters: start: directory (must exist and be a directory)
///			 target: target which should contain the contents of start in the end (except for excluded files). Will be created if necessary
///			 exclude: object with file names (without paths) which should be regarded as non existing in the start directory (will be removed from target directory if they exist)
///          fast: just copy files and directories whose checksums do not fit. If the checksums fit, do not copy anything even if files do not exist in target directory.
function copyRec(start, target, fast, _) {
	return _copyRecInt(start, target, EXCLUDES_COPY, fast, _); // copy contents of directory
}
exports.copyRec = copyRec;

/// _copyContents
/// copy contents start directory recursively into target directory
/// parameters: start: directory (must exist and be a directory)
///			 target: target which should contain the contents of start in the end (except for excluded files)
///			 exclude: object with file names (without paths) which should be excluded from copy action
///          fast: just copy files and directories whose checksums do not fit
///
function _copyContents(start, target, exclude, fast, _) {
	var checksumsStart;
	var checksumsTarget = {};
	var changes = false;
	var startFiles;
	if (fast) {
		checksumsStart = JSON.parse(fs.readFile(start+"/.checksums", "utf8", _));
		startFiles = Object.keys(checksumsStart);
		if (exclude && exists(start+"/"+exports.VERSION_FILE, _)) // root directory: copy version file
			startFiles.unshift(exports.VERSION_FILE); // version file should be copied as last file
		try {
			checksumsTarget = JSON.parse(fs.readFile(target+"/.checksums", "utf8", _));
		} catch (e) {
			fast = false;
			startFiles.push(".checksums");
		} // 
	} else {
		startFiles = fs.readdir(start, _);
	}
	var targetFiles = fs.readdir(target, _);
	var targetFilesHash = {};
	var i = targetFiles.length;
	while (--i >= 0) {
		targetFilesHash[targetFiles[i]] = "";
	}
	i = startFiles.length;
	while (--i >= 0) {
		var filename = startFiles[i];
		if (exclude && filename in exclude)
			continue; // exclude this file
		var startFile = start+"/"+filename;
		var targetFile = target+"/"+filename;
		if (fast && checksumsStart[filename]) {
			if (checksumsStart[filename] === checksumsTarget[filename]) {// checksums match: do not copy
				delete targetFilesHash[filename];
				continue;
			}
			if (checksumsStart[filename].substr(0, 2) === "D ") { // fast copy subdirectory
				_copyRecInt(startFile, targetFile, null, true, _);
				delete targetFilesHash[filename];
				continue;
			}
		}
		changes = true;
		if (fs.stat(startFile, _).isDirectory()) { // start is directory: copy recursively
			_copyRecInt(startFile, targetFile, null, false, _);
		} else { // start file is no directory
			if (filename in targetFilesHash && fs.stat(targetFile, _).isDirectory())
				rmdirRec(targetFile, _);
			// copy file
			var buffer = fs.readFile(startFile, _);
			writeFile(targetFile, buffer, _);
			buffer = null;
		}
		// target file has been handled
		delete targetFilesHash[filename];
	}
	if (exclude) { // root directory: do not delete nodelocal.js
		delete targetFilesHash["nodelocal.js"];
		if (target.indexOf("..") >= 0) { // do not delete exclude directories when copying into parent directory
			for (var key in exclude) {
				delete targetFilesHash[key];
			}
		}
	}
	targetFiles = Object.keys(targetFilesHash);
	i = targetFiles.length;
	while (--i >= 0) {
		var targetFile = target+"/"+targetFiles[i];
		if (fs.stat(targetFile, _).isDirectory())
			rmdirRec(targetFile, _);
		else
			fs.unlink(targetFile, _);
		changes = true;
	}
	if (fast && changes) { // update checksums file only when there are changes
		writeFile(target+"/.checksums", JSON.stringify(checksumsStart), _);
	}
		
		
}


// asynchronous file existence test for streamline
function exists(path, callback) {
	fs.exists(path, function(res, error) { return callback(error, res)});
} 

exports.exists = exists;

// delete directory recursively
function rmdirRec(path, _) {
	var files = fs.readdir(path, _);
	var i = files.length;
	/* Loop through and delete everything in the sub-tree after checking it */
	while (--i >= 0) {
		var currFile = path + "/" + files[i];
		
		if(fs.stat(currFile, _).isDirectory()) // Recursive function back to the beginning
			rmdirRec(currFile, _);
		else {// Assume it's a file - perhaps a try/catch belongs here?
			try {
				fs.unlink(currFile, _);
			} catch (e) {
				if (e.code === 'EPERM') {
					fs.chmod(currFile, '666', _) // allow write access
					fs.unlink(currFile, _);
				}
			}
		}
	}
	return fs.rmdir(path, _);
};

exports.rmdirRec = rmdirRec;

/// getChecksumContent
/// reads and parses the contents of the .checksums file in the specified directory
/// returns empty object if file does not exist yet
function getChecksumContent(directory, path, _) {
	var buffer = null;
	try {
		buffer = fs.readFile(directory+"/"+path+"/.checksums", _);
		try {
			var result = JSON.parse(buffer.toString("utf8"));
			return result;
		} catch (e) {
			throw new Error(locale.format(module, "invalidChecksumFile", path, ""+e))
		}
	} catch (e) {
		if (e.code === 'ENOENT') {
			var result = {};
			return result; // file does not exist yet				
		} else
			throw new Error(locale.format(module, "invalidChecksumFile", path, ""+e));
	}

}

exports.getChecksumContent = getChecksumContent;

/// updateChecksums
/// After applying the patch, the contents of some directories have changed. 
/// The object 'list' contains the names of these directories as keys and the contents of their checksum files as values.
/// This function adds the missing intermediate directories and updates all checksum files
/// return value: checksum of top level directory
function updateChecksums(directory, list, _) {	
	var current = Object.keys(list);
	var parents = [];
	var i;
	// add intermediate directories. Strategy: loop over all directories and add parent directory (and its checksums contents) if it does not exist yet.
	// after the loop over directories has finished, start a new loop of the added parent directories	
	while (current.length > 0) {
		i = current.length;
		while (--i >= 0) {
			var parent = path.dirname(current[i]);
			if (!(parent in list)) { // parent directory not yet in the list
				parents.push(parent);
				list[parent] = getChecksumContent(directory, parent, _);
			}
		}		
		current = parents;
		parents = [];
	}
	
	// sort with ascending length so that no directory will be scanned before any of its subdirectories
	// in order to assure that root directory "." is really the last, a special comparison puts the char code of "." (46) to be the lowest value.
	var dirs = Object.keys(list).sort(function(a,b) { return (a.length-b.length || a.length === 1 && Math.abs(a.charCodeAt(0)-46)-Math.abs(b.charCodeAt(0)-46) || 0); });
	i = dirs.length;	
	while (--i >= 0) {
		// write checksum file of current directory
		var dir = dirs[i];
		var buffer = new Buffer(JSON.stringify(list[dir]), "utf8");
		var sha1 = get_sha1_binary(buffer);
		fs.writeFile(directory+"/"+dir+"/.checksums", buffer, _);		
		if (dir !== ".") {
			// update contents of checksums of parent directory. The ordering of dirs assures that the checksums of the parent directory will be written later
			list[path.dirname(dir)][path.basename(dir)] = "D "+sha1;
		} else {
			return sha1;
		}
	}
	return "";
}

// files which should be excluded from checksum generation in any directory (in contrast to EXCLUDES_CHECKSUM which is just for the top level directory
function excludeFile(file) {
	if (file.substr(-4) === ".bbb" || file === ".checksums" || file.substr(-3) === ".md") {
		return true;
	}
	return false;
}

exports.updateChecksums = updateChecksums;

/// creates checksum files. Assumes that all line endings are unix style endings (therefore no distinction between binary and text files)
/// parameters: directory: directory to search
///			 errors: if not null, checksum files will be assumed to exist and to be correct, and mismatches will be appended to array
/// return value: checksum of current directory (i. e. of .checksums file)
function makeChecksums(path, _) {
	return makeChecksumsInt(path, EXCLUDES_CHECKSUM, _);
}

// recursive function: extra excludes parameter
function makeChecksumsInt(path, exclude, _) {
		var files = fs.readdir(path, _);
		var contents = {};
		var i = files.length;
		while (--i >= 0) {
			var file = files[i];
			// do not include checksums file and backup files
			if (excludeFile(file) || (exclude && file in exclude)) {
				continue;				
			}
			var filePath = path + "/" + file;
			var currFile = fs.stat(filePath, _);
			if (currFile.isDirectory()) { // Recursive function back to the beginning
				var sha1 = makeChecksumsInt(filePath, null, _);
				contents[file] = "D "+ sha1;
			} else {
				var buffer = fs.readFile(filePath, _)
				var sha1 = get_sha1_binary(buffer);
				contents[file] = "F "+ sha1;
			}
		}
		var buf = new Buffer(JSON.stringify(contents), "utf8");
		fs.writeFile(path+"/.checksums", buf, _);
		return get_sha1_binary(buf);
}

exports.makeChecksums = makeChecksums;

/// check checksums and compares them with total checksum in version file
function checkChecksumsV(path, _) {
	var errors = [];
	var sha1 = checkChecksums(path, errors, EXCLUDES_CHECKSUM, _);
	try {
		var version = readVersionFile(path, _);
		if (sha1 != version.sha1)
			errors.push(locale.format(module, "checksumMismatch"));
	} catch (e) {
		errors.push(locale.format(module, "versionFileError", ""+e));
	}
	return errors;
}

exports.checkChecksumsV = checkChecksumsV;

/// checks checksum files. Assumes that all line endings are unix style endings (therefore no distinction between binary and text files)
/// parameters: directory: directory to search
///			 errors: if not null, checksum files will be assumed to exist and to be correct, and mismatches will be appended to array
/// return value: checksum of current directory (i. e. of .checksums file)
function checkChecksums(path, errors, exclude, _) {
	try {
		var files = fs.readdir(path, _);
		var contents = {};
		var result = ""; 
		try {
			buf = fs.readFile(path+"/.checksums", "utf8", _)
	  		contents = JSON.parse(buf);
			result = get_sha1_binary(buf);
		} catch (e) {
	  		errors.push(e.toString());
	  		return result;
	  	}
		for (var i = 0; i < files.length; i++) {
			var file = files[i];
			var filePath = path + "/"+file;
			// do not include checksums file and backup files
			if (excludeFile(file) || (exclude && file in exclude))
				continue;
			if (!(file in contents)) {
				errors.push(locale.format(module, "newFile", filePath));
				continue;
			}	  	
			var currFile = fs.stat(filePath, _);			
			if (currFile.isDirectory()) { // Recursive function back to the beginning
				if (contents[file].substr(0, 2) != "D ")
					errors.push(locale.format(module, "noDirectoryExpected", filePath));
				else {
					var sha1 = checkChecksums(path + "/" + file, errors, null, _);
					if (contents[file] !== "D "+sha1) {
						errors.push(locale.format(module, "wrongChecksum", filePath))
					}					
				}
			} else {
				var buffer = fs.readFile(filePath, _)
				var sha1 = get_sha1_binary(buffer);
				if (contents[file] !== "F "+sha1) {
					if (contents[file].substr(0, 2) != "F ")
						errors.push(locale.format(module, "directoryExpected", filePath));
					else
						errors.push(locale.format(module, "wrongChecksum", filePath));
				}
			}
			delete contents[file];
		}
		// errors for remaining files
		for (var file in contents) {
			errors.push(locale.format(module, "deletedFile", path+"/"+file))
		}
	} catch (err) { errors.push(locale.format(module, "otherError", ""+err)); }
	return result;
}

exports.checkChecksums = checkChecksums;


/// get sha1 sum of binary content
function get_sha1_binary(buffer) {
	var shasum = crypto.createHash('sha1');
	// GIT specific header
	shasum.update("blob "+buffer.length+"\0", "utf8");
	shasum.update(buffer);
	var digest = shasum.digest('hex');
	return digest;	
}

exports.get_sha1_binary = get_sha1_binary;

/// tests SHA1 checksums for equality or whether one is an abbreviation of the other
function equal_sha1(sum, sumGit) {
	if (sum === undefined || sumGit === undefined)
		return false;
	if (sum.substr(0, 2) === "F ")
		sum = sum.substr(2); // strip directory/file information in checksums file
	return sum === sumGit || sum.substr(0, sumGit.length) == sumGit; 
}

exports.equal_sha1 = equal_sha1;

/// check whether checksum is correct for binary file
/// input
function check_sha1_binary(buffer, sha1) {
	return (get_sha1_binary(buffer).substr(0, sha1.length) === sha1);
}
exports.check_sha1_binary = check_sha1_binary;

// tests whether the corresponding file exists. 'path' should be the relative path within 'dir'. 
function _nodetest(dir, path) {
	if (fs.existsSync(dir+"/"+path))
		return dir+"/"+path;
	return null;
}

// start a new detached node process 
function exchangeProcess(dir, logfile, parameter) {
	if (config.mockServer) {
		console.log("Do not spawn process within cluster")
		return; // do not spawn process within cluster
	}
	config.mockServer = exports.server;
    out = fs.openSync(logfile, 'a'),
    err = fs.openSync(logfile, 'a');
    var osArch = process.platform+"_"+process.arch
    var file = _nodetest(dir, "nodejs/"+osArch+"/node.exe") || _nodetest(dir, "nodejs/"+osArch+"/node") || 
               _nodetest(dir, "nodejs/"+process.platform+"/node.exe") || _nodetest(dir, "nodejs/"+process.platform+"/node") || 
               _nodetest(dir, "nodejs/node.exe");
    if (!file)
    	throw new Error(locale.format(module, "nodeNotFound"))
	var child = child_process.spawn(file, ["index.js", parameter ], { detached: true, stdio: [ 'ignore', out, err ], cwd: dir });
	child.unref(); 
}

exports.exchangeProcess = exchangeProcess;

function readVersionFile(path, _) {
	return JSON.parse(fs.readFile(path+"/"+exports.VERSION_FILE, "utf8", _));
}

exports.readVersionFile = readVersionFile;