import { _ } from 'streamline-runtime';
import * as	path from 'path';
import { Dict, Exporter, Tracker, Descriptor, FileStruct, ResourceManager } from './exporter'

export interface Options {
	specialExport?: boolean;
	modulesFilter?: string;
	incremental?: boolean;
	incrementalDate?: Date;
	skipTexts?: boolean;
	exportToMongo?: boolean;
	skipGit?: boolean;
}

interface ColumnMeta {
	columnName: string;
	dimension: number;
}

interface TableMeta {
	tableName: string;
	columns: Dict<ColumnMeta>;
}

interface QueryOptions {
	entityDescriptor?: Descriptor;
	formatValues?: boolean;
	specialExport?: boolean;
}

type DataPart = any;


var locale = require('streamline-locale'),
	jsonHelper = require('scm-lib/jsonHelper');


//var defaultExcludedColumns = ["AUUID", "UPDDAT", "UPDUSR", "UPDDATTIM", "CREDAT", "CREUSR", "CREDATTIM", "UPDTICK"];
var defaultExcludedColumns = ["AUUID"];



/// !doc 
/// ### run(_, exporter, entity, tracker, entityNames)
/// Exports the metadata of a set of entities into JSON files
/// This function returns a list of {absoluteFilename: xxx, status: xxx} from the name of the generated files. 
///
/// * exporter : the exporter that will be used to serialize the metadata into JSON files
/// * entity : the entity descriptor
/// * tracker : the tracker
/// * entityNames : the name of the entities
exports.run = function (_: _, resourceManager : ResourceManager, exporter: Exporter, entity: Descriptor, tracker: Tracker, entityNames: string[], options?: Options) {

	const COLNAME_UPDDATTIM = "UPDDATTIM";
	const COLNAME_MODULE = "MODULE";

	function _trace(message: string, severity?: string) {
		trace && trace(message);
		if (severity) {
			if (tracker) {
				tracker.$diagnoses = tracker.$diagnoses || [];
				tracker.$diagnoses.push({
					$severity: severity,
					$message: message,
				});
			}
		}
	}

	// Query the data for ONE item (i.e. one table, one class, ...)
	// pkValue : PK of the item to query
	// dataFromDummyObject : set when querying the children of a dummyObject
	function queryData(_: _, tableMeta: TableMeta, entityDescriptor: Descriptor, pkValue: any, dataFromDummyObject?: any) {

		// dummyObjectRows is only set when the parent of the children we are querying is a dummyObject. 
		// it contains all the rows that were read with the partial PK (the PK set on the dummyObject)
		function _queryChildren(_: _, childDescriptor: Descriptor, parentRow: any, blockIndex: number, dummyObjectRows?: boolean) {
			var childKeys = {} as any;
			var parentMappings = childDescriptor.parentMappings;
			if (!parentMappings || dummyObjectRows) {
				var propertiesToMap: string[] | string;
				if (dummyObjectRows) {
					// When dealing with children of a dummyObject, we ignore the parentMappings
					// set in the configuration file and we compute them from the distinct keys
					propertiesToMap = childDescriptor.distinctKey;
				} else
					propertiesToMap = entityDescriptor.primaryKey;
				if (propertiesToMap) {
					if (!Array.isArray(propertiesToMap))
						propertiesToMap = propertiesToMap.split(',');
					// Build the parent mappings from the PK of the parent
					// [x,y] => {x:x, y:y}
					parentMappings = propertiesToMap.reduce(function (global, item) {
						global[item] = item;
						return global;
					}, {} as any);
				} else {
					// Build the parent mappings from the parent mappings of the parent
					parentMappings = entityDescriptor.parentMappings;
				}
				childDescriptor.parentMappings = parentMappings;
			}
			var decodedParentRow: any;
			Object.keys(parentMappings).forEach(function (childPropertyName) {
				var parentProperty = parentMappings[childPropertyName];
				if ("function" === typeof (parentProperty)) {
					// Evaluate the function to get the value to map
					if (!decodedParentRow)
						decodedParentRow = jsonHelper.decodeObject(parentRow);
					childKeys[childPropertyName] = parentProperty(decodedParentRow);
				} else if ("object" === typeof (parentProperty)) {
					childKeys[childPropertyName] = parentProperty.value;
				} else if (parentProperty === "$INDEX") {
					childKeys[childPropertyName] = blockIndex + 1;
				} else if (parentProperty.indexOf('"') === 0 || parentProperty.indexOf("'") === 0) {
					// to manage constant in the keys
					childKeys[childPropertyName] = parentProperty;
				} else {
					childKeys[childPropertyName] = jsonHelper.decodeValueFromJsonString(null, parentRow[parentProperty]).value;
				}
				if (childKeys[childPropertyName] === "") {
					childKeys[childPropertyName] = " ";
				}
			});

			const childResults = queryData(_,
				getTableMeta(_, childDescriptor.tableName, childDescriptor.excludedColumns || defaultExcludedColumns),
				childDescriptor,
				childKeys,
				dummyObjectRows);
			return childResults;
		}

		function _queryTable(_: _, tableMeta: TableMeta, keys: any, entityDescriptor: Descriptor, parentDescriptor: Descriptor, queryOptions: QueryOptions) {
			queryOptions = queryOptions || {} as QueryOptions;
			entityDescriptor = entityDescriptor || {} as Descriptor;

			// Build the list of columns to query
			var colsToQuery = [] as string[];
			Object.keys(tableMeta.columns).forEach(function (colName) {
				var colDef = tableMeta.columns[colName];
				for (var i = 0; i < colDef.dimension; i++) {
					colsToQuery.push(colDef.columnName + "_" + i);
				};
			});

			var where = parentDescriptor ? parentDescriptor.primaryKey : "";

			queryOptions.entityDescriptor = entityDescriptor;
			var sqlResults = buildExecute(_, tableMeta, colsToQuery, tableMeta.tableName, entityDescriptor.orderBy, keys, where, queryOptions);
			return sqlResults;
		}

		// Some basic consistency checks
		if (entityDescriptor.itemIndex) {
			if (entityDescriptor.orderBy !== entityDescriptor.itemIndex) {
				throw new Error(locale.format(module, "orderByMismatchesItemIndex", entityDescriptor.tableName, entity.title));
			}
			if (!entityDescriptor.distinctKey) {
				throw new Error(locale.format(module, "distinctKeyIsMissing", entityDescriptor.tableName, entity.title));
			}
			if (entityDescriptor.distinctKey === entityDescriptor.itemIndex) {
				throw new Error(locale.format(module, "distinctKeyMatchesItemIndex", entityDescriptor.tableName, entity.title));
			}
		}
		// Read the data for the entity itself
		var data: DataPart[];
		if (dataFromDummyObject) {
			// We are about to read the data for a child of a dummyObject.
			// When the data of the dummyObject itself have been read, a partial PK (the one defined on the 
			// dummyObject) was used, so the row matching the current child should be in the rows read by
			// the dummyObject (the PK of children is an extend of the PK of the dummyObject)
			// We don't need to send a SQL request, we just need to retrieve the rows we are interested in
			// from all the rows read by the dummyObject.
			data = dataFromDummyObject.filter(function (row: any) {
				// retain all the items that match the PK values.
				return Object.keys(pkValue).every(function (colName) {
					var decodedValue = jsonHelper.decodeValueFromJsonString(null, row[colName]).value;
					if (decodedValue === "")
						decodedValue = " ";
					return pkValue[colName] === decodedValue;
				});
			});
		}
		if (!data)
			data = _queryTable(_, tableMeta, pkValue, entityDescriptor, null, {
				formatValues: true,
				specialExport: options.specialExport,
			});

		if (Object.keys(data).length === 0) {
			return data;
		}


		if (entityDescriptor.children) {
			var dummyObject = {} as any;
			// Now, query the data for the children of all the items
			Object.keys(entityDescriptor.children).forEach_(_, function (_, childName) {
				var childEntity = entityDescriptor.children[childName];
				var dummyObjectRows: any;
				if (entityDescriptor.dummyObject) {
					// Use a copy of data for the children as data will be modified during the next loop
					dummyObjectRows = JSON.parse(JSON.stringify(data));
					dummyObject[childName] = [];
				}
				data.forEach_(_, function (_, itemData) {
					var childData = _queryChildren(_, childEntity, itemData, undefined, dummyObjectRows);
					if (childEntity.parentCount) {
						// The child description defines a property that contains the number of children.
						// This property is a property of the parent : it needs to be removed and it will
						// be automatically recomputed when the JSON file will be read
						delete itemData[childEntity.parentCount];
					}
					if (childData.length) {
						if (entityDescriptor.dummyObject) {
							childData.forEach(function (item) {
								dummyObject[childName].push(item);
							})
						} else
							itemData[childName] = childData;
					}
				});
			});
			if (entityDescriptor.dummyObject) {
				// A dummyObject only contains children and no other properties
				data = [dummyObject];
			}
		}

		if (entityDescriptor.dummyObject) {
			//The data are already OK and dummyObjects can't have groups
		} else {
			// Create the groups
			data = data.map_(_, function (_, dataPart) {
				// Note : we call jsonHelper.createGroups even when no groups are defined for the entity : the createGroups
				// function also performs some tests
				dataPart = jsonHelper.createGroups(
					entityDescriptor,
					jsonHelper.sortJsonObject(_, resourceManager, dataPart, entityDescriptor),
					getTableMeta(_, entityDescriptor.tableName)
				);
				if (entityDescriptor.itemIndex) {
					// The property bound to 'itemIndex' is the index of the item in the collection
					// It must be removed from the JSON file and will be recomputed when reading the JSON file
					// This way, if some conflicts have to be resolved on this file and some other items are
					// inserted, they will be correctly re-indexed
					delete dataPart[entityDescriptor.itemIndex];
				}

				// Parse all the groups
				if (entityDescriptor.groups) {
					entityDescriptor.groups.filter(function (group) {
						// We are only interested in group with children
						return (group.children && dataPart[group.name]);
					}).forEach_(_, function (_, group) {
						// We have to parse each block (item of a group) that has been created by the createGroups function.
						dataPart[group.name].items.forEach_(_, function (_: _, blockData: any, blockIndex: number) {
							// Each child must be queried using the current context as a parent context.
							// The parent content will be composed of properties from the parent itself but also of properties
							// from a block.
							// For instance, when querying then 'AMSKZON' child, we will need the 'CODMSK' of the parent (screen)
							// to bind the zones to the right screen.
							Object.keys(group.children).forEach_(_, function (_, childName) {
								var child = group.children[childName];
								blockData[childName] = _queryChildren(_, child, dataPart, blockIndex);
							});
						});
					});
				}
				return dataPart;
			});
		}
		return data;
	}

	function getTableMeta(_: _, tableName: string, excludedColumns?: string[]) {
		var tableMeta = tableMetas[tableName];
		if (!tableMeta) {
			tableMeta = exporter.getTableMeta(_, tableName, excludedColumns);
			tableMetas[tableName] = tableMeta;
		}
		return tableMeta;
	}

	// Formats a bunch of values into a srtring
	// _formatPkValues(["AA", "BB"], {AA:1, BB:"XX", CC:"YY"}) => 1~XX
	function _formatPkValues(colNames: string[], data: any) {
		var arr = colNames.reduce(function (fullArray, colName) {
			var val = data[colName] || "";
			if (typeof (val) === "string") {
				val = val.trim();
			}
			fullArray.push(val);
			return fullArray;
		}, []);
		return arr.join("~").toUpperCase();
	}

	options = options || {};
	var tableMetas = {} as Dict<TableMeta>;

	var t0 = Date.now();
	var buildExecute = exporter.buildExecute,
		trace = exporter.trace;

	var excludedColumns = entity.excludedColumns || defaultExcludedColumns;

	var tableMeta = getTableMeta(_, entity.tableName, excludedColumns);

	var colsToQuery = [].concat(entity.primaryKey);
	if (options.incrementalDate)
		colsToQuery.push(COLNAME_UPDDATTIM);
	if (options.specialExport)
		colsToQuery.push(COLNAME_MODULE);

	if (entity.dummyObject) {
		// Rules for dumy objects : 
		// Rule #1 : the dummyObject must provide an OrderBy
		if (!entity.orderBy)
			throw new Error(locale.format(module, "dummyObjectWithoutOrderBy", entity.title));
		var dummyObjectPK = entity.primaryKey;
		if (!Array.isArray(dummyObjectPK))
			dummyObjectPK = dummyObjectPK.split(',');
		Object.keys(entity.children).forEach(function (childName) {
			const child = entity.children[childName];
			if (child.orderBy) {
				// Rule #2 : The children of a dummy object must not define an orderBy
				throw new Error(locale.format(module, "dummyObjectChildWithOrderBy", entity.title, childName));
			}

			// Rule #3 : The children of a dummyObject can only extend the PK the dummyObject
			var childPK = child.distinctKey;
			if (!childPK)
				throw new Error(locale.format(module, "dummyObjectChildWithoutDistinctKey", entity.title, childName));
			if (!Array.isArray(childPK))
				childPK = childPK.split(',');
			// REVIEW: typing was seriously wrong on next line
			if (childPK.length <= dummyObjectPK.length)
				throw new Error(locale.format(module, "dummyObjectChildDoesNotExtendPK", entity.title, childName));
			(dummyObjectPK as string[]).forEach(function (pkPart, index) {
				if (pkPart !== childPK[index])
					throw new Error(locale.format(module, "dummyObjectChildDoesNotExtendPK", entity.title, childName));
			});
		});
	}
	// First let's list entities stored in the repo in order to detect ones that aren't present in the X3 database anymore:
	if (entityNames.length === 0) var deletedEntities = exporter.listMetadataFiles(_, entity);

	// Second, we have to filter the entities, according to the provided filter
	// Note : here, we don't provide any pk, we want all the entities : they will be filtered later
	// We can't run a 'SELECT .. .FROM ... WHERE ...' query because we can have more than one entityName
	// This would lead to complex (and possibly too long) WHERE clauses : (... AND ... AND ...) OR (... AND ... AND ...) ... 
	var keysToUseForFilter = entity.primaryKey;
	if (!Array.isArray(keysToUseForFilter))
		keysToUseForFilter = keysToUseForFilter.split(',');
	var alreadyFilteredKeys: string[] = [];
	var pkValues = buildExecute(_, null, colsToQuery, entity.tableName, entity.primaryKey, null, null,
		options.incrementalDate ? null : {
			groupBy: entity.primaryKey
		}
	).filter(function (s) {
		if (entityNames.length > 0) {
			// note : entityNames contains names like X~Y~Z where X, Y, Z are the values of the primary key
			// We have to build a string with concatenated values to check if it belongs to entityName
			var pkValuesStr = (keysToUseForFilter as string[]).map(function (colName, index) {
				const pkVal = s[colName];
				if (pkVal === ' ')
					return '';
				else
					return pkVal;
			}).join('~');
			if (alreadyFilteredKeys.indexOf(pkValuesStr) !== -1) {
				// For instance, on documentationlinks, there is a dummy parent object with a partial primary key
				// all the rows that concern the children of this dummy parent will be accepted with the partial key
				// We only need to validate the first rown the other one will be processed as a child of this 
				// dummy parent
				return false;
			}
			alreadyFilteredKeys.push(pkValuesStr);
			if (entityNames.indexOf(pkValuesStr) === -1)
				return false;
		}
		if (deletedEntities) {
			delete deletedEntities[_formatPkValues(entity.primaryKey as string[], s)];
		}

		if (options.specialExport) {
			return options.modulesFilter.indexOf(s.MODULE) != -1;
		}

		if (options.incrementalDate) {
			if (!s[COLNAME_UPDDATTIM]) {
				// a null date is considered as a 'very long ago' date (sth like 1599/11/31 for oracle)
				// Items with a null date will only be synchronized on the first export.
				// options.incrementalDate is null for the first export (full or incremental)
				return false;
			}
			if (new Date(s[COLNAME_UPDDATTIM]) <= options.incrementalDate) {
				// This object has already been synchronized.
				return false;
			}
		}

		// if (entity.title === "screen") {
		// 	if (s["CODMSK"] < "PLPW2")
		// 		return false;
		// }

		if (entity.excludedInitials) {
			return !~entity.excludedInitials.indexOf(s[entity.primaryKey[0]][0]);
		}
		return true;
	}).map(function (row) {
		// Remove the UPDDATTIM column. It was only included to compare dates (incremental sync)
		if (options.incrementalDate) {
			delete row[COLNAME_UPDDATTIM];
		}
		if (options.specialExport) {
			delete row[COLNAME_MODULE];
		}
		return row;
	});


	// Here, pkValues contains all the keys of the entityType that should be exported
	// For instance, when exporting tables, pkValues will look like : 
	// [{"CODFIC" : "XXX1"}, {"CODFIC" : "XXX2"}, {"CODFIC" : "XXX3"}, .... ] where XXX1, XXX2, XXX3, are the PK in 
	// the ATABLE table of the tables to export.

	var fileStructs: FileStruct[] = [];

	_trace(locale.format(module, "exportingItems", entity.title, pkValues.length), "info");

	var count = {
		entities: 0,
		generatedFiles: 0,
		unchangedFiles: 0,
		filteredFiles: 0,
	};

	pkValues.forEach_(_, function (_, pkValue, idx) {
		const modulePart = exporter.product.name;
		var filename = _formatPkValues(entity.primaryKey as string[], pkValue);
		if (filename === "")
			throw new Error("Could not export entity " + entity.title + ", the primary key (" + entity.primaryKey + ") is empty");
		

	    if (!options.resume || !exporter.resourceExists(_, {path: path.join(modulePart, entity.subdir), fileName: filename})) {
			count.entities++;
			var mainObjectData = queryData(_, tableMeta, entity, pkValue)[0];

			if (entity.filter) {
				// Check that the current object matches the provided filter		
				if (!Object.keys(entity.filter).every(function (filteredProperty) {
					return mainObjectData[filteredProperty] === entity.filter[filteredProperty];
				})) {
					// This entity does not match the provided filter, skip it
					count.filteredFiles++;
					_trace(locale.format(module, "exportingItemsDetail", entity.title, (idx + 1), pkValues.length, 'does not match filter : ' + JSON.stringify(entity.filter)));
					return;
				}
			}

			_trace(locale.format(module, "exportingItemsDetail", entity.title, (idx + 1), pkValues.length, JSON.stringify(pkValue)));

			// Now, we can write the files
			const modulePart = exporter.product.name;

			// We have to re-sort the JSON object to avoid git differences
			// When a conflict will be detected on this JSON object, the conflict will be resolved and the JSON
			// will be sorted with jsonHelper.sortJsonObject() function. So, the original JSON file (the one we are building now)
			// must be sorted with the same function to limit git differences.
			mainObjectData = jsonHelper.sortJsonObject(_, resourceManager, mainObjectData, entity);

			var filename = _formatPkValues(entity.primaryKey as string[], pkValue);
			if (filename === "")
				throw new Error("Could not export entity " + entity.title + ", the primary key (" + entity.primaryKey + ") is empty");

			// Now, we can write the file
			var absoluteFilename = exporter.writeResource(_, {
				path: path.join(modulePart, entity.subdir),
				fileName: filename,
				data: mainObjectData
			});
			if (absoluteFilename === undefined) {
				// Unchanged file
				count.unchangedFiles++;
			} else {
				fileStructs.push({
					absoluteFilename: absoluteFilename,
					status: 'added'
				});
				count.generatedFiles++;
			}
		}
	});

	if (count.filteredFiles)
		_trace(locale.format(module, "exportDoneWithFilteredFiles", entity.title, count.entities, count.generatedFiles, count.filteredFiles, count.unchangedFiles, Math.round((Date.now() - t0) / 1000)), "info");
	else
		_trace(locale.format(module, "exportDone", entity.title, count.entities, count.generatedFiles, count.unchangedFiles, Math.round((Date.now() - t0) / 1000)), "info");

	// Eventually delete obsoletes entities
	if (deletedEntities) {
		if (Object.keys(deletedEntities).length) {
			Object.keys(deletedEntities).forEach(function (deletedKey) {
				fileStructs.push({
					absoluteFilename: deletedEntities[deletedKey],
					status: 'deleted'
				});
			});
		}
	}
	return fileStructs;
};