import { _ } from 'streamline-runtime';
import * as fs from 'fs';
import * as path from 'path';
import * as mongodb from 'mongodb';
import * as ez from 'ez-streams';
import * as ezf from 'f-streams';
import { Options } from './export-entity';
import { Dict, Tracker, Descriptor, FileStruct, Exporter } from './exporter';

/// !doc
/// # Sync Metadata
/// Tools to synchronize metadata from Sql (SqlServer, oracle, ...) to mongodb
/// Usage : 
/// ```
/// var sync = require('./sync').newSync(_, etnaConfig, tracker);
/// sync.exportMetadataAndPush(_);
/// ```

var locale = require('streamline-locale'),
	globals = require('streamline/lib/globals'),
	nodeLocalConfig = require('config'),
	jsonHelper = require('scm-lib/jsonHelper'),
	resourceManager = require('scm-lib/resourceManager');

interface Filters {
	solution?: string;
	folder?: string;
	entityType?: string;
	entityName?: string;
}

interface Text {
	NUMERO: number;
	TEXTE: string;
	COMMENT: string;
	LAN: string;
	LANORI: string;
	product: string;

}
export const $exported = true;

function getEndpoint(_: _, filters: Filters) {
	var endpoint: any;
	var AdminHelper = require("@sage/syracuse-lib/src/collaboration/helpers").AdminHelper;
	if (filters.solution && filters.folder) {
		// Search an endpoint that matches the solution and the folder
		var db = AdminHelper.getCollaborationOrm(_);
		var solution = db.fetchInstance(_, db.model.getEntity(_, "x3solution"), {
			jsonWhere: {
				code: filters.solution
			}
		});
		if (!solution) throw new Error("no x3solution found for:" + filters.solution);

		endpoint = db.fetchInstance(_, db.model.getEntity(_, "endPoint"), {
			jsonWhere: {
				x3solution: solution.$uuid,
				x3ServerFolder: filters.folder,
				gitFolder: {
					$exists: true,
					$ne: ""
				}
			}
		});
		if (!endpoint) throw new Error("no endpoint found for solution=" + filters.solution + " and folder=" + filters.folder);
	} else {
		// Let's use the current endpoint
		endpoint = AdminHelper.getEndpoint(_, {
			dataset: globals.context.endpointDataset,
		});
	}
	// console.log("endpoint:",endpoint);	
	return endpoint;
}


/// !doc
/// --------------------------
/// ### extractMetadataFromX3(_, entityType, entityName)
/// Intended to be invoked from X3, see `exportMetadataAndPush` for more details
export function extractMetadataFromX3(_: _, filters: Filters, options: Options) {
	try {
		filters = filters || {};
		var endpoint = getEndpoint(_, filters);
		var sync = newSync(_, endpoint);
		sync.exportMetadataAndPush(_, filters.entityType, filters.entityName, options);
	} catch (err) {
		console.error('ERROR extractMetadataFromX3 : ', err);
	}
};

/// !doc
/// --------------------------
/// ### removeMetadataFromX3(_, entityType, entityName)
/// Intended to be invoked from X3, see `removeMetadata` for more details
export function removeMetadataFromX3(_: _, filters: Filters, options: Options) {
	try {
		filters = filters || {};
		var endpoint = getEndpoint(_, filters);
		var sync = exports.newSync(_, endpoint);
		sync.removeMetadata(_, filters.entityType, filters.entityName, options);
	} catch (err) {
		console.error('ERROR removeMetadataFromX3 : ', err);
	}
};

export function newSync(_: _, endpoint: any, tracker?: Tracker) {

	nodeLocalConfig.etl = nodeLocalConfig.etl || {};
	var etnaConfig: any;
	if ((typeof (endpoint) === "object") && endpoint.userConfig) {
		// Init from an object (should only be used for testing purpose)
		etnaConfig = endpoint.etnaConfig;
		etnaConfig.outputFolder = endpoint.outputFolder;
	} else {
		// Init from an endpoint
		if (!endpoint.gitFolder(_))
			throw new Error("git-repo folder is not set for the endpoint " + endpoint.description(_));

		etnaConfig = endpoint.getEtnaConfig(_); // globals.context.config.etna;
		etnaConfig.outputFolder = path.join(endpoint.gitFolder(_), "META");
	}

	resourceManager.setConfiguration({
		rootFolder: etnaConfig.scmDefinitionFilesFolder,
	});

	// console.log("SQL CONFIG = " + require("util").inspect(etnaConfig.sql, false, null));
	// console.log("MONGO CONFIG = " + require("util").inspect(etnaConfig.mongo, false, null));
	// console.log("OUTPUT FOLDER = " + etnaConfig.outputFolder);

	if (!etnaConfig.outputFolder)
		throw new Error("No output folder was set");

	etnaConfig.trace = etnaConfig.trace || console.log;

	var trace = etnaConfig.trace;
	var entityDescriptors: Descriptor[] = [];

	/// !doc
	/// --------------------------
	/// ### exportMetadataToJSONs(_, entityType, entityNames)
	/// Export metadata to JSON files and returns the ABSOLUTE filename of the generated JSON files
	function exportMetadataToJSONs(_: _, entityType: string, entityNames: string[], options: Options): FileStruct[] {
		options = options || {};
		var fileStructs = [] as FileStruct[];
		var exporter = require("etna-etl/lib/exporter").newExporter({
			etnaConfig: etnaConfig,
			trace: etnaConfig.trace,
			outputFolder: etnaConfig.outputFolder,
			parallel: 1,
		});

		var mongoConfig = etnaConfig.mongo || {};
		var dbUrl = "mongodb://" + (mongoConfig.host || "localhost") + ":" + (mongoConfig.port || 27017) + "/" + mongoConfig.database;
		var mongoDb = mongodb.MongoClient.connect(dbUrl, {
			db: {
				w: 1
			}
		}).then(_, _);

		var mongoColn = mongoDb.collection("user_settings");
		var mongofilter = {
			type: "last_export_meta"
		};

		if (options.incremental) {
			// We have to read the date of the last sync (stored in mongoDb)
			var last = mongoColn.find(mongofilter, _).toArray().then(_, _)[0];
			if (last) {
				options.incrementalDate = new Date(last.date);
			}
		}

		if (options.incrementalDate) {
			_trace(locale.format(module, "incrementalExport", options.incrementalDate), "info");
		} else {
			_trace(locale.format(module, "fullExport"), "info");
		}

		var currentDate = new Date();

		if (!exporter.init(_, tracker)) {
			return [];
		}
		var entityExporter = require('./export-entity');
		if (entityType) {
			entityDescriptors = [resourceManager.getEntityDescriptor(_, entityType)];
		} else {
			entityDescriptors = resourceManager.getAllEntityDescriptors(_);
		}

		if (tracker) {
			tracker.phase = locale.format(module, "exportToJSONs");
			tracker.phaseDetail = "";
		}

		var messages: string[] = [];
		entityDescriptors.forEach_(_, function (_, entityDescriptor) {
			try {
				var generatedFilenames = entityExporter.run(_, resourceManager, exporter, entityDescriptor, tracker, entityNames, options);
				fileStructs = fileStructs.concat(generatedFilenames);
			} catch (e) {
				_trace(e.message, "warning");
				messages.push(e.message);
			}
		});
		messages.forEach(m => console.error(m));

		var syncData = {
			type: mongofilter.type,
			date: currentDate.toISOString()
		};
		mongoColn.update(mongofilter, syncData, {
			upsert: true
		}).then(_, _);
		exporter.release(_);

		return fileStructs;
	}

	/// !doc 
	/// ### importMetadataToMongo(_, fileStructs, entityType)
	/// Import some JSON files into mongoDb
	/// * fileStructs : list of {absoluteFilename:xxxx, status:xxx} of the JSON files to import
	/// * entityType : the type of entities 
	/// * fullSync ? drop the mongo collections before importing
	function importMetadataToMongo(_: _, fileStructs: FileStruct[], entityType: string, fullSync: boolean, options?: Options) {

		options = options || {};

		if (fileStructs) {
			// A list of absolute filenames is provided ...
			if (!entityType) {
				// ... but the entity type is not provided, we have to parse all the filenames, parse them and group 
				// them by entityType.
				var fileStructsByType = {} as Dict<FileStruct[]>;
				fileStructs.forEach(function (fileStruct) {
					var parseResult = jsonHelper.parseMetaFilename(fileStruct.absoluteFilename);
					fileStructsByType[parseResult.type] = fileStructsByType[parseResult.type] || [];
					fileStructsByType[parseResult.type].push(fileStruct);
				});
				Object.keys(fileStructsByType).forEach_(_, function (_, entityType) {
					importMetadataToMongo(_, fileStructsByType[entityType], entityType, fullSync, {
						skipTexts: true
					});
				});
				if (!options.skipTexts) {
					_syncText(_, "aplstd", options);
					_syncText(_, "atexte", options);
				}
				return;
			}
		}

		if (tracker) {
			tracker.phase = locale.format(module, "importType", entityType);
			tracker.phaseDetail = locale.format(module, "importType", entityType);
		}

		var t0 = Date.now();
		var importer = require('./importer').newImporter(_, resourceManager, {
			trace: etnaConfig.trace,
			mongo: etnaConfig.mongo,
			fileStructs: fileStructs,
			metaFolder: etnaConfig.outputFolder,
		});

		if (entityType) {
			entityDescriptors = [resourceManager.getEntityDescriptor(_, entityType)];
		} else {
			entityDescriptors = resourceManager.getAllEntityDescriptors(_);
		}
		var importedFilesCount = 0;
		entityDescriptors.forEach_(_, function (_, entityDescriptor) {
			importedFilesCount += importer.updateCollection(_, entityDescriptor, tracker, fullSync);
		});

		_trace(locale.format(module, "importDone", importedFilesCount, Math.round((Date.now() - t0) / 1000)), "info");
		if (!options.skipTexts) {
			_syncText(_, "aplstd", options);
			_syncText(_, "atexte", options);
		}
	}

	function _trace(message: string, severity?: string) {
		trace && trace(message);
		if (severity) {
			if (tracker) {
				tracker.$diagnoses = tracker.$diagnoses || [];
				tracker.$diagnoses.push({
					$severity: severity,
					$message: message,
				});
			}
		}
	}

	function _syncText(_: _, textType: string, options?: Options) {
		var incSync = require("./incrementalSync").newIncrementalSync(etnaConfig, tracker);
		return incSync.syncText(_, textType, options);
	}

	function _initTexts(_: _, baseUrl: string, product: string) {
		return _initTextsServer(_, baseUrl, product, "ATEXTE", "SYNC",
			(driver:Driver, args:any) => "select NUMERO_0, LAN_0, LANORI_0, TEXTE_0, COMMENT_0 from ATEXTE  where LAN_0= " + driver.param(0) + " order by LANCHP_0, LANNUM_0",
			(context:any,record:any) => record);
	}

	function _initMenus(_: _, baseUrl: string, product: string, langue?:string) {
		return _initTextsServer(_, baseUrl, product, "APLSTD", "SYNC_APLSTD",
			(driver:Driver, args:any) => {
				var sql  = "select a.LANCHP_0, a.LANNUM_0, a.LAN_0, a.LANMES_0, a.LANORI_0 , b.MENLOCAL_0 from APLSTD a"
					+ " inner join AMENLOC b on a.LANCHP_0 = b.MENLOC_0 and a.LAN_0= " + driver.param(0);
				if( !product ||product === "SU") {
					sql += " and b.FLGPRO_0 "+(product ? '=':'<>')+" 2";
				}
				return sql + " order by a.LANCHP_0, a.LANNUM_0";
			},
			(context:any,record:any) => {
				//console.log("record");
				context.chapters = context.chapters || {}; 
				// if(!context.chapters[record.LANCHP]) console.log(product+':'+record.LANCHP);
				if (context.chapters[record.LANCHP] === undefined)// CCC : add if undefined otherwise it's overwritten by next records but maybe LANMES has been set with LANNUM=0
					context.chapters[record.LANCHP] = { 	
								product: product,
								LANCHP: record.LANCHP,
								LANNUM: 0,
								LAN: record.LAN,
								LANMES: '',
								LANORI: record.LAN,// Default value!
								CODE: '' };
				record.CODE = '';
				if(record.MENLOCAL === 2) {
					if(record.LANNUM === 0) {
						context.codes = record.LANMES.split('').reduce( (r,c,i) => {
								r[i+1] = c;
								return r;
							},{});
					} else {
						record.CODE = context.codes[''+record.LANNUM] || '';
					}
				}
				else if(record.LANNUM === 0) {
					context.codes = {};
					context.chapters[record.LANCHP].LANORI = record.LANORI;  //CCC
					context.chapters[record.LANCHP].LANMES = record.LANMES;  //CCC
				}
				return record.LANNUM ? record : undefined;
			}, 
			(driver:Driver,context:any,lan:string) => {
				context.chapters = [0,200].reduce((r,menu) => {
					var sql  = "select LANNUM_0, LANMES_0, LANORI_0 from APLSTD where LANCHP_0 = " + driver.param(0) + " and LAN_0= " + driver.param(1);
					var args = [menu,lan];
					driver.reader(sql, args).toArray().forEach( (record:any) =>  {
						// CCC : add the value of 'menu' as it has been substracted in X3 database
						if(r[record.LANNUM_0+menu] !== undefined) {
							r[record.LANNUM_0+menu].LANMES = record.LANMES_0; 
							r[record.LANNUM_0+menu].LANORI = record.LANORI_0; 
						}
					});
					return r;
				},context.chapters);
				return Object.keys(context.chapters).map( chapter => context.chapters[chapter]);
			},
			langue
			);
	}

	function _initTextsServer(_: _, baseUrl: string, product: string, type: string, post: string, getSql: (driver:Driver, args:any) => string, transform:(context:any,record:any) => void, afterTransform?:(driver:Driver,context:any,lan:string) => any[], langue?:string) {
		console.log("initTextsServer:" + baseUrl + " product:" + product);
		let exporter = require('./exporter').newExporter({
			etnaConfig: etnaConfig,
			trace: etnaConfig.trace,
			outputFolder: etnaConfig.outputFolder,
			parallel: 1,
		}) as Exporter;

		function send(method: string, url: string, data?: any) {
			console.log(method + " " + url);
			let request = ezf.devices.http.client({
				method: method,
				url: baseUrl + url,
				headers: {
					//			Authorization: 'Basic YWRtaW46YWRtaW4=',
					'content-type': 'application/json',
				},
			}).proxyConnect();
			let response = request.end(JSON.stringify(data || {})).response();
			let result = JSON.parse(response.readAll());
			console.log(method + " " + url + "=>", result);
		}

		function syncLan(lan: string) {
			console.log(["synchronize", product, "for", lan].join(' '));
			let args = [lan];
			let driver = exporter.sqlDriver;
			let sql = getSql(driver, args);

			console.log(sql, args);
			console.log("user=",nodeLocalConfig.etna.sql);
			let context:any = {};
			driver.reader(sql, args)					  
				  .map((record: any) => Object.keys(record).reduce((r: any, k: string) =>
					(r[k.substring(0, k.length - 2)] = (record[k] === " " ? "" : record[k]), r), { product: product }) )
				  .transform((reader: ezf.Reader<Instance>, writer: ezf.Writer<Instance>) => {
					// Group records for the texts server
					let item: any;
					let items: any[] = [];
					while ((item = reader.read()) !== undefined) {
						item = transform(context,item);
						if (items.length >= 100) {
							writer.write(items);
							items = [];
						}
						item && items.push(item);
					}
					writer.write(items);
				})
				.pipe(ezf.devices.generic.writer((items: any) => items && send("POST", '/api/' + post, items)));
			
			if(afterTransform) {
				let descriptions = afterTransform(driver,context,lan);
				let items: any[] = [];
				while((items = descriptions.splice(0,100)) && items.length)  send("POST", '/api/' + post, items);
			}

			console.log([product, "for", lan, "synchronized"].join(' '));
		};
		
		if (langue!==undefined && langue.trim()!=="") {
			send("DELETE", ["/api", type + "L", product,langue].join('/'));
			syncLan(langue);
		} else {
			send("DELETE", ["/api", type + "S", product].join('/'));
			// Get languages defined for the product:		
			var results = exporter.buildExecute(_, null, "NBRLAN_0, " + (new Array(20)).fill(0).map((e, i) => "LAN_" + i).join(','), "ADOSSIER", [], {
				DOSSIER: etnaConfig.sql.user
			}, null, null);

			if (results.length) {
				for (var i = 0; i < results[0].NBRLAN; i++) syncLan(results[0].LAN[i]);
			}
		}
	}

	return {
		/// !doc
		/// ### exportMetadataAndPush(_, entityType, entityNames, options)
		/// Exports the metadata of set of entities.
		/// An export consists in :
		/// * Creating JSON files that contain the metadata
		/// * importing the generated JSON files to mongDb
		/// * committing and pushing the generated files to git
		///
		/// When no `entityType` is provided, all the types will be processed (tables, views, representations, ...).
		/// When no `entityNames` is provided, all the entities will be processed (ABANK, ABICOND, ...).
		///
		/// In the file system, the JSON files will be dispatched in many folders, according to 
		/// the pattern : `moduleName/entitySubDir/entityName`
		/// * moduleName : the name of the module that uses the meta (SUPERV, FINANCE BP, ...)
		/// * entitySubDir : depends of the type of entity : TABLES, ACTIONS, VIEWS, ....
		/// * entityName : the name of the entity
		/// There might be several generated files for a given entity, for instance, one describing
		/// the class, another for the SQL table, ...
		exportMetadataAndPush: function (_: _, entityType: string, entityNames: string[] | string = [], options: Options) {
			if (tracker) {
				tracker.phase = locale.format(module, "exportToJSONs");
				tracker.phaseDetail = locale.format(module, "exportToJSONs");
			}

			if (!Array.isArray(entityNames))
				entityNames = entityNames.split(',');

			options = options || {};

			var t0 = Date.now();
			var fileStructs = exportMetadataToJSONs(_, entityType, entityNames, options);
			_trace(locale.format(module, "exportDone", fileStructs.length, Math.round((Date.now() - t0) / 1000)), "info");
			var absoluteFilenames = fileStructs.map(function (fs) {
				return fs.absoluteFilename;
			})
			_trace(locale.format(module, "generatedFiles", absoluteFilenames));


			// Import the metadata to mongo db
			if (options.exportToMongo && fileStructs.length)
				importMetadataToMongo(_, fileStructs, entityType, false, options);
		},

		/// !doc
		/// ### removeMetadata(_, entityType, entityNames, options)
		/// Remove the metadata of set of entities.
		removeMetadata: function (_: _, entityType: string, entityNames: string[] | string = [], options: Options) {
			options = options || {};

			if (!Array.isArray(entityNames))
				entityNames = entityNames.split(',');

			var exporter = require('./exporter').newExporter({
				etnaConfig: etnaConfig,
				trace: etnaConfig.trace,
				outputFolder: etnaConfig.outputFolder,
				parallel: 1,
			});
			exporter.init(_, tracker);

			const entity = resourceManager.getEntityDescriptor(_, entityType);
			const primaryKey = Array.isArray(entity.primaryKey) ? entity.primaryKey : [entity.primaryKey];

			var mongoConfig = etnaConfig.mongo || {};
			const dbUrl = "mongodb://" + (mongoConfig.host || "localhost") + ":" + (mongoConfig.port || 27017) + "/" + mongoConfig.database;
			if (options.exportToMongo) { // or endpoint.useEtna ?
				var db = mongodb.MongoClient.connect(dbUrl, {
					db: {
						w: 1
					}
				}).then(_, _);
				db.open().then(_, _);
			}

			entityNames.forEach_(_, function (_, entityName) {
				const absolutePath = path.join(etnaConfig.outputFolder, exporter.product.name, entity.subdir, '_' + entityName + '.json');
				_trace("remove file " + absolutePath);
				fs.unlink(absolutePath, _);
				if (db) {
					const key = entityName.split('~').reduce(function (r, e, i) {
						r[primaryKey[i]] = e;
						return r;
					}, {} as any);
					_trace("remove " + JSON.stringify(key) + " from mongo collection " + entity.tableName);
					db.collection(entity.tableName).remove(key).then(_, _);
				}
			});
		},
		/// !doc
		/// ### importMetadata(_)
		/// Imports metadata from file system (JSON files) to mongoDb
		/// This function will import ALL the JSON files that were updated since the last time
		/// an import was done. It relies on git to retrieve the updated files (from the last sha1 to the HEAD) 
		importMetadata: function (_: _, options: Options) {
			options = options || {};

			_trace(locale.format(module, "processAll"), "info");
			try {
				importMetadataToMongo(_, undefined, undefined, false, options);
			} catch (err) {
				console.error("ERROR importMetadataToMongo : ", err);
			}

			trace(locale.format(module, "importDoneMark"));

		},

		importMetadataToMongo: importMetadataToMongo,
		syncText: _syncText,
		initTexts: _initTexts,
		initMenus: _initMenus,
	};
};