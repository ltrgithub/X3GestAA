"use strict";
var supervisor = require("etna-supervisor/lib/supervisor");
var fs = require("streamline-fs");
var ez = require("ez-streams");
var file = ez.devices.file;
var ezS3 = require("ez-s3");
var jsonTrans = require("ez-streams").transforms.json;
var s3Helper = require('ez-s3/lib/s3Helper');
var nodeLocalConfig = require('config');


var skipList;
// var skipList = /^(ACLOB|ADOCCLB|ADOCFLD|AESPION|AUDIT\w+|STAT|AMOULINETTE|APROCTEXTE|AWRKHIS\w+|CONTACT2)$/;
//var skipList = /^([XYZ].*|ACLOB|ADOCCLB|ADOCFLD|AESPION|AUDIT\w+|STAT|AMOULINETTE|ALISTER|ALOGIN|APROCTEXTE|AWRKHIS\w+|CONTACT2)$/;
//var skipList = /^(ATABLE|AABREV)$/;
var skipList = /^ADOC/;

function retrieveRootFolderName(_, superv) {
	var rootFolderName;
	var row = superv.sqlDriver.reader(_, "select VALEUR_0 from ADOVAL where PARAM_0=" + superv.sqlDriver.param(0), ['SUPDOS']).toArray(_)[0];
	if (!row)
		throw new Error("Could not retrieve the root folder name");
	rootFolderName = row.VALEUR_0;
	return rootFolderName;

}

function executeWithRetries(_, body, retriesCount) {
	retriesCount = retriesCount || 3;
	var lastError;
	for (var i = 0; i < retriesCount; i++) {
		try {
			if (i > 0)
				setTimeout(~_, 200);
			body(_);
			// if we reach this point, then everything is OK, the body could be executed.

			if (i > 0)
				console.log("executeWithRetries OK (" + i + ") : " + body.toString() + " / " + lastError.message);
			return;
		} catch (err) {
			// Nothing to do, just store the error.
			lastError = err;
		}
	};
	// we only reach this line when all the executions failed.
	// We just can thrown the last error
	throw lastError;
}

function importFiles(_, config, files, tracker) {
	// don't take risks with non local SQL databases for now
	if ((!config.sql.bypassLocalhost) && (config.sql.hostname !== "localhost"))
		throw new Error("IMPORT only allowed on localhost");

	function trace(str) {
		if (tracker)
			tracker.phaseDetail = str;
		if (config.trace)
			config.trace(str);
	}
	var superv = supervisor.create(_, config);

	// The current folder might be a child folder. To access to system tables (ADOSSIER, ...) we'll need to know the name of the root folder
	config.rootFolderName = retrieveRootFolderName(_, superv);

	var tableCount = 0;

	// Read the folder configuration file
	var folderFilename = nodeLocalConfig.etl.dataFolder + "/__folder.json";
	if (!fs.exists(folderFilename, _))
		throw new Error("The folder configuration file '__folder.json' is missing !!!");

	var folderConfig = file.text.reader(folderFilename).transform(jsonTrans.parser()).read(_);

	if (config.rootFolderName == superv.folderName) {
		// We are trying to import in a root folder
		if (!folderConfig.isRoot)
			throw new Error("The folder contains dumps from a root folder and can only be imported in a root folder");
	} else {
		if (folderConfig.isRoot)
			throw new Error("The folder contains dumps from a child folder and can only be imported in a child folder");
	}

	files.forEach_(_, function(_, fname) {
		if (/^__/.test(fname)) {
			// Skip the configuration files
			return;
		}
		var name = fname.substring(0, fname.length - 5); // strip .json extension
		// if ((name != "ABIDIMFLD") && (name != "ABIDIMFLD"))
		// 	return;
		if (skipList && skipList.test(name))
			return;
		if (tracker && tracker.abortRequested)
			return;
		if (tracker)
			tracker.phase = "importing file " + fname;

		var fullFilename = nodeLocalConfig.etl.dataFolder + '/' + fname;
		var reader = file.text.reader(fullFilename).transform(jsonTrans.parser());
		// The first JSON object in the object is the metadata of the table (see exportAll for more details)
		var tableDef = reader.read(_);

		tableDef.schemaName = superv.folderName;
		trace("Processing table " + tableDef.tableName + " from file " + fullFilename);
		tableCount++;

		//		tableDef.tableName = "ZZZZ" + tableDef.tableName;

		// ---------------------------------------------
		// Create table
		// ---------------------------------------------
		trace("\tCreating table");
		superv.sqlDriver.createTableFromTableDefinition(_, tableDef, {
			skipIndexes: true
		});
		// Note : for now, we don't create the indexes (it would slow down the insertions)
		// Indexes will be created once all the data are inserted in the table
		// ---------------------------------------------
		// Import data
		// ---------------------------------------------
		if (!false) {

			trace("\tImporting data");
			var sqlWriter = superv.sqlDriver.createTableWriter(_, tableDef);

			var rowCount = 0;
			while (true) {
				var inVal = reader.read(_);
				if (inVal === undefined) {
					// EOF
					sqlWriter.write(_, undefined);
					break;
				}

				//				console.log("--------------- " + JSON.stringify(inVal));
				tableDef.columns.forEach(function(column) {
					// A lot of properties will be read from json with a wrong type 
					// for instance, datetimes are read as string
					// The values need to be converted now into a javascript type that match the column definition
					switch (column.type) {
						case "date":
						case "datetime":
							if (inVal[column.name] === "NULL_DATE")
								inVal[column.name] = new Date(superv.sqlDriver.nullDate());
							else {
								// datetimes are read as strings and must be converted to Date
								inVal[column.name] = new Date(Date.parse(inVal[column.name]));
							}
							break;
						case "uuid":
						case "binary":
							// byte arrays are read as (hex) strings and must be converted to buffers
							inVal[column.name] = new Buffer(inVal[column.name], "hex");
							break;
					}
					//console.log("=============== " + column.name + " : " + column.type + " / " + inVal[column.name] + " / " + typeof(inVal[column.name]));
				});
				// console.log(">>> " + JSON.stringify(inVal));
				sqlWriter.write(_, inVal);
				rowCount++;
			}
			trace("\t" + rowCount + " rows processed");
		}

		// ---------------------------------------------
		// Create permissions
		// ---------------------------------------------
		trace("\tCreating permissions");
		superv.sqlDriver.createPermissions(_, tableDef, config);

		// ---------------------------------------------
		// Create the indexes
		// ---------------------------------------------
		trace("\tCreating indexes");
		superv.sqlDriver.createTableFromTableDefinition(_, tableDef, {
			onlyIndexes: true
		});

	});
	trace("Done : " + tableCount + " tables processed");
};



exports.exportAll = function(_, config, tracker, s3Cfg) {

	if (!nodeLocalConfig.etl || !nodeLocalConfig.etl.dataFolder)
		throw new Error("Missing etl.dataFolder in nodelocal.js.");
	config.trace = config.trace || console.log;

	if (s3Cfg) {
		if (!s3Cfg.connector)
			throw new Error("S3 'connector' is missing");
		if (!s3Cfg.settings)
			throw new Error("S3 'settings' are missing");
		if (!s3Cfg.settings.bucket)
			throw new Error("S3 'settings.bucket' is missing");
	}

	function trace(str) {
		if (tracker)
			tracker.phaseDetail = str;
		if (config.trace)
			config.trace(str);
	}

	function writeObjectToFile(_, filename, object) {
		var reader = ez.devices.array.reader([object]);
		reader = reader.transform(jsonTrans.formatter({
			space: '\t',
		}));
		reader.pipe(_, file.text.writer(filename));
	}

	function formatRow(_, superv, rec, decimalColumns) {
		rec.ROWID = undefined;
		Object.keys(rec).forEach(function(key) {
			if (Buffer.isBuffer(rec[key])) {
				// Buffers must be exposed as a hex string for example [125, 85, 69, 78] -> "7D55454E"
				rec[key] = rec[key].toString('hex').toUpperCase();
			}
			if (rec[key] instanceof Date) {
				if (superv.sqlDriver.isNullDate(rec[key]))
					rec[key] = "NULL_DATE";
			}
		});
		decimalColumns.forEach(function(columnName) {
			// We have to re-format the decimal so that exports from Oracle and from SqlServer look the same
			var val = rec[columnName];
			if (val) {
				val = val.trim();
				var startIdx = 0;
				var endIdx = val.length - 1;
				if ("0" === val[endIdx]) {

					// Remove trailing "0"
					while ((endIdx > 0) && ("0" === val[endIdx]))
						endIdx--;
					if ((endIdx >= 0) && ("." === val[endIdx])) {
						// The decimal ends with '.', remove it
						endIdx--;
					}
					val = val.slice(0, endIdx + 1);
				}
				if ((val.length == 0) || ("." === val[0]))
					val = "0" + val;
			}

			rec[columnName] = val;
		});
		//trace(tableDef.tableName + ": " + rowCount + " ... ");
		return rec;
	}



	if (!fs.exists(nodeLocalConfig.etl.dataFolder, _))
		fs.mkdir(nodeLocalConfig.etl.dataFolder, _);

	var superv = supervisor.create(_, config);

	// The current folder might be a child folder. To access to system tables (ADOSSIER, ...) we'll need to know the name of the root folder
	config.rootFolderName = retrieveRootFolderName(_, superv);

	// Check if a folder configuration file exists
	var folderFilename = nodeLocalConfig.etl.dataFolder + "/__folder.json";
	var progressFilename = nodeLocalConfig.etl.dataFolder + "/__progress.json";
	var folderConfig;
	var progressConfig;

	if (fs.exists(folderFilename, _)) {
		folderConfig = file.text.reader(folderFilename).transform(jsonTrans.parser()).read(_);
		if (folderConfig.name !== superv.folderName)
			throw new Error("Error : the output folder already contains dumps from folder '" + folderConfig.name + "'");

		if (fs.exists(progressFilename, _)) {
			progressConfig = file.text.reader(progressFilename).transform(jsonTrans.parser()).read(_);
			var msg = "Will resume previous export from configuration file : " + folderFilename;
			if (tracker) {
				tracker.phase = msg;
				config.trace && config.trace(msg);
			}
		}
	}

	progressConfig = progressConfig || {
		tables: {}
	};

	if (!folderConfig) {
		folderConfig = {
			name: superv.folderName,
			isRoot: config.rootFolderName == superv.folderName,
		};
		writeObjectToFile(_, folderFilename, folderConfig);
	}

	var count = {
		processed: 0,
		skipped: 0,
		upToDate: 0
	};
	var tableCountOk = 0,
		tableCountSkipped = 0;

	superv.sqlDriver.readTables(_, superv.folderName).forEach_(_, function(_, tableDef, i) {
		try {
			// if ((tableDef.tableName != "AABREV")&&(tableDef.tableName != "ABANK"))
			// 	return;
			if (tableDef.tableName != "ADOSSIER")
				return;
			superv.sqlDriver.readTableSchema(_, tableDef, tracker);

			if (skipList && skipList.test(tableDef.tableName))
				return;
			if (tracker && tracker.abortRequested)
				return;
			if (tracker)
				tracker.phase = "exporting " + tableDef.tableName;

			var previousTableExport = progressConfig.tables[tableDef.tableName];
			if (previousTableExport) {
				if (Date.parse(previousTableExport.lastUpdate) === Date.parse(tableDef.lastUpdate)) {
					// This table was already processed in the previous export
					trace(tableDef.tableName + " : up-to-date");
					count.upToDate++;
					return;
				}
			}

			if (tableDef.columns.some(function(col) {
				return /^blob$/.test(col.type);
			})) {
				console.error("BLOB COLUMN NOT SUPPORTED YET; SKIPPING " + tableDef.tableName);
				count.skipped++;
				return;
			}

			count.processed++;

			// Build a reader that will first return the metadata (tableDef) and then all the rows of the sql table
			var filename = nodeLocalConfig.etl.dataFolder + "/" + tableDef.tableName + ".json";
			var writer = file.text.writer(filename);
			var rowCount = 0;
			var message = filename;

			var decimalColumns = [];
			tableDef.columns.forEach(function(columnDef) {
				if ("decimal" === columnDef.type)
					decimalColumns.push(columnDef.name);
			});


			var tableReader;
			if (tableDef.tableName == "ADOSSIER") {
				// Only read the row related to the current folder
				tableReader = superv.sqlDriver.createTableReader(_, tableDef, [{
					key: 'DOSSIER_0',
					operator: '=',
					value: superv.folderName
				}]);
			} else {
				// Read all the rows
				tableReader = superv.sqlDriver.createTableReader(_, tableDef);
			}

			tableReader = tableReader.map(function(_, rec) {
				rowCount++;
				return formatRow(_, superv, rec, decimalColumns);
			});
			tableReader = ez.devices.array.reader([tableDef]).concat(tableReader);

			//var tableReader = ez.devices.array.reader([tableDef]);

			// try {
			var transformedReader = tableReader.transform(jsonTrans.formatter({
				space: '\t',
			}));

			if (s3Cfg) {
				// An amazon S3 configuration is provided, we have to tee the reader to a S3 writer.
				s3Cfg.settings.key = tableDef.tableName + ".json";
				message += ', S3(' + s3Cfg.settings.bucket + '/' + s3Cfg.settings.key + ')';
				var s3Writer = ezS3.writer(_, s3Cfg.connector, s3Cfg.settings);
				transformedReader = transformedReader.tee(s3Writer);
			}

			//var reader = ez.devices.array.reader([tableDef]).concat(transformedReader);

			transformedReader.pipe(_, writer);
			// } catch (ex) {
			// 	if (/ORA-00942/.test(ex.message)) trace(tableDef.tableName + ": " + ex.message);
			// 	else console.error("Could not process table " + tableDef.tableName + ", reason = " + ex.message); // TEMP HACK FOR INCONSISTENT DATA throw ex;
			// }
			progressConfig.tables[tableDef.tableName] = {
				lastUpdate: tableDef.lastUpdate
			};

			trace(tableDef.tableName + ": " + rowCount + " -> " + message);
		} catch (err) {
			console.error("ERROR " + err.message);
		} finally {
			// Write the progress configuration file so that next export will not process again the same tables
			writeObjectToFile(_, progressFilename, progressConfig);
		}
	});

	trace("Done : " + count.processed + " processed, " + count.skipped + " skipped, " + count.upToDate + " upToDate");
};

exports.importFilesFromS3 = function(_, config, tracker, s3Cfg) {
	if (!nodeLocalConfig.etl || !nodeLocalConfig.etl.dataFolder)
		throw new Error("Missing etl.dataFolder in nodelocal.js.");

	if (!s3Cfg)
		throw new Error('S3 configuration is missing');
	if (!s3Cfg.connector)
		throw new Error("S3 'connector' is missing");
	if (!s3Cfg.settings)
		throw new Error("S3 'settings' are missing");
	if (!s3Cfg.settings.bucket)
		throw new Error("S3 'settings.bucket' is missing");

	function trace(str) {
		if (tracker)
			tracker.phaseDetail = str;
		if (config.trace)
			config.trace(str);
	}

	var list = s3Helper.listObjects(_, s3Cfg.connector, s3Cfg.settings);
	var files = list.Contents.map(function(item) {
		console.log(item);
		return item.Key;
	});

	if (!fs.exists(nodeLocalConfig.etl.dataFolder, _))
		fs.mkdir(nodeLocalConfig.etl.dataFolder, _);

	var downloadedFiles = [];
	// First, we have to import the file to the local file system
	files.forEach_(_, function(_, name) {
		s3Cfg.settings.key = name;
		var filename = nodeLocalConfig.etl.dataFolder + "/" + name;

		var fileWriter = file.text.writer(filename);
		trace("Download file : " + name);

		var s3Reader = ezS3.reader(_, s3Cfg.connector, s3Cfg.settings);
		try {
			s3Reader.pipe(_, fileWriter);
		} catch (err) {
			console.error("Could not download file '" + name + "', reason = " + err.message);
		}
		downloadedFiles.push(name);
	});

	// Now, we can process these files (i.e. import them into Sql)
	importFiles(_, config, downloadedFiles, tracker);

};

exports.importAll = function(_, config, tracker) {
	if (!nodeLocalConfig.etl || !nodeLocalConfig.etl.dataFolder)
		throw new Error("Missing etl.dataFolder in nodelocal.js.");

	if (!fs.exists(nodeLocalConfig.etl.dataFolder, _))
		fs.mkdir(nodeLocalConfig.etl.dataFolder, _);
	var files = fs.readdir(nodeLocalConfig.etl.dataFolder, _);
	return importFiles(_, config, files, tracker);
};

exports.importTest = function(_, config, tracker) {
	if (!nodeLocalConfig.etl || !nodeLocalConfig.etl.dataFolder)
		throw new Error("Missing etl.dataFolder in nodelocal.js.");

	if (!fs.exists(nodeLocalConfig.etl.dataFolder, _))
		fs.mkdir(nodeLocalConfig.etl.dataFolder, _);
	var files = fs.readdir(nodeLocalConfig.etl.dataFolder, _);
	//	return importFiles(_, config, ["AABREV.json"], tracker);
	return importFiles(_, config, ["ABITABDAT.json"], tracker);
};