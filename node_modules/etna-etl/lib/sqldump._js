"use strict";
var supervisor = require("etna-supervisor/lib/supervisor");
var fs = require("streamline-fs");
var ez = require("ez-streams");
var file = ez.devices.file;
var ezS3 = require("ez-s3");
var jsonTrans = require("ez-streams").transforms.json;
var s3Helper = require('ez-s3/lib/s3Helper');
var nodeLocalConfig = require('config');


var skipList;
// var skipList = /^(ACLOB|ADOCCLB|ADOCFLD|AESPION|AUDIT\w+|STAT|AMOULINETTE|APROCTEXTE|AWRKHIS\w+|CONTACT2)$/;
//var skipList = /^([XYZ].*|ACLOB|ADOCCLB|ADOCFLD|AESPION|AUDIT\w+|STAT|AMOULINETTE|ALISTER|ALOGIN|APROCTEXTE|AWRKHIS\w+|CONTACT2)$/;
//var skipList = /^(ATABLE|AABREV)$/;
//var skipList = /^(ATABLE)$/;

function executeWithRetries(_, body, retriesCount) {
	retriesCount = retriesCount || 3;
	var lastError;
	for (var i = 0; i < retriesCount; i++) {
		try {
			if (i > 0)
				setTimeout(~_, 200);
			body(_);
			// if we reach this point, then everything is OK, the body could be executed.

			if (i > 0)
				console.log("executeWithRetries OK (" + i + ") : " + body.toString() + " / " + lastError.message);
			return;
		} catch (err) {
			// Nothing to do, just store the error.
			lastError = err;
		}
	};
	// we only reach this line when all the executions failed.
	// We just can thrown the last error
	throw lastError;
}

function importFiles(_, config, files, tracker) {
	// don't take risks with non local SQL databases for now
	if (config.sql.hostname !== "localhost") throw new Error("IMPORT only allowed on localhost");

	function trace(str) {
		if (tracker)
			tracker.phaseDetail = str;
		if (config.trace)
			config.trace(str);
	}
	var superv = supervisor.create(_, config);

	var tableCount = 0;

	files.forEach_(_, function(_, fname) {
		var name = fname.substring(0, fname.length - 5); // strip .json extension
		// if ((name != "ABATABTD") && (name != "ABANK"))
		// 	return;
		if (skipList && skipList.test(name))
			return;
		if (tracker && tracker.abortRequested)
			return;
		if (tracker)
			tracker.phase = "importing file " + fname;

		var reader = file.text.reader(nodeLocalConfig.etl.dataFolder + '/' + fname).transform(jsonTrans.parser());
		// The first JSON object in the object is the metadata of the table (see exportAll for more details)
		var tableDef = reader.read(_);

		tableDef.schemaName = superv.folderName;
		trace("Processing table " + tableDef.tableName);
		tableCount++;


		tableDef.tableName = "ZZZZ" + tableDef.tableName;

		// ---------------------------------------------
		// Create table
		// ---------------------------------------------
		trace("\tCreating table in database");
		superv.sqlDriver.createTableFromTableDefinition(_, tableDef, {
			skipIndexes: true
		});
		// Note : for now, we don't create the indexes (it would slow down the insertions)
		// Indexes will be created once all the data are inserted in the table
		// ---------------------------------------------
		// Import data
		// ---------------------------------------------
		if (!false) {

			trace("\tImporting data");
			var sqlWriter = superv.sqlDriver.createTableWriter(_, tableDef);

			var rowCount = 0;
			while (true) {
				var inVal = reader.read(_);
				if (inVal === undefined) {
					// EOF
					sqlWriter.write(_, undefined);
					break;
				}

				//				console.log("--------------- " + JSON.stringify(inVal));
				tableDef.columns.forEach(function(column) {
					// A lot of properties will be read from json with a wrong type 
					// for instance, datetimes are read as string
					// The values need to be converted now into a javascript type that match the column definition
					switch (column.type) {
						case "datetime":
							// datetimes are read as strings and must be converted to Date
							inVal[column.name] = new Date(Date.parse(inVal[column.name]));
							break;
						case "uuid":
						case "binary":
							// byte arrays are read as array and must be converted to buffers
							inVal[column.name] = new Buffer(inVal[column.name]);
							break;
					}
					//console.log("=============== " + column.name + " : " + column.type + " / " + inVal[column.name]);
				});
				//				console.log("*************************************");
				sqlWriter.write(_, inVal);
				rowCount++;
			}
			trace("\t" + rowCount + " rows processed");
		}

		// ---------------------------------------------
		// Create permissions
		// ---------------------------------------------
		trace("\tCreating permissions");
		superv.sqlDriver.createPermissions(_, tableDef);

		// ---------------------------------------------
		// Create the indexes
		// ---------------------------------------------
		trace("\tCreating indexes");
		superv.sqlDriver.createTableFromTableDefinition(_, tableDef, {
			onlyIndexes: true
		});

	});
	trace("Done : " + tableCount + " tables processed");
};



exports.exportAll = function(_, config, tracker, s3Cfg) {

	if (!nodeLocalConfig.etl || !nodeLocalConfig.etl.dataFolder)
		throw new Error("Missing etl.dataFolder in nodelocal.js.");
	config.trace = config.trace || console.log;

	if (s3Cfg) {
		if (!s3Cfg.connector)
			throw new Error("S3 'connector' is missing");
		if (!s3Cfg.settings)
			throw new Error("S3 'settings' are missing");
		if (!s3Cfg.settings.bucket)
			throw new Error("S3 'settings.bucket' is missing");
	}

	function trace(str) {
		if (tracker)
			tracker.phaseDetail = str;
		if (config.trace)
			config.trace(str);
	}

	if (!fs.exists(nodeLocalConfig.etl.dataFolder, _))
		fs.mkdir(nodeLocalConfig.etl.dataFolder, _);
	var superv = supervisor.create(_, config);

	var countOk = 0,
		countSkipped = 0;
	superv.sqlDriver.readTables(_, superv.folderName).forEach_(_, function(_, tableDef, i) {

		//if (i < 125) return;
		if (skipList && skipList.test(tableDef.tableName))
			return;
		if (tracker && tracker.abortRequested)
			return;
		if (tracker)
			tracker.phase = "exporting " + tableDef.tableName;

		superv.sqlDriver.readTableSchema(_, tableDef);

		if (tableDef.columns.some(function(col) {
			return /^blob$/.test(col.type);
		})) {
			console.error("BLOB COLUMN NOT SUPPORTED YET; SKIPPING " + tableDef.tableName);
			countSkipped++;
			return;
		}
		countOk++;
		// if ((tableDef.tableName != "BANREC") && (tableDef.tableName != "BANKPOSD"))
		// 	return;

		var tableReader = superv.sqlDriver.createTableReader(_, tableDef);

		// Build a reader that will first return the metadata (tableDef) and then all the rows of the sql table
		//var reader = tableReader;
		var reader = ez.devices.array.reader([tableDef]).concat(tableReader);

		var filename = nodeLocalConfig.etl.dataFolder + "/" + tableDef.tableName + ".json";

		var writer = file.text.writer(filename);
		//writer.write(_, JSON.stringify(tableDef));
		var rec, count = 0;
		var message = filename;

		// try {
		var transformedReader = reader.map(function(_, rec) {
			rec.ROWID = undefined;
			count++;
			//trace(tableDef.tableName + ": " + count + " ... ");
			return rec;
		}).transform(jsonTrans.formatter({
			space: '\t',
		}));

		if (s3Cfg) {
			// An amazon S3 configuration is provided, we have to tee the reader to a S3 writer.
			s3Cfg.settings.key = tableDef.tableName + ".json";
			message += ', S3(' + s3Cfg.settings.bucket + '/' + s3Cfg.settings.kezey + ')';
			var s3Writer = ezS3.writer(_, s3Cfg.connector, s3Cfg.settings);
			transformedReader = transformedReader.tee(s3Writer);
		}
		transformedReader.pipe(_, writer);
		// } catch (ex) {
		// 	if (/ORA-00942/.test(ex.message)) trace(tableDef.tableName + ": " + ex.message);
		// 	else console.error("Could not process table " + tableDef.tableName + ", reason = " + ex.message); // TEMP HACK FOR INCONSISTENT DATA throw ex;
		// }
		trace(tableDef.tableName + ": " + count + " -> " + message);
	});
	trace("Done : " + countOk + " processed, " + countSkipped + " skipped");
};

exports.importFilesFromS3 = function(_, config, tracker, s3Cfg) {
	if (!nodeLocalConfig.etl || !nodeLocalConfig.etl.dataFolder)
		throw new Error("Missing etl.dataFolder in nodelocal.js.");

	if (!s3Cfg)
		throw new Error('S3 configuration is missing');
	if (!s3Cfg.connector)
		throw new Error("S3 'connector' is missing");
	if (!s3Cfg.settings)
		throw new Error("S3 'settings' are missing");
	if (!s3Cfg.settings.bucket)
		throw new Error("S3 'settings.bucket' is missing");

	function trace(str) {
		if (tracker)
			tracker.phaseDetail = str;
		if (config.trace)
			config.trace(str);
	}

	var list = s3Helper.listObjects(_, s3Cfg.connector, s3Cfg.settings);
	var files = list.Contents.map(function(item) {
		console.log(item);
		return item.Key;
	});

	if (!fs.exists(nodeLocalConfig.etl.dataFolder, _))
		fs.mkdir(nodeLocalConfig.etl.dataFolder, _);

	var downloadedFiles = [];
	// First, we have to import the file to the local file system
	files.forEach_(_, function(_, name) {
		s3Cfg.settings.key = name;
		var filename = nodeLocalConfig.etl.dataFolder + "/" + name;

		var fileWriter = file.text.writer(filename);
		trace("Download file : " + name);

		var s3Reader = ezS3.reader(_, s3Cfg.connector, s3Cfg.settings);
		try {
			s3Reader.pipe(_, fileWriter);
		} catch (err) {
			console.error("Could not download file '" + name + "', reason = " + err.message);
		}
		downloadedFiles.push(name);
	});

	// Now, we can process these files (i.e. import them into Sql)
	importFiles(_, config, downloadedFiles, tracker);

};

exports.importAll = function(_, config, tracker) {
	if (!nodeLocalConfig.etl || !nodeLocalConfig.etl.dataFolder)
		throw new Error("Missing etl.dataFolder in nodelocal.js.");

	if (!fs.exists(nodeLocalConfig.etl.dataFolder, _))
		fs.mkdir(nodeLocalConfig.etl.dataFolder, _);
	var files = fs.readdir(nodeLocalConfig.etl.dataFolder, _);
	return importFiles(_, config, files, tracker);
};

exports.importTest = function(_, config, tracker) {
	if (!nodeLocalConfig.etl || !nodeLocalConfig.etl.dataFolder)
		throw new Error("Missing etl.dataFolder in nodelocal.js.");

	if (!fs.exists(nodeLocalConfig.etl.dataFolder, _))
		fs.mkdir(nodeLocalConfig.etl.dataFolder, _);
	var files = fs.readdir(nodeLocalConfig.etl.dataFolder, _);
	//	return importFiles(_, config, ["AABREV.json"], tracker);
	return importFiles(_, config, ["ABITABDAT.json"], tracker);
};