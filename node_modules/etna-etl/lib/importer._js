"use strict";
var fs = require("streamline-fs"),
	path = require('path'),
	mongodb = require('mongodb'),
	datetime = require('syracuse-core').types.datetime,
	jsonHelper = require('scm-helper/lib/jsonHelper');

exports.newImporter = function(_, config) {
	var trace = config.trace;
	var jsondir = config.metaFolder;
	var mongoConfig = config.mongo || {};
	var mgConf = mongoConfig;
	var dbUrl = "mongodb://" + (mgConf.connectionString || ((mgConf.host || "localhost") + ":" + (mgConf.port || 27017))) + "/" + (mgConf.database || "x3meta");

	console.log("IMPORT TO MONGO : " + dbUrl);
	var db = mongodb.MongoClient.connect(dbUrl, mgConf.options || {
		db: {
			w: 1
		}
	}, _);

	// files[xxx] contains only ABSOLUTE filenames
	var files = {
		/*		
		TEXTS: [],
		MENUS: []
*/
	};

	buildFilesList(_);

	function readSingleJsonFile(_, path) {
		var data = JSON.parse(fs.readFile(path, 'utf8', _));
		return jsonHelper.cleanData(data);
	}

	function buildFilesList(_) {
		// Note : config.absoluteFilenames contains ABSOLUTE filenames
		if (config.absoluteFilenames && config.absoluteFilenames.length) {
			config.absoluteFilenames.forEach(function(absoluteFilename) {
				var result = jsonHelper.parseMetaFilename(absoluteFilename);
				files[result.type] = files[result.type] || {};
				files[result.type][result.shortFilename.substring(0, result.shortFilename.length - 5)] = absoluteFilename;
			});
			return;
		}

		// No filenames are provided, we have to parse jsondir to retrieve all the available metadata files

		fs.readdir(jsondir, _).forEach_(_, function(_, module) {
			var moduleDir = path.join(jsondir, module);
			if (fs.stat(moduleDir, _).isDirectory()) {
				fs.readdir(moduleDir, _).forEach_(_, function(_, type) {
					var typeDir = path.join(moduleDir, type);
					if (fs.stat(typeDir, _).isDirectory()) {
						var entries = fs.readdir(typeDir, _);

						entries.filter(function(name) {
							if (!(/\.json$/).test(name))
								return false;
							return true;
						}).forEach_(_, function(_, name) {
							files[type] = files[type] || {};
							files[type][name.substring(0, name.length - 5)] = path.join(typeDir, name);
						});

						entries.filter(function(name) {
							return (/^\w\w\w$/).test(name);
						}).forEach_(_, function(_, lan) {
							fs.readdir(typeDir + "/" + lan, _).filter(function(name) {
								return (/\.json$/).test(name);
							}).forEach_(_, function(_, name) {
								var dest = type === "MENUS" ? files.MENUS : files.TEXTS;
								var absoluteFilename = path.join(jsondir, module, type, lan, name);
								if (config.absoluteFilenames) {
									// The file must belong to the provided list of files to process
									if (config.absoluteFilenames.indexOf(absoluteFilename) == -1)
										return;
								}
								dest.push(absoluteFilename);
							});
						});
					}
				});
			}
		});

	}

	return {
		open: function(_) {
			//db.open(_);
			return this;
		},
		fillTables: function(_, entity, tracker, dropCollectionBeforeImport) {

			function addDiagnose(message, severity) {
				if (!tracker)
					return;
				tracker.$diagnoses = tracker.$diagnoses || [];
				tracker.$diagnoses.push({
					$severity: severity || "info",
					$message: message,
				});
			}

			trace && trace("inserting " + entity.title + " metadata ...");
			var t0 = Date.now();
			var groups = [];
			var lastShortName = "";
			var currentGroup;
			Object.keys(files[entity.subdir] || {}).sort().forEach(function(name) {
				// Note 1 : when dealing with activity codes, there will be more than one file for an entity
				// for instance, there will be ATABLES/AOBJTXT.json that contains all the common properties
				// and ATABLES/AOBJTXT.ARCH.json that will only contain the properties whose activity code is 'ARCH'
				// All the files that describe the same entity must be assembled and imported in one pass otherwise, 
				// the import of the second file of an entity might possibly delete the import of the first file
				var index = name.indexOf('.');
				var shortName;
				if (-1 === index) {
					// We are on the main file (AOBJTXT => AOBJTXT.json)
					shortName = name;
				} else {
					// We are on a secondary file (AOBJTXT.ARCH => AOBJTXT.ARCH.json)
					shortName = name.substring(0, index);
				}
				if (shortName !== lastShortName) {
					// This is a new group
					lastShortName = shortName;
					currentGroup = [];
					groups.push(currentGroup);
				}
				currentGroup.push(name);
			});
			var coln = db.collection(entity.tableName, _);

			if (dropCollectionBeforeImport) {
				addDiagnose("Full synchronization : dropping collection " + entity.tableName);
				coln.drop();
			}

			var etag = datetime.now().toString();
			groups.forEach_(_, function(_, group, i) {
				var allFiles = group.map(function(name) {
					return files[entity.subdir][name];
				});
				if (tracker && tracker.abortRequested) return;

				addDiagnose("Importing " + entity.title + " " + (i + 1) + "/" + groups.length + " : " + group[0]);

				trace && trace(entity.title + ": " + (i + 1) + "/" + groups.length + " importing " + group[0] + " from " + allFiles);
				var data = jsonHelper.readAndMergeJsonFiles(_, allFiles, entity, {
					cleanData: true
				});

				var filter = {};
				filter[entity.primaryKey] = data[entity.primaryKey];
				data._etag = etag;
				coln.update(filter, data, {
					upsert: true
				}, _);

			});
			if (tracker && tracker.abortRequested) return;
			trace && trace(entity.title + ": " + groups.length + " resources created in " + Math.round((Date.now() - t0) / 1000) + " seconds");

			addDiagnose("Create unique index based on primary key (" + entity.primaryKey + ")");
			console.log("Create unique index based on primary key (" + entity.title + "." + entity.primaryKey + ")");
			var keys = {};
			keys[entity.primaryKey] = 1;
			coln.ensureIndex(keys, {
				unique: true
			}, _);
		},
		readInstance: function(_, entity, key) {
			var filter = {};
			filter[entity.primaryKey] = key;
			return db.collection(entity.tableName, _).find(filter, _).toArray(_)[0];
		},
	};
};