"use strict";

var fs = require('streamline-fs'),
	path = require('path'),
	program = require('commander'),
	mongodb = require('streamline-mongodb');

var gitWrapper = require('git-wrapper2');


exports.newSync = function(config, tracker) {
	var solutionPath = config.solutionPath || path.resolve(__dirname, '../../../tmp');
	config.metaFolder = config.metafolder || "META";
	config.trace = config.trace || console.log;

	var trace = config.trace;
	var entities = [],
		texts = [];

	function getResources(_, folder) {
		var resPath = path.join(__dirname, folder);
		if (program.test) {
			return fs.exists(path.join(resPath, program.test + "._js"), _) ? [require(path.join(resPath, program.test))] : [];
		} else {
			return fs.readdir(resPath, _).map_(_, function(_, filename) {
				return (/(.*)\.(_js)/).test(filename) ? require(path.join(resPath, filename)) : null;
			}).filter(function(file) {
				return file !== null;
			});
		}
	};

	function getResource(_, folder, entityType) {
		var filename = path.join(__dirname, folder, entityType);
		return require(filename);
	};

	function extractMetadata(_, entityType, entityName) {
		var exporter = require("etna-etl/lib/exporter").newExporter({
			sql: config.sql,
			force: true, // for now!
			parallel: 1,
		});
		var entityExporter = require("etna-etl/lib/export-entity");
		var entityResource = getResource(_, "entities", entityType);
		var result = entityExporter.extractSingle(_, exporter, entityResource.entity, entityName);
		return result;
	}

	function exportMetadataToJSONs(_, entityNames) {
		var filenames = [];
		if (tracker)
			tracker.phase = "Export metadata to JSON files ...";
		var exporter = require("etna-etl/lib/exporter").newExporter({
			sql: config.sql,
			trace: config.trace,
			solutionPath: solutionPath,
			metaFolder: config.metaFolder,
			force: true, // for now!
			parallel: 1,
		});

		// if (!program.test || program.clean) exporter.clean(_);

		exporter.init(_);
		if (!entityNames) {
			// It's a global export, we need to clean the output folder
			exporter.clean(_);
		}
		exporter.check(_);

		var entityExporter = require("etna-etl/lib/export-entity");
		entities = getResources(_, "entities");
		entities.forEach_(_, function(_, entityResource) {
			var f = entityExporter.run(_, exporter, entityResource.entity, null, entityNames);
			filenames.push.apply(filenames, f);
		});
		/*
	var textExporter = require("etna-etl/lib/export-text");
	texts = getResources(_, "texts");
	texts.forEach_(_, function(_, textResource) {
		textExporter.run(_, exporter, textResource.text);
	});
*/
		return filenames;
	}

	function getGitWrapper() {
		var options = {
			"git-dir": config.solutionPath + '\\.git'
		};
		var git = new gitWrapper(options);
		return git;
	}

	function executeGitCommand(_, title, cmd) {
		_log("'" + title + "' ...");
		var git = getGitWrapper();
		var process = cmd(git);

		var result = {
			stdout: '',
			stderr: '',
			exitCode: 0
		};

		process.stdout.on("data", function(txt) {
			result.stdout += txt.toString('utf8');
		});
		process.stderr.on("data", function(txt) {
			result.stderr += txt;
		});

		try {
			process.on("close", ~_);
		} catch (err) {
			if (typeof err == 'number')
				result.exitCode = err;
		}
		_log("'" + title + "' done, exitCode = " + JSON.stringify(result.exitCode));
		return result;
	}

	function gitAdd(_, filenames) {
		filenames.forEach_(_, function(_, filename) {
			executeGitCommand(_, "git add", function(git) {
				return git.add(filename);
			});
			_log("   " + filename + " added ... ", "info");
		});
	}

	function gitPull(_) {
		if (tracker)
			tracker.phase = "Git pull ...";
		// Try 5 times (may have some temporary network related problems)		
		for (var attempt = 0; attempt < 5; attempt++) {
			var result = executeGitCommand(_, 'git pull', function(git) {
				return git.spawn("pull");
			});
			if (result.exitCode == 0) {
				if (tracker)
					(tracker.$diagnoses = tracker.$diagnoses || []).push({
						severity: "success",
						message: "'git pull' ok",
					});
				return true;
			}
			_log("'git pull' failed, will try again in 5 seconds, reason = " + result.stderr, "warning");
			setTimeout(~_, 5000);
		}
		throw new Error("Cancel : 'git pull' failed 5 successive times, last reason = " + result.stderr);
		return false;
	}

	function gitCommitAndPush(_, msg) {
		if (tracker)
			tracker.phase = "Commit/push to git ...";
		var result = executeGitCommand(_, "git commit", function(git) {
			return git.commit(msg, ['-a']);
		});
		if (result.exitCode == 1) {
			// Everything up-to-date
			_log("'git commit' done, nothing to push", "success");
			return;
		}
		if (tracker)
			(tracker.$diagnoses = tracker.$diagnoses || []).push({
				severity: "success",
				message: "'git commit' ok",
			});
		result = executeGitCommand(_, "git push", function(git) {
			return git.push("", "");
		});
		if (tracker)
			(tracker.$diagnoses = tracker.$diagnoses || []).push({
				severity: "success",
				message: "'git push' ok",
			});
	}

	function getFilenamesFromGitDiff(_, fromSha1, toSha1) {
		var allText = '';
		executeGitCommand(_, "git diff --stat", function(git) {
			var process = git.spawn("diff", ['--stat', fromSha1 + '..' + toSha1]);
			process.stdout.on("data", function(txt) {
				allText += txt;
			});
			return process;
		});

		var filenames = [];

		allText.split('\n').forEach(function(item) {
			// Each line looks like 'META/SUPERV/CLASSES/ATABLE.json         | 2357 ++++++++++++++'
			var reg = /\s*([^\s]+)\s*\|\s*\d+\s*([\+\-]+)/.exec(item);
			if (reg) {
				var filename = reg[1];
				filenames[filename] = true;
			}
		});
		return Object.keys(filenames);
	}

	function getGitHead(_) {
		var text = '';
		executeGitCommand(_, "git log -1", function(git) {
			var process = git.spawn("log", [-1, '--oneline', '--no-abbrev-commit']);
			process.stdout.on("data", function(txt) {
				text += txt;
			});
			return process;
		});
		// The line to parse looks like '696f4ed3ca7823edb78be87959522068e78ab93b Commit modifications of entities AABREV'
		return /(\w+)/.exec(text)[0];
	}

	function generatePatch(_) {
		if (tracker)
			tracker.phase = "Generating update patch ...";
		console.log("------------------- GENERATION DU PATCH ------------------------");
	}

	function importMetadataToMongo(_, filenamesToimport) {

		if (tracker)
			tracker.phase = "Import to mongo db ...";

		var importer = require("etna-etl/lib/importer").newImporter(_, {
			trace: config.trace,
			rootFolder: solutionPath,
			metaFolder: config.metaFolder,
			mongo: config.mongo,
			filenames: filenamesToimport,
		}).open(_);
		importer.buildFilesList(_);
		if (!program.test) importer.createTables(_);
		entities = getResources(_, "entities");
		entities.forEach_(_, function(_, entityResource) {
			importer.fillTables(_, entityResource.entity);
		});
		importer.fillTexts(_);
	}

	function _log(message, severity) {
		trace && trace(message);
		if (severity) {
			if (tracker)
				(tracker.$diagnoses = tracker.$diagnoses || []).push({
					severity: severity,
					message: message,
				});
		}

	}

	return {
		/// !doc
		/// # Copies the metadata of a single entity from Sql (Sql Server/Oracle) to MongoDb
		/// The exported entity is described by its type (classes, views, representations, ...)
		/// and its name.
		/// for instance, to copy the metadata of table 'AABREV' : `copyMetaFromSqlToMongo(_, "tables", "AABREV")`
		copyMetaFromSqlToMongo: function(_, entityType, entityName) {
			var meta = extractMetadata(_, entityType, entityName);
			var importer = require("etna-etl/lib/importer").newImporter(_, {
				mongo: config.mongo,
			}).open(_);
			importer.persistMetadataInMongo(_, meta.tableName, meta.data);
		},

		/// !doc
		/// # Exports the metadata of set of entities.
		/// An export consists in :
		/// - Creating JSON files that contain the metadata
		/// - importing the generated JSON files to mongDb
		/// - committing and pushing the generated files to git
		///
		/// When no `entityNames` is provided, all the entities will be exported.
		///
		/// In the file system, the JSON files will be dispatched in many folders, according to 
		/// the pattern : `moduleName/entitySubDir/entityName`
		/// - moduleName : the name of the module that uses the meta (SUPERV, FINANCE BP, ...)
		/// - entitySubDir : depends of the type of entity : TABLES, ACTIONS, VIEWS, ....
		/// - entityName : the name of the entity
		/// There might be several generated files for a given entity, for instance, one describing
		/// the class, another for the SQL table, ...
		exportMetadataAndPush: function(_, entityNames) {
			var commitMessage;
			if (entityNames)
				commitMessage = "Commit modifications of entities " + entityNames.join(",");
			else
				commitMessage = "Global commit";

			gitPull(_);

			var filenames = exportMetadataToJSONs(_, entityNames);

			// Import the metadata to update metadata in mongo
			importMetadataToMongo(_, filenames);

			gitAdd(_, filenames);
			gitCommitAndPush(_, commitMessage);
		},

		/// !doc
		/// # Imports metadata from file system (JSON files) to mongoDb
		/// This function will import all the JSON files that were updated since the last time
		/// an import was done. It relies on git to retrieve the updated files (from the last sha1 to the HEAD) 
		importMetadata: function(_) {
			gitPull(_);
			var mongoConfig = config.mongo || {};
			var server = new mongodb.Server(mongoConfig.host || "localhost", mongoConfig.port || 27017, {});
			var db = new mongodb.Db(mongoConfig.database || "x3meta", server, {
				w: 1 //"majority"
			});
			db.open(_);

			// first, we have to retrieve the sha1 of the last time we used this import
			var coln = db.collection("user_settings", _);
			var last = coln.find({
				type: "last_import_meta"
			}, _).toArray(_)[0];

			var headSha1 = getGitHead(_);

			// We have to process all the files that we be retrieved by the 'pull git'
			var filenames;
			var somethingToImport = true;
			_log("Importing to Mongodb", "info");
			if (last) {
				// We only have to process files that were modified since the last import
				// i.e those who are concerned by a commit from the last sha1 to HEAD
				if (last.data.sha1 === headSha1) {
					_log("Mongodb is already up-to-date", "success");
					return;
				}
				filenames = getFilenamesFromGitDiff(_, last.data.sha1, headSha1);
				_log("Processing files from revisions : " + last.data.sha1 + ".." + headSha1, "info");

				if (!filenames || filenames.length == 0) {
					_log("Nothing to process", "success");
					somethingToImport = false;
				} else {
					_log("Found " + filenames.length + " files to process...", "info");
				}
			} else {
				// first import. we have to import all the files
				_log("Processing all files", "info");
			}

			if (somethingToImport)
				importMetadataToMongo(_, filenames);

			// Now, we have to write the current HEAD so that the next import will only process
			// files from this head.
			var sha1Doc = {
				type: "last_import_meta",
				data: {
					sha1: headSha1
				}
			};
			if (last)
				coln.update({
					type: "last_import_meta"
				}, sha1Doc, _);
			else
				coln.insert(sha1Doc, _);

			_log("Reference head = " + headSha1, "info");

			generatePatch(_);

		},

		exportData: function(_, tracker) {
			require("etna-etl/lib/sqldump").exportAll(_, config);
		},

		importData: function(_, tracker) {
			require("etna-etl/lib/sqldump").importAll(_);
		}


	};
};