"use strict";

/// !doc
/// # Sync Metadata
/// Tools to synchronize metadata from Sql (SqlServer, oracle, ...) to mongodb
/// Usage : 
/// ```
/// var sync = require("etna-etl/lib/sync").newSync(_, etnaConfig, tracker);
/// sync.exportMetadataAndPush(_);
/// ```

var locale = require('streamline-locale'),
	fs = require('streamline-fs'),
	path = require('path'),
	mongodb = require('mongodb'),
	gitWrapper = require('git-wrapper'),
	globals = require('streamline/lib/globals'),
	nodeLocalConfig = require('config'),
	jsonHelper = require('scm-lib/jsonHelper'),
	ez = require('ez-streams'),
	resourceManager = require('scm-lib/resourceManager');

var gitWrapperInstance,
	defaultOptions = {
		skipGit: true
	};

exports.$exported = true;

function getEndpoint(_, filters) {
	var endpoint;
	var AdminHelper = require("syracuse-collaboration/lib/helpers").AdminHelper;
	if (filters.solution && filters.folder) {
		// Search an endpoint that matches the solution and the folder
		var db = AdminHelper.getCollaborationOrm(_);
		var solution = db.fetchInstance(_, db.model.getEntity(_, "x3solution"), {
			jsonWhere: {
				code: filters.solution
			}
		});
		if (!solution) throw new Error("no x3solution found for:" + filters.solution);

		endpoint = db.fetchInstance(_, db.model.getEntity(_, "endPoint"), {
			jsonWhere: {
				x3solution: solution.$uuid,
				x3ServerFolder: filters.folder,
				gitFolder: {
					$exists: true,
					$ne: ""
				}
			}
		});
		if (!endpoint) throw new Error("no endpoint found for solution=" + filters.solution + " and folder=" + filters.folder);
	} else {
		// Let's use the current endpoint
		endpoint = AdminHelper.getEndpoint(_, {
			dataset: globals.context.endpointDataset,
		});
	}
	// console.log("endpoint:",endpoint);	
	return endpoint;
}


/// !doc
/// --------------------------
/// ### extractMetadataFromX3(_, entityType, entityName)
/// Intended to be invoked from X3, see `exportMetadataAndPush` for more details
exports.extractMetadataFromX3 = function(_, filters, options) {
	try {
		filters = filters || {};
		var endpoint = getEndpoint(_, filters);
		var sync = exports.newSync(_, endpoint);

		options = options || defaultOptions;

		if (options.skipGit === undefined) {
			options.skipGit = true;
		}
		sync.exportMetadataAndPush(_, filters.entityType, filters.entityName, options);
	} catch (err) {
		console.error('ERROR extractMetadataFromX3 : ', err);
	}
};

/// !doc
/// --------------------------
/// ### removeMetadataFromX3(_, entityType, entityName)
/// Intended to be invoked from X3, see `removeMetadata` for more details
exports.removeMetadataFromX3 = function(_, filters, options) {
	try {
		filters = filters || {};
		var endpoint = getEndpoint(_, filters);
		var sync = exports.newSync(_, endpoint);
		sync.removeMetadata(_, filters.entityType, filters.entityName, options);
	} catch (err) {
		console.error('ERROR removeMetadataFromX3 : ', err);
	}
};

exports.newSync = function(_, endpoint, tracker) {

	nodeLocalConfig.etl = nodeLocalConfig.etl || {};
	var etnaConfig;
	if ((typeof(endpoint) === "object") && endpoint.userConfig) {
		// Init from an object (should only be used for testing purpose)
		etnaConfig = endpoint.etnaConfig;
		etnaConfig.outputFolder = endpoint.outputFolder;
	} else {
		// Init from an endpoint
		if (!endpoint.gitFolder(_))
			throw new Error("git-repo folder is not set for the endpoint " + endpoint.description(_));

		etnaConfig = endpoint.getEtnaConfig(_); // globals.context.config.etna;
		etnaConfig.outputFolder = path.join(endpoint.gitFolder(_), "META");
	}


	// console.log("SQL CONFIG = " + require("util").inspect(etnaConfig.sql, false, null));
	// console.log("MONGO CONFIG = " + require("util").inspect(etnaConfig.mongo, false, null));
	// console.log("OUTPUT FOLDER = " + etnaConfig.outputFolder);

	if (!etnaConfig.outputFolder)
		throw new Error("No output folder was set");

	etnaConfig.trace = etnaConfig.trace || console.log;

	var trace = etnaConfig.trace;
	var entityDescriptors = [];

	/// !doc
	/// --------------------------
	/// ### exportMetadataToJSONs(_, entityType, entityNames)
	/// Export metadata to JSON files and returns the ABSOLUTE filename of the generated JSON files
	function exportMetadataToJSONs(_, entityType, entityNames, options) {
		options = options || {};
		var absoluteFilenames = [];
		var exporter = require("etna-etl/lib/exporter").newExporter(_, {
			etnaConfig: etnaConfig,
			trace: etnaConfig.trace,
			outputFolder: etnaConfig.outputFolder,
			parallel: 1,
		});

		var mongoColn;

		var mongoConfig = etnaConfig.mongo || {};
		var dbUrl = "mongodb://" + (mongoConfig.host || "localhost") + ":" + (mongoConfig.port || 27017) + "/" + mongoConfig.database;
		var mongoDb = mongodb.MongoClient.connect(dbUrl, {
			db: {
				w: 1
			}
		}, _);

		mongoColn = mongoDb.collection("user_settings", _);
		var mongofilter = {
			type: "last_export_meta"
		};

		if (options.incremental) {
			// We have to read the date of the last sync (stored in mongoDb)
			var last = mongoColn.find(mongofilter, _).toArray(_)[0];
			if (last) {
				options.incrementalDate = new Date(last.date);
			}
		}

		if (options.incrementalDate) {
			_trace(locale.format(module, "incrementalExport", options.incrementalDate), "info");
		} else {
			_trace(locale.format(module, "fullExport"), "info");
		}

		var currentDate = new Date();

		if (!exporter.init(_, tracker)) {
			return [];
		}
		if (0) { //(!options.incremental && !entityNames && !entityType) {
			// It's a global export, we need to clean the output folder
			try {
				exporter.clean(_, tracker);
			} catch (err) {
				trace && trace("ERROR : " + locale.format(module, "couldNotCleanOutput", err.message));
				return [];
			}
		}

		var entityExporter = require("etna-etl/lib/export-entity");
		if (entityType) {
			entityDescriptors = [resourceManager.getEntityDescriptor(_, entityType)];
		} else {
			entityDescriptors = resourceManager.getAllEntityDescriptors(_);
		}

		if (tracker) {
			tracker.phase = locale.format(module, "exportToJSONs");
			tracker.phaseDetail = "";
		}

		var messages = [];
		entityDescriptors.forEach_(_, function(_, entityDescriptor) {
			try {
				var generatedFilenames = entityExporter.run(_, exporter, entityDescriptor, tracker, entityNames, options);
				absoluteFilenames = absoluteFilenames.concat(generatedFilenames);
			} catch (e) {
				_trace(e.message, "warning");
				messages.push(e.message);
			}
		});
		messages.forEach(m => console.error(m));

		var syncData = {
			type: mongofilter.type,
			date: currentDate.toISOString()
		};
		mongoColn.update(mongofilter, syncData, {
			upsert: true
		}, _);
		exporter.release(_);

		return absoluteFilenames;
	}

	function getGitWrapper(_) {
		if (!gitWrapperInstance) {
			_trace(locale.format(module, "gitWrapperInit", etnaConfig.outputFolder));
			var options = {
				folder: etnaConfig.outputFolder,
			};
			if (nodeLocalConfig.etl.git) {
				options.trace = nodeLocalConfig.etl.git.trace;
			}

			gitWrapperInstance = new gitWrapper.git(options);
			if (nodeLocalConfig.etl.git && nodeLocalConfig.etl.git.checkCredentials)
				gitWrapperInstance.checkCredentials(_, nodeLocalConfig.etl.git.checkCredentials);
		}
		return gitWrapperInstance;
	}

	function gitAdd(_, filenames) {
		getGitWrapper(_).add(_, filenames);
		_trace(locale.format(module, "gitAddOk"), "success");
	}

	function gitPull(_) {
		if (tracker) {
			tracker.phase = locale.format(module, "gitPull");
			tracker.phaseDetail = "";
		}
		// Try 5 times (may have some temporary network related problems)		
		for (var attempt = 0; attempt < 5; attempt++) {
			var result = getGitWrapper(_).pull(_);
			if (result.exitCode == 0) {
				_trace(locale.format(module, "gitPullOk"), "success");
				return true;
			}
			_trace(locale.format(module, "gitPullFailed", result.err), "warning");
			setTimeout(_, 5000);
		}
		throw new Error(locale.format(module, "gitPullCancel", result.err));
		return false;
	}

	function gitCommitAndPush(_, msg, push) {
		if (tracker) {
			tracker.phase = locale.format(module, "commitPush");
			tracker.phaseDetail = "";
		}
		var result = getGitWrapper(_).commit(_, msg, {
			args: ['-a']
		});
		if (result.exitCode == 1) {
			if (result.err) {
				// result.err may contains warnings and/or errors
				// We have to make sure that an error really occured
				var reg = /^error:\s+(.+$)/im.exec(result.err);
				if (reg) {
					throw new Error(locale.format(module, "commitFailed", reg[0]));
				}
			}
		}
		_trace(locale.format(module, "commitOk"), "success");
		if (push) {
			getGitWrapper(_).push(_);
			_trace(locale.format(module, "pushOk"), "success");
		}
	}

	function getFilenamesFromGitDiff(_, fromSha1, toSha1) {
		var result = getGitWrapper(_).diffStat(_, fromSha1, toSha1);
		return result.diffs.map(function(diff) {
			return diff.filename;
		});
	}

	function getGitHead(_) {
		return getGitWrapper(_).getHead(_).sha1;
	}

	/// !doc 
	/// ### importMetadataToMongo(_, absoluteFilenamesToimport, entityType)
	/// Import some JSON files into mongoDb
	/// * absoluteFilenamesToimport : filenames fo the JSON files to import
	/// * entityType : the type of entities 
	/// * fullSync ? drop the mongo collections before importing
	function importMetadataToMongo(_, absoluteFilenamesToimport, entityType, fullSync, options) {

		options = options || {};
		if (absoluteFilenamesToimport) {
			// A list of absolute filenames is provided ...
			if (!entityType) {
				// ... but the entity type is not provided, we have to parse all the filenames, parse them and group 
				// them by entityType.
				var filesByType = {};
				absoluteFilenamesToimport.forEach(function(filename) {
					var parseResult = jsonHelper.parseMetaFilename(filename);
					filesByType[parseResult.type] = filesByType[parseResult.type] || [];
					filesByType[parseResult.type].push(filename);
				});
				Object.keys(filesByType).forEach_(_, function(_, entityType) {
					importMetadataToMongo(_, filesByType[entityType], entityType, fullSync, {
						skipTexts: true
					});
				});
				if (!options.skipTexts) {
					_syncText(_, "aplstd", options);
					_syncText(_, "atexte", options);
				}
				return;
			}
		}

		if (tracker) {
			tracker.phase = locale.format(module, "importType", entityType);
			tracker.phaseDetail = locale.format(module, "importType", entityType);
		}

		var t0 = Date.now();
		var importer = require("etna-etl/lib/importer").newImporter(_, {
			trace: etnaConfig.trace,
			mongo: etnaConfig.mongo,
			absoluteFilenames: absoluteFilenamesToimport,
			metaFolder: etnaConfig.outputFolder,
		});

		if (entityType) {
			entityDescriptors = [resourceManager.getEntityDescriptor(_, entityType)];
		} else {
			entityDescriptors = resourceManager.getAllEntityDescriptors(_);
		}
		var importedFilesCount = 0;
		entityDescriptors.forEach_(_, function(_, entityDescriptor) {
			importedFilesCount += importer.fillTables(_, entityDescriptor, tracker, fullSync);
		});

		_trace(locale.format(module, "importDone", importedFilesCount, Math.round((Date.now() - t0) / 1000)), "info");
		if (!options.skipTexts) {
			_syncText(_, "aplstd", options);
			_syncText(_, "atexte", options);
		}
	}

	function _trace(message, severity) {
		trace && trace(message);
		if (severity) {
			if (tracker) {
				tracker.$diagnoses = tracker.$diagnoses || [];
				tracker.$diagnoses.push({
					$severity: severity,
					$message: message,
				});
			}
		}
	}

	function _syncText(_, textType, options) {
		var incSync = require("./incrementalSync").newIncrementalSync(etnaConfig, tracker);
		return incSync.syncText(_, textType, options);
	}


	function _syncTextServer(_, baseUrl, product) {
		console.log("syncTextServer:" + baseUrl + " product:" + product);

		var exporter = require("etna-etl/lib/exporter").newExporter(_, {
			etnaConfig: etnaConfig,
			trace: etnaConfig.trace,
			outputFolder: etnaConfig.outputFolder,
			parallel: 1,
		});

		// Export ATEXTE
		var texts = [];
		var nbTexts = 0;

		function flush(_) {
			var request = ez.devices.http.client({
				method: "POST",
				url: baseUrl + '/api/SYNC',
				headers: {
					//			Authorization: 'Basic YWRtaW46YWRtaW4=',
					'content-type': 'application/json',
				},
			});
			var response = request.end(JSON.stringify(texts)).response(_);
			var result = JSON.parse(response.readAll(_));
			// console.log("", result);
			texts = [];
			nbTexts = 0;
		}

		function syncLan(_, lan) {
			console.log("synchronize " + product + " for " + lan);

			function clean(s) {
				return s === " " ? "" : s;
			}

			var textResults = exporter.buildExecute(_, null, "NUMERO_0, LAN_0, LANORI_0, TEXTE_0, COMMENT_0", "ATEXTE", "NUMERO, LAN", {
				LAN: lan
			}, null, null);

			textResults.forEach_(_, function(_, line) {
				if (++nbTexts >= 100) flush(_);
				texts.push({
					NUMERO: line.NUMERO,
					TEXTE: line.TEXTE,
					COMMENT: clean(line.COMMENT),
					LAN: line.LAN,
					LANORI: clean(line.LANORI),
					product: product
				});
			});
			flush(_);
			console.log("language " + lan + " synchronized");
		};

		var request = ez.devices.http.client({
			method: "DELETE",
			url: baseUrl + ["/api/ATEXTES", product].join('/'),
			headers: {
				'content-type': 'application/json',
			}
		});
		var response = request.end(JSON.stringify({})).response(_);
		var result = JSON.parse(response.readAll(_));
		console.log("delete result:", result);


		// Get languages defined for the product:		
		var results = exporter.buildExecute(_, null, "NBRLAN_0, " + (new Array(20)).fill(0).map((e, i) => "LAN_" + i).join(','), "ADOSSIER", null, {
			DOSSIER: etnaConfig.sql.user
		}, null, null);

		if (results.length)
			for (var i = 0; i < results[0].NBRLAN; i++) syncLan(_, results[0].LAN[i]);

		/*
		// Export APLSTD
		var aplStdResults = exporter.buildExecute(_, 
			null, 
			"LANCHP_0, LANMES_0, LANNUM_0, LAN_0", 
			"APLSTD", 
			"LANCHP, LANNUM, LAN", 
			null, null);

		aplStdResults.forEach_(_, function(_, line) {
			var request = ez.devices.http.client({
				method: "post",
				url: baseUrl + '/api/APLSTD/SUPERV/' + line.LANCHP,
				headers: {
					//			Authorization: 'Basic YWRtaW46YWRtaW4=',
					'content-type': 'application/json',
				},
			});

			var response = request.end(JSON.stringify({
				LANNUM: line.LANNUM,
				LANMES: line.LANMES,
				LAN: line.LAN,
			})).response(_);
			var result = JSON.parse(response.readAll(_));
			console.log("", result);
		});
*/
	}

	return {
		/// !doc
		/// ### exportMetadataAndPush(_, entityType, entityNames, options)
		/// Exports the metadata of set of entities.
		/// An export consists in :
		/// * Creating JSON files that contain the metadata
		/// * importing the generated JSON files to mongDb
		/// * committing and pushing the generated files to git
		///
		/// When no `entityType` is provided, all the types will be processed (tables, views, representations, ...).
		/// When no `entityNames` is provided, all the entities will be processed (ABANK, ABICOND, ...).
		///
		/// In the file system, the JSON files will be dispatched in many folders, according to 
		/// the pattern : `moduleName/entitySubDir/entityName`
		/// * moduleName : the name of the module that uses the meta (SUPERV, FINANCE BP, ...)
		/// * entitySubDir : depends of the type of entity : TABLES, ACTIONS, VIEWS, ....
		/// * entityName : the name of the entity
		/// There might be several generated files for a given entity, for instance, one describing
		/// the class, another for the SQL table, ...
		exportMetadataAndPush: function(_, entityType, entityNames, options) {
			if (tracker) {
				tracker.phase = locale.format(module, "exportToJSONs");
				tracker.phaseDetail = locale.format(module, "exportToJSONs");
			}

			if (entityNames && !Array.isArray(entityNames))
				entityNames = entityNames.split(',');

			options = options || defaultOptions;
			var commitMessage;
			if (!(entityType || entityNames)) {
				commitMessage = locale.format(module, "globalCommit");
			} else {
				commitMessage = locale.format(module, "partialCommit");
				if (entityType) {
					commitMessage += ", type = '" + entityType + "'";
				}
				if (entityNames) {
					commitMessage += ", name = [" + entityNames + "]";
				}
			}

			if (!options.skipGit) {
				gitPull(_);
			}

			// Note : the gitWrapper may have changed the current folder
			var t0 = Date.now();
			var absoluteFilenames = exportMetadataToJSONs(_, entityType, entityNames, options);
			_trace(locale.format(module, "exportDone", absoluteFilenames.length, Math.round((Date.now() - t0) / 1000)), "info");
			_trace(locale.format(module, "generatedFiles", absoluteFilenames));

			if (!options.skipGit) {
				// We only add the files, but don't commit them
				// to allow many exports to be grouped into a single commit.
				gitAdd(_, absoluteFilenames);
				// gitCommitAndPush(_, commitMessage, false);
			}

			// Import the metadata to mongo db
			if (options.exportToMongo)
				importMetadataToMongo(_, absoluteFilenames, entityType, false, options);
		},

		/// !doc
		/// ### removeMetadata(_, entityType, entityNames, options)
		/// Remove the metadata of set of entities.
		removeMetadata: function(_, entityType, entityNames, options) {
			options = options || defaultOptions;

			if (entityNames && !Array.isArray(entityNames))
				entityNames = entityNames.split(',');

			var exporter = require("etna-etl/lib/exporter").newExporter(_, {
				etnaConfig: etnaConfig,
				trace: etnaConfig.trace,
				outputFolder: etnaConfig.outputFolder,
				parallel: 1,
			});
			exporter.init(_, tracker);

			const entity = resourceManager.getEntityDescriptor(_, entityType);
			const primaryKey = Array.isArray(entity.primaryKey) ? entity.primaryKey : [entity.primaryKey];

			var mongoConfig = etnaConfig.mongo || {};
			const dbUrl = "mongodb://" + (mongoConfig.host || "localhost") + ":" + (mongoConfig.port || 27017) + "/" + mongoConfig.database;
			if (options.exportToMongo) { // or endpoint.useEtna ?
				var db = mongodb.MongoClient.connect(dbUrl, {
					db: {
						w: 1
					}
				}, _);
				db.open(_);
			}

			entityNames.forEach_(_, function(_, entityName) {
				const absolutePath = path.join(etnaConfig.outputFolder, exporter.product.name, entity.subdir, '_' + entityName + '.json');
				_trace("remove file " + absolutePath);
				fs.unlink(absolutePath, _);
				if (db) {
					const key = entityName.split('~').reduce(function(r, e, i) {
						r[primaryKey[i]] = e;
						return r;
					}, {});
					_trace("remove " + JSON.stringify(key) + " from mongo collection " + entity.tableName);
					db.collection(entity.tableName, _).remove(key, null, _);
				}
			});
		},
		/// !doc
		/// ### importMetadata(_)
		/// Imports metadata from file system (JSON files) to mongoDb
		/// This function will import ALL the JSON files that were updated since the last time
		/// an import was done. It relies on git to retrieve the updated files (from the last sha1 to the HEAD) 
		importMetadata: function(_, options) {
			options = options || defaultOptions;
			var mongoConfig = etnaConfig.mongo || {};
			var server = new mongodb.Server(mongoConfig.host || "localhost", mongoConfig.port || 27017, {});
			var db = new mongodb.Db(mongoConfig.database || "x3meta", server, {
				w: 1 //"majority"
			});
			db.open(_);

			var headSha1;
			var last;
			if (!options.skipGit) {
				gitPull(_);
				// first, we have to retrieve the sha1 of the last time we used this import
				var coln = db.collection("user_settings", _);
				last = coln.find({
					type: "last_import_meta"
				}, _).toArray(_)[0];

				headSha1 = getGitHead(_);
			}

			// We have to process all the files that were retrieved by the 'pull git'
			var filenames = options.absoluteFilenames;
			var somethingToImport = true;
			_trace(locale.format(module, "importingToMongo"), "info");
			if (last) {
				// We only have to process files that were modified since the last import
				// i.e those who are concerned by a commit from the last sha1 to HEAD
				if (last.data.sha1 === headSha1) {
					_trace(locale.format(module, "mongoIsUpToDate"), "success");
					return;
				}
				filenames = getFilenamesFromGitDiff(_, last.data.sha1, headSha1);
				_trace(locale.format(module, "processFromRevision", last.data.sha1, headSha1), "info");

				if (!filenames || filenames.length == 0) {
					_trace(locale.format(module, "nothingToProcess"), "success");
					somethingToImport = false;
				} else {
					_trace(locale.format(module, "processFound", filenames.length), "info");
				}
			} else {
				// first import. we have to import all the files
				_trace(locale.format(module, "processAll"), "info");
			}

			if (somethingToImport) {
				try {
					importMetadataToMongo(_, filenames, undefined, false, options);
				} catch (err) {
					console.error("ERROR importMetadataToMongo : ", err);
				}
			}

			// Now, we have to write the current HEAD so that the next import will only process
			// files from this head.
			if (!options.skipGit) {
				var sha1Doc = {
					type: "last_import_meta",
					data: {
						sha1: headSha1
					}
				};
				coln.update({
					type: "last_import_meta"
				}, sha1Doc, {
					upsert: true
				}, _);

				_trace(locale.format(module, "refHead", headSha1), "info");
			}
			trace(locale.format(module, "importDoneMark"));

		},
		/* This module is not used ... for now
		exportData: function(_, s3Cfg) {
			try {
				require("etna-etl/lib/sqldump").exportAll(_, etnaConfig, tracker, s3Cfg);
			} catch (err) {
				console.error(err.message);
				throw new Error(err.message);
			}
		},

		importData: function(_) {
			try {
				require("etna-etl/lib/sqldump").importAll(_, etnaConfig, tracker);
			} catch (err) {
				console.error(err.message);
				throw new Error(err.message);
			}
		},
*/
		importMetadataToMongo: importMetadataToMongo,

		/* This module is not used ... for now
		importTest: function(_) {
			require("etna-etl/lib/sqldump").importTest(_, etnaConfig, tracker);
		},

		importFilesFromS3: function(_, s3Cfg) {
			require("etna-etl/lib/sqldump").importFilesFromS3(_, etnaConfig, tracker, s3Cfg);
		},
*/
		syncText: _syncText,
		syncTextServer: _syncTextServer,
	};
};