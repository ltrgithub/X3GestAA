"use strict";

/// !doc
/// # Sync Metadata
/// Tools to synchronize metadata from Sql (SqlServer, oracle, ...) to mongodb
/// Usage : 
/// ```
/// var sync = require("etna-etl/lib/sync").newSync(config, tracker);
/// sync.exportMetadataAndPush(_);
/// ```

var fs = require('streamline-fs'),
	path = require('path'),
	program = require('commander'),
	mongodb = require('streamline-mongodb'),
	gitWrapper = require('etna-etl/lib/gitWrapper'),
	globals = require('streamline/lib/globals'),
	supervisor = require("etna-supervisor/lib/supervisor"),
	defaultOptions = {
		skipGit: true
	};

exports.$exported = true;

/// !doc
/// --------------------------
/// ### extractMetadataFromX3(_, entityType, entityName)
/// Intended to be invoked from X3, see `exportMetadataAndPush` for more details
exports.extractMetadataFromX3 = function(_, filters, options) {
	try {
		filters = filters || {};
		var endpoint = require("syracuse-collaboration/lib/helpers").AdminHelper.getEndpoint(_, {
			dataset: globals.context.endpointDataset,
		});
		var config = endpoint.getEtnaConfig(_); // globals.context.config.etna;
		var sync = exports.newSync(config);

		options = options || defaultOptions;
		if (options.skipGit === undefined)
			options.skipGit = true;

		sync.exportMetadataAndPush(_, filters.entityType, filters.entityName, options);
	} catch (err) {
		console.error('ERROR : ' + err.message);
		console.error(err.stack);
	}

};

exports.newSync = function(config, tracker) {

	var solutionPath = config.solutionPath || path.resolve(__dirname, '../../../tmp');
	config.metaFolder = config.metafolder || config.folderName + "/META";
	config.trace = config.trace || console.log;

	var trace = config.trace;
	var entities = [],
		texts = [];

	function getResources(_, folder) {
		var resPath = path.join(__dirname, folder);
		return fs.readdir(resPath, _).map_(_, function(_, filename) {
			return (/(.*)\.(_js)/).test(filename) ? require(path.join(resPath, filename)) : null;
		}).filter(function(file) {
			return file !== null;
		});
	};

	function getResource(_, folder, entityType) {
		var filename = path.join(__dirname, folder, entityType);
		return require(filename);
	};

	function extractMetadata(_, entityType, entityName) {
		var exporter = require("etna-etl/lib/exporter").newExporter({
			sql: config.sql,
			force: true, // for now!
			parallel: 1,
		});
		var entityExporter = require("etna-etl/lib/export-entity");
		var entityResource = getResource(_, "entities", entityType);
		var result = entityExporter.extractSingle(_, exporter, entityResource.entity, entityName);
		return result;
	}

	function exportMetadataToJSONs(_, entityType, entityNames) {
		var filenames = [];
		var exporter = require("etna-etl/lib/exporter").newExporter({
			sql: config.sql,
			trace: config.trace,
			solutionPath: solutionPath,
			metaFolder: config.metaFolder,
			force: true, // for now!
			parallel: 1,
		});

		// if (!program.test || program.clean) exporter.clean(_);

		exporter.init(_, tracker);
		if (!entityNames && !entityType) {
			// It's a global export, we need to clean the output folder
			exporter.clean(_, tracker);
		}
		exporter.check(_);

		var entityExporter = require("etna-etl/lib/export-entity");
		if (entityType) {
			entities = [getResource(_, "entities", entityType)];
		} else {
			entities = getResources(_, "entities");
		}

		if (tracker) {
			tracker.phase = "Export metadata to JSON files ...";
			tracker.phaseDetail = "";
		}

		entities.forEach_(_, function(_, entityResource) {
			var generatedFilenames = entityExporter.run(_, exporter, entityResource.entity, tracker, entityNames);
			// Note : generatedFilenames is an array 
			filenames.push.apply(filenames, generatedFilenames);
		});


		var textExporter = require("etna-etl/lib/export-text");
		texts = getResources(_, "texts");
		texts.forEach_(_, function(_, textResource) {
			var generatedFilenames = textExporter.run(_, exporter, textResource.text, tracker, entityType, entityNames);
			// Note : generatedFilenames is an array 
			filenames.push.apply(filenames, generatedFilenames);
		});
		return filenames;
	}

	function getGitWrapper() {
		var options = {
			folder: config.solutionPath
		};
		return new gitWrapper.git(options);
	}

	function gitAdd(_, filenames) {
		getGitWrapper().add(_, filenames);
		_trace("'git add' OK", "success");
	}

	function gitPull(_) {
		if (tracker) {
			tracker.phase = "Git pull ...";
			tracker.phaseDetail = "";
		}
		// Try 5 times (may have some temporary network related problems)		
		for (var attempt = 0; attempt < 5; attempt++) {
			var result = getGitWrapper().pull(_);
			if (result.exitCode == 0) {
				_trace("'git pull' OK", "success");
				return true;
			}
			_trace("'git pull' failed, will try again in 5 seconds, reason = " + result.err, "warning");
			setTimeout(~_, 5000);
		}
		throw new Error("Cancel : 'git pull' failed 5 successive times, last reason = " + result.err);
		return false;
	}

	function gitCommitAndPush(_, msg) {
		if (tracker) {
			tracker.phase = "Commit/push to git ...";
			tracker.phaseDetail = "";
		}
		var result = getGitWrapper().commit(_, msg, {
			args: ['-a']
		});
		if (result.exitCode == 1) {
			if (result.err) {
				// result.err may contains warnings and/or errors
				// We have to make sure that an error really occured
				var reg = /^error:\s+(.+$)/im.exec(result.err);
				if (reg) {
					throw new Error("Could not commit changes, reason = " + reg[0]);
				}
			}
		}
		_trace("'git commit' OK", "success");
		getGitWrapper().push(_);
		_trace("'git push' OK", "success");
	}

	function getFilenamesFromGitDiff(_, fromSha1, toSha1) {
		var allText = '';
		var result = getGitWrapper().diffStat(_, fromSha1, toSha1);
		return result.diffs.map(function(diff) {
			return diff.filename;
		});
	}

	function getGitHead(_) {
		return getGitWrapper().getHead(_).sha1;
	}

	function generatePatch(_) {
		if (tracker) {
			tracker.phase = "Generating update patch ...";
			tracker.phaseDetail = "";
		}
		console.log("------------------- GENERATION DU PATCH ------------------------");
	}

	function importMetadataToMongo(_, filenamesToimport) {

		if (tracker) {
			tracker.phase = "Import to mongo db ...";
			tracker.phaseDetail = "";
		}
		var importer = require("etna-etl/lib/importer").newImporter(_, {
			trace: config.trace,
			rootFolder: solutionPath,
			metaFolder: config.metaFolder,
			mongo: config.mongo,
			filenames: filenamesToimport,
		}).open(_);
		importer.buildFilesList(_);
		// importer.createTables(_);
		entities = getResources(_, "entities");
		entities.forEach_(_, function(_, entityResource) {
			importer.fillTables(_, entityResource.entity, tracker);
		});
		importer.fillTexts(_);
	}

	function _trace(message, severity) {
		trace && trace(message);
		if (severity) {
			if (tracker)
				(tracker.$diagnoses = tracker.$diagnoses || []).push({
					severity: severity,
					message: message,
				});
		}

	}
	return {
		/// !doc

		/*		
		/// ### copyMetaFromSqlToMongo(_, entityType, entityName)
		/// Copies the metadata of a single entity from Sql (Sql Server/Oracle) to MongoDb
		/// The exported entity is described by its type (classes, views, representations, ...)
		/// and its name.
		/// for instance, to copy the metadata of table 'AABREV' : `copyMetaFromSqlToMongo(_, "tables", "AABREV")`
		copyMetaFromSqlToMongo: function(_, entityType, entityName) {

			var filenames = exportMetadataToJSONs(_, entityType, entityName);
			return;

			var meta = extractMetadata(_, entityType, entityName);
			var importer = require("etna-etl/lib/importer").newImporter(_, {
				mongo: config.mongo,
			}).open(_);
			importer.persistMetadataInMongo(_, meta.tableName, meta.data);
		},
*/

		/// !doc 
		/// ### exportMetadataAndPush(_, entityType, entityNames, options)
		/// Exports the metadata of set of entities.
		/// An export consists in :
		/// * Creating JSON files that contain the metadata
		/// * importing the generated JSON files to mongDb
		/// * committing and pushing the generated files to git
		///
		/// When no `entityType` is provided, all the types will be processed (tables, views, representations, ...).
		/// When no `entityNames` is provided, all the entities will be processed (ABANK, ABICOND, ...).
		///
		/// In the file system, the JSON files will be dispatched in many folders, according to 
		/// the pattern : `moduleName/entitySubDir/entityName`
		/// * moduleName : the name of the module that uses the meta (SUPERV, FINANCE BP, ...)
		/// * entitySubDir : depends of the type of entity : TABLES, ACTIONS, VIEWS, ....
		/// * entityName : the name of the entity
		/// There might be several generated files for a given entity, for instance, one describing
		/// the class, another for the SQL table, ...
		exportMetadataAndPush: function(_, entityType, entityNames, options) {
			options = options || defaultOptions;
			var commitMessage;
			if (!(entityType || entityNames))
				commitMessage = "Global commit";
			else {
				commitMessage = "Partial commit";
				if (entityType)
					commitMessage += ", type = '" + entityType + "'";
				if (entityNames)
					commitMessage += ", name = [" + entityNames + "]";
			}

			if (!options.skipGit)
				gitPull(_);

			var filenames = exportMetadataToJSONs(_, entityType, entityNames);

			if (!options.skipGit) {
				gitAdd(_, filenames);
				gitCommitAndPush(_, commitMessage);
			}
			// Import the metadata to mongo db
			importMetadataToMongo(_, filenames);
		},

		/// !doc
		/// ### importMetadata(_)
		/// Imports metadata from file system (JSON files) to mongoDb
		/// This function will import ALL the JSON files that were updated since the last time
		/// an import was done. It relies on git to retrieve the updated files (from the last sha1 to the HEAD) 
		importMetadata: function(_, options) {
			options = options || defaultOptions;
			var mongoConfig = config.mongo || {};
			var server = new mongodb.Server(mongoConfig.host || "localhost", mongoConfig.port || 27017, {});
			var db = new mongodb.Db(mongoConfig.database || "x3meta", server, {
				w: 1 //"majority"
			});
			db.open(_);

			var headSha1;
			if (!options.skipGit) {
				gitPull(_);
				// first, we have to retrieve the sha1 of the last time we used this import
				var coln = db.collection("user_settings", _);
				var last = coln.find({
					type: "last_import_meta"
				}, _).toArray(_)[0];

				headSha1 = getGitHead(_);
			}

			// We have to process all the files that were retrieved by the 'pull git'
			var filenames;
			var somethingToImport = true;
			_trace("Importing to Mongodb", "info");
			if (last) {
				// We only have to process files that were modified since the last import
				// i.e those who are concerned by a commit from the last sha1 to HEAD
				if (last.data.sha1 === headSha1) {
					_trace("Mongodb is already up-to-date", "success");
					return;
				}
				filenames = getFilenamesFromGitDiff(_, last.data.sha1, headSha1);
				_trace("Processing files from revisions : " + last.data.sha1 + ".." + headSha1, "info");

				if (!filenames || filenames.length == 0) {
					_trace("Nothing to process", "success");
					somethingToImport = false;
				} else {
					_trace("Found " + filenames.length + " files to process...", "info");
				}
			} else {
				// first import. we have to import all the files
				_trace("Processing all files", "info");
			}

			if (somethingToImport)
				importMetadataToMongo(_, filenames);

			// Now, we have to write the current HEAD so that the next import will only process
			// files from this head.
			if (!options.skipGit) {
				var sha1Doc = {
					type: "last_import_meta",
					data: {
						sha1: headSha1
					}
				};
				coln.update({
					type: "last_import_meta"
				}, sha1Doc, {
					upsert: true
				}, _);

				_trace("Reference head = " + headSha1, "info");
			}
			generatePatch(_);

		},

		exportData: function(_, s3Cfg) {
			require("etna-etl/lib/sqldump").exportAll(_, config, tracker, s3Cfg);
		},

		importData: function(_) {
			require("etna-etl/lib/sqldump").importAll(_, config, tracker);
		},

		importFilesFromS3: function(_, s3Cfg) {
			require("etna-etl/lib/sqldump").importFilesFromS3(_, config, tracker, s3Cfg);
		},

		syncAplStd: function(_, options) {

			var superv = supervisor.create(_, config);

			options = options || defaultOptions;
			var mongoConfig = config.mongo || {};
			var mongoServer = new mongodb.Server(mongoConfig.host || "localhost", mongoConfig.port || 27017, {});
			var mongoDb = new mongodb.Db(mongoConfig.database || "x3meta", mongoServer, {
				w: 1 //"majority"
			});
			mongoDb.open(_);
			var settingsColn = mongoDb.collection("user_settings", _);
			var dataColn = mongoDb.collection("APLSTD", _);
			console.log(mongoConfig);
			var lastSyncFilter = {
				type: "last_aplstd_sync"
			};
			var lastSync = settingsColn.find(lastSyncFilter, _).toArray(_)[0];
			var fullSync = options.fullSync;

			if (!lastSync) {
				// First sync : full sync
				fullSync = true;
			}

			var lastSyncDateFilter;

			if (fullSync) {
				// Full sync
				dataColn.remove({}, {}, _);
				_trace("Full synchronization : the mongoDb collection has been cleared.");
				lastSyncDateFilter = null;
			} else {
				// Incremental sync. We only process update / insert (remove are ignored, for now)
				var lastSyncDate = new Date(Date.parse(lastSync.date));
				_trace("Incremental synchronization : will only process changes from " + lastSyncDate);
				lastSyncDateFilter = {
					"CREDATTIM_0": {
						$gt: lastSyncDate
					}
				};
			}
			var count = 0;
			var tableReader = superv.sqlDriver.createTableReader(_, superv.sqlDriver.getTableDef(_, "APLSTD"));
			if (lastSyncDateFilter != null)
				tableReader = tableReader.filter(lastSyncDateFilter);
			tableReader.forEach(_, function(_, row) {
				var filter = {
					"LAN_0": row["LAN_0"],
					"LANNUM_0": row["LANNUM_0"],
					"LANCHP_0": row["LANCHP_0"],
				};
				count++;
				if ((count % 100) == 0)
					_trace("Processed " + count + ", last = " + JSON.stringify(filter), "info");

				if (fullSync)
					dataColn.insert(row, _);
				else
					dataColn.update(filter, row, {
						upsert: true
					}, _);
			});

			_trace("Processed " + count + " records");

			settingsColn.update(lastSyncFilter, {
				type: lastSyncFilter.type,
				date: new Date(),
			}, {
				upsert: true
			}, _);

		},


	};
};