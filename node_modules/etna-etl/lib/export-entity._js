"use strict";

var locale = require('streamline-locale'),
	jsonHelper = require('scm-helper/lib/jsonHelper'),
	path = require('path'),
	productsHelper = require('scm-helper/lib/productsHelper');


//var defaultExcludedColumns = ["AUUID", "UPDDAT", "UPDUSR", "UPDDATTIM", "CREDAT", "CREUSR", "CREDATTIM", "UPDTICK"];
var defaultExcludedColumns = ["AUUID"];



/// !doc 
/// ### run(_, exporter, entity, tracker, entityNames)
/// Exports the metadata of a set of entities into JSON files
/// This function returns the names of the generated files. 
/// WARNING : The filenames are ABSOLUTE to the solution.metaFolder
///
/// * exporter : the exporter that will be used to serialize the metadata into JSON files
/// * entity : the entity descriptor
/// * tracker : the tracker
/// * entityNames : the name of the entities
exports.run = function(_, exporter, entity, tracker, entityNames, options) {

	const COLNAME_UPDDATTIM = "UPDDATTIM";
	const COLNAME_MODULE = "MODULE";

	function _trace(message, severity) {
		trace && trace(message);
		if (severity) {
			if (tracker) {
				tracker.$diagnoses = tracker.$diagnoses || [];
				tracker.$diagnoses.push({
					$severity: severity,
					$message: message,
				});
			}
		}
	}

	// Query the data for ONE item (i.e. one table, one class, ...)
	// pkValue : PK of the item to query
	function queryData(_, tableMeta, entityDescriptor, pkValue) {
		function queryChildren(_, childDescriptor, parentRow, blockIndex) {
			var childKeys = {};
			var parentMappings = childDescriptor.parentMappings;
			if (!parentMappings) {
				if (entityDescriptor.primaryKey) {
					// Build the parent mappings from the PK of the parent
					parentMappings = entity.primaryKey.reduce(function(global, item) {
						global[item] = item;
						return global;
					}, {});
				} else {
					// Build the parent mappings from the parent mappings of the parent
					parentMappings = entityDescriptor.parentMappings;
				}
				childDescriptor.parentMappings = parentMappings;
			}
			var decodedParentRow;
			Object.keys(parentMappings).forEach(function(childPropertyName) {
				var parentProperty = parentMappings[childPropertyName];
				if ("function" === typeof(parentProperty)) {
					// Evaluate the function to get the value to map
					if (!decodedParentRow)
						decodedParentRow = jsonHelper.decodeObject(parentRow);
					childKeys[childPropertyName] = parentProperty(decodedParentRow);
				} else if ("object" === typeof(parentProperty)) {
					childKeys[childPropertyName] = parentProperty.value;
				} else if (parentProperty === "$INDEX") {
					childKeys[childPropertyName] = blockIndex + 1;
				} else if (parentProperty.indexOf('"') === 0 || parentProperty.indexOf("'") === 0) {
					// to manage constant in the keys
					childKeys[childPropertyName] = parentProperty;
				} else {
					childKeys[childPropertyName] = jsonHelper.decodeValueFromJsonString(null, parentRow[parentProperty]).value;
				}
				if (childKeys[childPropertyName] === "") {
					childKeys[childPropertyName] = " ";
				}
			});

			return queryData(_,
				getTableMeta(_, childDescriptor.tableName, childDescriptor.excludedColumns || defaultExcludedColumns),
				childDescriptor,
				childKeys);
		}

		function queryTable(_, tableMeta, keys, entityDescriptor, parentDescriptor, queryOptions) {
			queryOptions = queryOptions || {};
			entityDescriptor = entityDescriptor || {};

			// Build the list of columns to query
			var colsToQuery = [];
			Object.keys(tableMeta.columns).forEach(function(colName) {
				var colDef = tableMeta.columns[colName];
				for (var i = 0; i < colDef.dimension; i++) {
					colsToQuery.push(colDef.columnName + "_" + i);
				};
			});

			var where = parentDescriptor ? parentDescriptor.primaryKey : "";

			queryOptions.entityDescriptor = entityDescriptor;
			var sqlResults = buildExecute(_, tableMeta, colsToQuery, tableMeta.tableName, entityDescriptor.orderBy, keys, where, queryOptions);
			return sqlResults;
		}

		// Some basic consistency checks
		if (entityDescriptor.itemIndex) {
			if (entityDescriptor.orderBy !== entityDescriptor.itemIndex) {
				throw new Error(locale.format(module, "orderByMismatchesItemIndex", entityDescriptor.tableName, entity.title));
			}
			if (!entityDescriptor.distinctKey) {
				throw new Error(locale.format(module, "distinctKeyIsMissing", entityDescriptor.tableName, entity.title));
			}
			if (entityDescriptor.distinctKey === entityDescriptor.itemIndex) {
				throw new Error(locale.format(module, "distinctKeyMatchesItemIndex", entityDescriptor.tableName, entity.title));
			}
		}
		// Read the data for the entity itself
		var data = queryTable(_, tableMeta, pkValue, entityDescriptor, null, {
			formatValues: true,
			specialExport: options.specialExport,
		});

		if (Object.keys(data).length === 0) {
			return data;
		}

		if (entityDescriptor.children) {
			// Now, query the data for the children of all the items
			Object.keys(entityDescriptor.children).forEach_(_, function(_, childName) {
				var childEntity = entityDescriptor.children[childName];
				data.forEach_(_, function(_, itemData) {
					var childData = queryChildren(_, childEntity, itemData);
					if (childEntity.parentCount) {
						// The child description defines a property that contains the number of children.
						// This property is a property of the parent : it needs to be removed and it will
						// be automatically recomputed when the JSON file will be read
						delete itemData[childEntity.parentCount];
					}
					if (childData.length)
						itemData[childName] = childData;
				});
			});
		}

		data = data.map_(_, function(_, dataPart) {
			if (entityDescriptor.dummyObject) {
				Object.keys(dataPart).forEach(function(propertyName) {
					var remove;
					if (Array.isArray(dataPart[propertyName])) {
						// is it a child ? 
						remove = !(entityDescriptor.children && entityDescriptor.children.hasOwnProperty(propertyName));
					} else {
						remove = true;
					}
					if (remove)
						delete dataPart[propertyName];
				});
			} else {
				dataPart = jsonHelper.createGroups(
					entityDescriptor,
					jsonHelper.sortJsonObject(_, dataPart, entityDescriptor),
					getTableMeta(_, entityDescriptor.tableName)
				);
				if (entityDescriptor.itemIndex) {
					// The property bound to 'itemIndex' is the index of the item in the collection
					// It must be removed from the JSON file and will be recomputed when reading the JSON file
					// This way, if some conflicts have to be resolved on this file and some other items are
					// inserted, they will be correctly re-indexed
					delete dataPart[entityDescriptor.itemIndex];
				}

				// Parse all the groups
				if (entityDescriptor.groups) {
					entityDescriptor.groups.filter(function(group) {
						// We are only interested in group with children
						return (group.children && dataPart[group.name]);
					}).forEach_(_, function(_, group) {
						// We have to parse each block (item of a group) that has been created by the createGroups function.
						dataPart[group.name].items.forEach_(_, function(_, blockData, blockIndex) {

							// Each child must be queried using the current context as a parent context.
							// The parent content will be composed of properties from the parent itself but also of properties
							// from a block.
							// For instance, when querying then 'AMSKZON' child, we will need the 'CODMSK' of the parent (screen)
							// to bind the zones to the right screen.
							Object.keys(group.children).forEach_(_, function(_, childName) {
								var child = group.children[childName];
								blockData[childName] = queryChildren(_, child, dataPart, blockIndex);
							});
						});
					});
				}
			}
			return dataPart;
		});
		return data;
	}

	function getTableMeta(_, tableName, excludedColumns) {
		var tableMeta = tableMetas[tableName];
		if (!tableMeta) {
			tableMeta = exporter.getTableMeta(_, tableName, excludedColumns);
			tableMetas[tableName] = tableMeta;
		}
		return tableMeta;
	}

	// Formats a bunch of values into a srtring
	// _formatPkValues(["AA", "BB"], {AA:1, BB:"XX", CC:"YY"}) = 1~XX
	function _formatPkValues(colNames, data) {
		var arr = colNames.reduce(function(fullArray, colName) {
			var val = data[colName] || "";
			if (typeof(val) === "string") {
				val = val.trim();
			}
			fullArray.push(val);
			return fullArray;
		}, []);
		return arr.join("~");
	}

	options = options || {};
	var tableMetas = {};

	var t0 = Date.now();
	var buildExecute = exporter.buildExecute,
		trace = exporter.trace;

	var excludedColumns = entity.excludedColumns || defaultExcludedColumns;

	var tableMeta = getTableMeta(_, entity.tableName, excludedColumns);

	var colsToQuery = [].concat(entity.primaryKey);
	if (options.incrementalDate)
		colsToQuery.push(COLNAME_UPDDATTIM);
	if (options.specialExport)
		colsToQuery.push(COLNAME_MODULE);

	// First, we have to filter the entities, according to the provided filter
	// Note : here, we don't provide any pk, we want all the entities : they will be filtered later
	// We can't run a 'SELECT .. .FROM ... WHERE ...' query because we can have more than one entityName
	// This would lead to complex (and possibly too long) WHERE clauses : (... AND ... AND ...) OR (... AND ... AND ...) ... 
	var keysToUseForFilter = entity.primaryKey;
	if (!Array.isArray(keysToUseForFilter))
		keysToUseForFilter = keysToUseForFilter.split(',');
	var alreadyFilteredKeys = [];
	var pkValues = buildExecute(_, null, colsToQuery, entity.tableName, entity.primaryKey, null, null, {
		groupBy: entity.primaryKey,
	}).filter(function(s) {
		if (entityNames) {
			// note : entityNames contains names like X~Y~Z where X, Y, Z are the values of the primary key
			// We have to build a string with concatenated values to check if it belongs to entityName
			var pkValuesStr = keysToUseForFilter.map(function(colName, index) {
				const pkVal = s[colName];
				if (pkVal === ' ')
					return '';
				else
					return pkVal;
			}).join('~');
			if (alreadyFilteredKeys.indexOf(pkValuesStr) !== -1) {
				// For instance, on documentationlinks, there is a dummy parent object with a partial primary key
				// all the rows that concern the children of this dummy parent will be accepted with the partial key
				// We only need to validate the first rown the other one will be processed as a child of this 
				// dummy parent
				return false;
			}
			alreadyFilteredKeys.push(pkValuesStr);
			return entityNames.indexOf(pkValuesStr) !== -1;
		}


		if (options.specialExport) {
			return options.modulesFilter.indexOf(s.MODULE) != -1;
		}

		if (options.incrementalDate) {
			if (!s[COLNAME_UPDDATTIM]) {
				// a null date is considered as a 'very long ago' date (sth like 1599/11/31 for oracle)
				// Items with a null date will only be synchronized on the first export.
				// options.incrementalDate is null for the first export (full or incremental)
				return false;
			}
			if (new Date(s[COLNAME_UPDDATTIM]) <= options.incrementalDate) {
				// This object has already been synchronized.
				return false;
			}
		}
		// if (entity.title === "screen") {
		// 	if (s["CODMSK"] < "PLPW2")
		// 		return false;
		// }

		if (entity.excludedInitials) {
			return !~entity.excludedInitials.indexOf(s[entity.primaryKey[0]][0]);
		}
		return true;
	}).map(function(row) {
		// Remove the UPDDATTIM column. It was only included to compare dates (incremental sync)
		if (options.incrementalDate) {
			delete row[COLNAME_UPDDATTIM];
		}
		if (options.specialExport) {
			delete row[COLNAME_MODULE];
		}
		return row;
	});

	// Here, pkValues contains all the keys of the entityType that should be exported
	// For instance, when exporting tables, pkValues will look like : 
	// [{"CODFIC" : "XXX1"}, {"CODFIC" : "XXX2"}, {"CODFIC" : "XXX3"}, .... ] where XXX1, XXX2, XXX3, are the PK in 
	// the ATABLE table of the tables to export.

	var absoluteFilenames = [];

	_trace(locale.format(module, "exportingItems", entity.title, pkValues.length), "info");

	pkValues.forEach_(_, function(_, pkValue, idx) {
		_trace(locale.format(module, "exportingItemsDetail", entity.title, (idx + 1), pkValues.length, JSON.stringify(pkValue)));

		var mainData = queryData(_, tableMeta, entity, pkValue);


		// Now, we can write the files
		var modulePart;
		if (exporter.productId === productsHelper.knownProducts.supervisor) {
			// Supervisor specific : to not dispatch the JSON files from their module : always use 'SUPERV'
			modulePart = "SUPERV";
		} else if (exporter.moduleNames[mainData.MODULE]) {
			if (options.specialExport) {
				modulePart = mainData.MODULE + "-" + exporter.moduleNames[mainData.MODULE];
			} else {
				modulePart = exporter.moduleNames[mainData.MODULE];
			}
		} else if (mainData.CHAPITRE) {
			var chapter = mainData.CHAPITRE;
			var idx = chapter.indexOf('|');
			if (idx != -1) {
				chapter = chapter.substring(idx + 1);
			}
			modulePart = path.join("CHAPTERS", chapter);
		} else
			modulePart = "GLOBAL";

		var dataPart = mainData[0];

		// We have to re-sort the JSON object to avoid git differences
		// When a conflict will be detected on this JSON object, the conflict will be resolved and the JSON
		// will be sorted with jsonHelper.sortJsonObject() function. So, the original JSON file (the one we are building now)
		// must be sorted with the same function to limit git differences.
		dataPart = jsonHelper.sortJsonObject(_, dataPart, entity);

		var filename = _formatPkValues(entity.primaryKey, pkValue);
		if (filename === "")
			throw new Error("Could not export entity " + entity.title + ", the primary key (" + entity.primaryKey + ") is empty");

		// Now, we can write the file
		var absoluteFilename = exporter.writeResource(_, {
			path: path.join(modulePart, entity.subdir),
			fileName: filename,
			data: dataPart
		});
		absoluteFilenames.push(absoluteFilename);
	});


	if (absoluteFilenames.length)
		_trace(locale.format(module, "exportDone", entity.title, JSON.stringify(pkValues.length), Math.round((Date.now() - t0) / 1000)), "info");

	return absoluteFilenames;
};