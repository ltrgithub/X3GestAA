"use strict";

var locale = require('streamline-locale'),
	jsonHelper = require('scm-helper/lib/jsonHelper'),
	path = require('path'),
	productsHelper = require('scm-helper/lib/productsHelper');


//var defaultExcludedColumns = ["AUUID", "UPDDAT", "UPDUSR", "UPDDATTIM", "CREDAT", "CREUSR", "CREDATTIM", "UPDTICK"];
var defaultExcludedColumns = ["AUUID"];



/// !doc 
/// ### run(_, exporter, entity, tracker, entityNames)
/// Exports the metadata of a set of entities into JSON files
/// This function returns the names of the generated files. 
/// WARNING : The filenames are ABSOLUTE to the solution.metaFolder
///
/// * exporter : the exporter that will be used to serialize the metadata into JSON files
/// * entity : the entity descriptor
/// * tracker : the tracker
/// * entityNames : the name of the entities
exports.run = function(_, exporter, entity, tracker, entityNames, options) {

	const COLNAME_UPDDATTIM = "UPDDATTIM";
	const COLNAME_MODULE = "MODULE";

	function _trace(message, severity) {
		trace && trace(message);
		if (severity) {
			if (tracker) {
				tracker.$diagnoses = tracker.$diagnoses || [];
				tracker.$diagnoses.push({
					$severity: severity,
					$message: message,
				});
			}
		}
	}

	// Query the data for ONE item (i.e. one table, one class, ...)
	// pkValue : PK of the item to query
	// dataFromDummyObject : set when querying the children of a dummyObject
	function queryData(_, tableMeta, entityDescriptor, pkValue, dataFromDummyObject) {

		// dummyObjectRows is only set when the parent of the children we are querying is a dummyObject. 
		// it contains all the rows that were read with the partial PK (the PK set on the dummyObject)
		function _queryChildren(_, childDescriptor, parentRow, blockIndex, dummyObjectRows) {
			var childKeys = {};
			var parentMappings = childDescriptor.parentMappings;
			if (!parentMappings || dummyObjectRows) {
				var propertiesToMap;
				if (dummyObjectRows) {
					// When dealing with children of a dummyObject, we ignore the parentMappings
					// set in the configuration file and we compute them from the distinct keys
					propertiesToMap = childDescriptor.distinctKey;
				} else
					propertiesToMap = entityDescriptor.primaryKey;
				if (propertiesToMap) {
					if (!Array.isArray(propertiesToMap))
						propertiesToMap = propertiesToMap.split(',');
					// Build the parent mappings from the PK of the parent
					// [x,y] => {x:x, y:y}
					parentMappings = propertiesToMap.reduce(function(global, item) {
						global[item] = item;
						return global;
					}, {});
				} else {
					// Build the parent mappings from the parent mappings of the parent
					parentMappings = entityDescriptor.parentMappings;
				}
				childDescriptor.parentMappings = parentMappings;
			}
			var decodedParentRow;
			Object.keys(parentMappings).forEach(function(childPropertyName) {
				var parentProperty = parentMappings[childPropertyName];
				if ("function" === typeof(parentProperty)) {
					// Evaluate the function to get the value to map
					if (!decodedParentRow)
						decodedParentRow = jsonHelper.decodeObject(parentRow);
					childKeys[childPropertyName] = parentProperty(decodedParentRow);
				} else if ("object" === typeof(parentProperty)) {
					childKeys[childPropertyName] = parentProperty.value;
				} else if (parentProperty === "$INDEX") {
					childKeys[childPropertyName] = blockIndex + 1;
				} else if (parentProperty.indexOf('"') === 0 || parentProperty.indexOf("'") === 0) {
					// to manage constant in the keys
					childKeys[childPropertyName] = parentProperty;
				} else {
					childKeys[childPropertyName] = jsonHelper.decodeValueFromJsonString(null, parentRow[parentProperty]).value;
				}
				if (childKeys[childPropertyName] === "") {
					childKeys[childPropertyName] = " ";
				}
			});

			const childResults = queryData(_,
				getTableMeta(_, childDescriptor.tableName, childDescriptor.excludedColumns || defaultExcludedColumns),
				childDescriptor,
				childKeys,
				dummyObjectRows);
			return childResults;
		}

		function _queryTable(_, tableMeta, keys, entityDescriptor, parentDescriptor, queryOptions) {
			queryOptions = queryOptions || {};
			entityDescriptor = entityDescriptor || {};

			// Build the list of columns to query
			var colsToQuery = [];
			Object.keys(tableMeta.columns).forEach(function(colName) {
				var colDef = tableMeta.columns[colName];
				for (var i = 0; i < colDef.dimension; i++) {
					colsToQuery.push(colDef.columnName + "_" + i);
				};
			});

			var where = parentDescriptor ? parentDescriptor.primaryKey : "";

			queryOptions.entityDescriptor = entityDescriptor;
			var sqlResults = buildExecute(_, tableMeta, colsToQuery, tableMeta.tableName, entityDescriptor.orderBy, keys, where, queryOptions);
			return sqlResults;
		}

		// Some basic consistency checks
		if (entityDescriptor.itemIndex) {
			if (entityDescriptor.orderBy !== entityDescriptor.itemIndex) {
				throw new Error(locale.format(module, "orderByMismatchesItemIndex", entityDescriptor.tableName, entity.title));
			}
			if (!entityDescriptor.distinctKey) {
				throw new Error(locale.format(module, "distinctKeyIsMissing", entityDescriptor.tableName, entity.title));
			}
			if (entityDescriptor.distinctKey === entityDescriptor.itemIndex) {
				throw new Error(locale.format(module, "distinctKeyMatchesItemIndex", entityDescriptor.tableName, entity.title));
			}
		}
		// Read the data for the entity itself
		var data;
		if (dataFromDummyObject) {
			// We are about to read the data for a child of a dummyObject.
			// When the data of the dummyObject itself have been read, a partial PK (the one defined on the 
			// dummyObject) was used, so the row matching the current child should be in the rows read by
			// the dummyObject (the PK of children is an extend of the PK of the dummyObject)
			// We don't need to send a SQL request, we just need to retrieve the rows we are interested in
			// from all the rows read by the dummyObject.
			data = dataFromDummyObject.filter(function(row) {
				// retain all the items that match the PK values.
				return Object.keys(pkValue).every(function(colName) {
					return pkValue[colName] === jsonHelper.decodeValueFromJsonString(null, row[colName]).value;
				});
			});
		}
		if (!data)
			data = _queryTable(_, tableMeta, pkValue, entityDescriptor, null, {
				formatValues: true,
				specialExport: options.specialExport,
			});

		if (Object.keys(data).length === 0) {
			return data;
		}


		if (entityDescriptor.children) {
			var dummyObject = {};
			// Now, query the data for the children of all the items
			Object.keys(entityDescriptor.children).forEach_(_, function(_, childName) {
				var childEntity = entityDescriptor.children[childName];
				var dummyObjectRows;
				if (entityDescriptor.dummyObject) {
					// Use a copy of data for the children as data will be modified during the next loop
					dummyObjectRows = JSON.parse(JSON.stringify(data));
					dummyObject[childName] = [];
				}
				data.forEach_(_, function(_, itemData) {
					var childData = _queryChildren(_, childEntity, itemData, undefined, dummyObjectRows);
					if (childEntity.parentCount) {
						// The child description defines a property that contains the number of children.
						// This property is a property of the parent : it needs to be removed and it will
						// be automatically recomputed when the JSON file will be read
						delete itemData[childEntity.parentCount];
					}
					if (childData.length) {
						if (entityDescriptor.dummyObject) {
							childData.forEach(function(item) {
								dummyObject[childName].push(item);
							})
						} else
							itemData[childName] = childData;
					}
				});
			});
			if (entityDescriptor.dummyObject) {
				// A dummyObject only contains children and no other properties
				data = [dummyObject];
			}
		}

		if (entityDescriptor.dummyObject) {
			//The data are already OK and dummyObjects can't have groups
		} else {
			// Create the groups
			data = data.map_(_, function(_, dataPart) {
				// Note : we call jsonHelper.createGroups even when no groups are defined for the entity : the createGroups
				// function also performs some tests
				dataPart = jsonHelper.createGroups(
					entityDescriptor,
					jsonHelper.sortJsonObject(_, dataPart, entityDescriptor),
					getTableMeta(_, entityDescriptor.tableName)
				);
				if (entityDescriptor.itemIndex) {
					// The property bound to 'itemIndex' is the index of the item in the collection
					// It must be removed from the JSON file and will be recomputed when reading the JSON file
					// This way, if some conflicts have to be resolved on this file and some other items are
					// inserted, they will be correctly re-indexed
					delete dataPart[entityDescriptor.itemIndex];
				}

				// Parse all the groups
				if (entityDescriptor.groups) {
					entityDescriptor.groups.filter(function(group) {
						// We are only interested in group with children
						return (group.children && dataPart[group.name]);
					}).forEach_(_, function(_, group) {
						// We have to parse each block (item of a group) that has been created by the createGroups function.
						dataPart[group.name].items.forEach_(_, function(_, blockData, blockIndex) {
							// Each child must be queried using the current context as a parent context.
							// The parent content will be composed of properties from the parent itself but also of properties
							// from a block.
							// For instance, when querying then 'AMSKZON' child, we will need the 'CODMSK' of the parent (screen)
							// to bind the zones to the right screen.
							Object.keys(group.children).forEach_(_, function(_, childName) {
								var child = group.children[childName];
								blockData[childName] = _queryChildren(_, child, dataPart, blockIndex);
							});
						});
					});
				}
				return dataPart;
			});
		}
		return data;
	}

	function getTableMeta(_, tableName, excludedColumns) {
		var tableMeta = tableMetas[tableName];
		if (!tableMeta) {
			tableMeta = exporter.getTableMeta(_, tableName, excludedColumns);
			tableMetas[tableName] = tableMeta;
		}
		return tableMeta;
	}

	// Formats a bunch of values into a srtring
	// _formatPkValues(["AA", "BB"], {AA:1, BB:"XX", CC:"YY"}) => 1~XX
	function _formatPkValues(colNames, data) {
		var arr = colNames.reduce(function(fullArray, colName) {
			var val = data[colName] || "";
			if (typeof(val) === "string") {
				val = val.trim();
			}
			fullArray.push(val);
			return fullArray;
		}, []);
		return arr.join("~");
	}

	options = options || {};
	var tableMetas = {};

	var t0 = Date.now();
	var buildExecute = exporter.buildExecute,
		trace = exporter.trace;

	var excludedColumns = entity.excludedColumns || defaultExcludedColumns;

	var tableMeta = getTableMeta(_, entity.tableName, excludedColumns);

	var colsToQuery = [].concat(entity.primaryKey);
	if (options.incrementalDate)
		colsToQuery.push(COLNAME_UPDDATTIM);
	if (options.specialExport)
		colsToQuery.push(COLNAME_MODULE);

	if (entity.dummyObject) {
		// Rules for dumy objects : 
		// Rule #1 : the dummyObject must provide an OrderBy
		if (!entity.orderBy)
			throw new Error(locale.format(module, "dummyObjectWithoutOrderBy", entity.title));
		var dummyObjectPK = entity.primaryKey;
		if (!Array.isArray(dummyObjectPK))
			dummyObjectPK = dummyObjectPK.split(',');
		Object.keys(entity.children).forEach(function(childName) {
			const child = entity.children[childName];
			if (child.orderBy) {
				// Rule #2 : The children of a dummy object must not define an orderBy
				throw new Error(locale.format(module, "dummyObjectChildWithOrderBy", entity.title, childName));
			}

			// Rule #3 : The children of a dummyObject can only extend the PK the dummyObject
			var childPK = child.distinctKey;
			if (!childPK)
				throw new Error(locale.format(module, "dummyObjectChildWithoutDistinctKey", entity.title, childName));
			if (!Array.isArray(childPK))
				childPK = childPK.split(',');
			if (child.length <= dummyObjectPK.lenght)
				throw new Error(locale.format(module, "dummyObjectChildDoesNotExtendPK", entity.title, childName));
			dummyObjectPK.forEach(function(pkPart, index) {
				if (pkPart !== childPK[index])
					throw new Error(locale.format(module, "dummyObjectChildDoesNotExtendPK", entity.title, childName));
			});
		});
	}


	// First, we have to filter the entities, according to the provided filter
	// Note : here, we don't provide any pk, we want all the entities : they will be filtered later
	// We can't run a 'SELECT .. .FROM ... WHERE ...' query because we can have more than one entityName
	// This would lead to complex (and possibly too long) WHERE clauses : (... AND ... AND ...) OR (... AND ... AND ...) ... 
	var keysToUseForFilter = entity.primaryKey;
	if (!Array.isArray(keysToUseForFilter))
		keysToUseForFilter = keysToUseForFilter.split(',');
	var alreadyFilteredKeys = [];
	var pkValues = buildExecute(_, null, colsToQuery, entity.tableName, entity.primaryKey, null, null, {
		groupBy: entity.primaryKey,
	}).filter(function(s) {
		if (entityNames) {
			// note : entityNames contains names like X~Y~Z where X, Y, Z are the values of the primary key
			// We have to build a string with concatenated values to check if it belongs to entityName
			var pkValuesStr = keysToUseForFilter.map(function(colName, index) {
				const pkVal = s[colName];
				if (pkVal === ' ')
					return '';
				else
					return pkVal;
			}).join('~');
			if (alreadyFilteredKeys.indexOf(pkValuesStr) !== -1) {
				// For instance, on documentationlinks, there is a dummy parent object with a partial primary key
				// all the rows that concern the children of this dummy parent will be accepted with the partial key
				// We only need to validate the first rown the other one will be processed as a child of this 
				// dummy parent
				return false;
			}
			alreadyFilteredKeys.push(pkValuesStr);
			return entityNames.indexOf(pkValuesStr) !== -1;
		}


		if (options.specialExport) {
			return options.modulesFilter.indexOf(s.MODULE) != -1;
		}

		if (options.incrementalDate) {
			if (!s[COLNAME_UPDDATTIM]) {
				// a null date is considered as a 'very long ago' date (sth like 1599/11/31 for oracle)
				// Items with a null date will only be synchronized on the first export.
				// options.incrementalDate is null for the first export (full or incremental)
				return false;
			}
			if (new Date(s[COLNAME_UPDDATTIM]) <= options.incrementalDate) {
				// This object has already been synchronized.
				return false;
			}
		}
		// if (entity.title === "screen") {
		// 	if (s["CODMSK"] < "PLPW2")
		// 		return false;
		// }

		if (entity.excludedInitials) {
			return !~entity.excludedInitials.indexOf(s[entity.primaryKey[0]][0]);
		}
		return true;
	}).map(function(row) {
		// Remove the UPDDATTIM column. It was only included to compare dates (incremental sync)
		if (options.incrementalDate) {
			delete row[COLNAME_UPDDATTIM];
		}
		if (options.specialExport) {
			delete row[COLNAME_MODULE];
		}
		return row;
	});
	// Here, pkValues contains all the keys of the entityType that should be exported
	// For instance, when exporting tables, pkValues will look like : 
	// [{"CODFIC" : "XXX1"}, {"CODFIC" : "XXX2"}, {"CODFIC" : "XXX3"}, .... ] where XXX1, XXX2, XXX3, are the PK in 
	// the ATABLE table of the tables to export.

	var absoluteFilenames = [];

	_trace(locale.format(module, "exportingItems", entity.title, pkValues.length), "info");

	pkValues.forEach_(_, function(_, pkValue, idx) {
		_trace(locale.format(module, "exportingItemsDetail", entity.title, (idx + 1), pkValues.length, JSON.stringify(pkValue)));

		var mainData = queryData(_, tableMeta, entity, pkValue);

		// Now, we can write the files
		var modulePart;
		if (exporter.productId === productsHelper.knownProducts.supervisor) {
			// Supervisor specific : to not dispatch the JSON files from their module : always use 'SUPERV'
			modulePart = "SUPERV";
		} else if (exporter.moduleNames[mainData.MODULE]) {
			if (options.specialExport) {
				modulePart = mainData.MODULE + "-" + exporter.moduleNames[mainData.MODULE];
			} else {
				modulePart = exporter.moduleNames[mainData.MODULE];
			}
		} else if (mainData.CHAPITRE) {
			var chapter = mainData.CHAPITRE;
			var idx = chapter.indexOf('|');
			if (idx != -1) {
				chapter = chapter.substring(idx + 1);
			}
			modulePart = path.join("CHAPTERS", chapter);
		} else
			modulePart = "GLOBAL";

		var dataPart = mainData[0];

		// We have to re-sort the JSON object to avoid git differences
		// When a conflict will be detected on this JSON object, the conflict will be resolved and the JSON
		// will be sorted with jsonHelper.sortJsonObject() function. So, the original JSON file (the one we are building now)
		// must be sorted with the same function to limit git differences.
		dataPart = jsonHelper.sortJsonObject(_, dataPart, entity);

		var filename = _formatPkValues(entity.primaryKey, pkValue);
		if (filename === "")
			throw new Error("Could not export entity " + entity.title + ", the primary key (" + entity.primaryKey + ") is empty");

		// Now, we can write the file
		var absoluteFilename = exporter.writeResource(_, {
			path: path.join(modulePart, entity.subdir),
			fileName: filename,
			data: dataPart
		});
		absoluteFilenames.push(absoluteFilename);
	});


	if (absoluteFilenames.length)
		_trace(locale.format(module, "exportDone", entity.title, JSON.stringify(pkValues.length), Math.round((Date.now() - t0) / 1000)), "info");

	return absoluteFilenames;
};